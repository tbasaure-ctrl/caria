{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diagnóstico de Overfitting - Caria Models\n",
        "\n",
        "Este notebook evalúa los modelos `.pkl` (quality, valuation, momentum) y el modelo PyTorch Lightning para detectar overfitting comparando métricas entre train/val/test.\n",
        "\n",
        "## Pasos:\n",
        "1. Instalar dependencias\n",
        "2. Montar Google Drive\n",
        "3. Cargar datos y modelos\n",
        "4. Evaluar modelos\n",
        "5. Generar reporte\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Instalar Dependencias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar dependencias necesarias\n",
        "!pip install -q pandas numpy scikit-learn xgboost lightgbm joblib pyarrow pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar PyTorch Lightning (opcional, para evaluar SimpleFusionModel)\n",
        "!pip install -q pytorch-lightning torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Montar Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Configurar ruta base donde están tus datos\n",
        "# AJUSTA ESTA RUTA según donde subiste los archivos en Drive\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/caria_data'  # Cambia esto a tu ruta\n",
        "\n",
        "# Verificar que existe\n",
        "if os.path.exists(DRIVE_BASE_PATH):\n",
        "    print(f\"✓ Ruta encontrada: {DRIVE_BASE_PATH}\")\n",
        "    print(f\"Contenido:\")\n",
        "    for item in os.listdir(DRIVE_BASE_PATH):\n",
        "        print(f\"  - {item}\")\n",
        "else:\n",
        "    print(f\"⚠ Ruta no encontrada: {DRIVE_BASE_PATH}\")\n",
        "    print(\"Por favor ajusta DRIVE_BASE_PATH a la ubicación de tus datos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configurar Rutas y Crear Estructura\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Crear estructura de directorios en Colab\n",
        "BASE_DIR = Path('/content/caria_workspace')\n",
        "BASE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Directorios necesarios\n",
        "(BASE_DIR / 'data' / 'gold').mkdir(parents=True, exist_ok=True)\n",
        "(BASE_DIR / 'models').mkdir(parents=True, exist_ok=True)\n",
        "(BASE_DIR / 'artifacts' / 'diagnostics').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Estructura creada en: {BASE_DIR}\")\n",
        "\n",
        "# Copiar archivos desde Drive (ajusta las rutas según tu estructura)\n",
        "drive_data_path = Path(DRIVE_BASE_PATH)\n",
        "\n",
        "# Copiar archivos parquet de gold\n",
        "for split in ['train', 'val', 'test']:\n",
        "    src = drive_data_path / 'data' / 'gold' / f'{split}.parquet'\n",
        "    dst = BASE_DIR / 'data' / 'gold' / f'{split}.parquet'\n",
        "    if src.exists():\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"✓ Copiado: {split}.parquet ({dst.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"⚠ No encontrado: {src}\")\n",
        "\n",
        "# Copiar modelos .pkl\n",
        "for model_name in ['quality_model', 'valuation_model', 'momentum_model', 'feature_config']:\n",
        "    src = drive_data_path / 'models' / f'{model_name}.pkl'\n",
        "    dst = BASE_DIR / 'models' / f'{model_name}.pkl'\n",
        "    if src.exists():\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"✓ Copiado: {model_name}.pkl\")\n",
        "    else:\n",
        "        print(f\"⚠ No encontrado: {src}\")\n",
        "\n",
        "# Copiar checkpoint de PyTorch (opcional)\n",
        "checkpoint_src = drive_data_path / 'artifacts' / 'models' / 'epoch=20-val_loss=0.0038.ckpt'\n",
        "if checkpoint_src.exists():\n",
        "    checkpoint_dst = BASE_DIR / 'artifacts' / 'models' / checkpoint_src.name\n",
        "    checkpoint_dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy2(checkpoint_src, checkpoint_dst)\n",
        "    print(f\"✓ Copiado checkpoint: {checkpoint_src.name}\")\n",
        "else:\n",
        "    print(f\"⚠ Checkpoint no encontrado (opcional)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "\n",
        "# Intentar importar PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "    HAS_PYTORCH = True\n",
        "    print(\"✓ PyTorch Lightning disponible\")\n",
        "except ImportError:\n",
        "    HAS_PYTORCH = False\n",
        "    print(\"⚠ PyTorch Lightning no disponible - solo modelos .pkl serán evaluados\")\n",
        "\n",
        "\n",
        "def evaluate_pkl_model(\n",
        "    model_path: Path,\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    X_val: pd.DataFrame,\n",
        "    y_val: pd.Series,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_test: pd.Series,\n",
        "    is_classifier: bool = False,\n",
        ") -> dict[str, dict[str, float]]:\n",
        "    \"\"\"Evalúa un modelo .pkl en train/val/test.\"\"\"\n",
        "    model = joblib.load(model_path)\n",
        "    results: dict[str, dict[str, float]] = {}\n",
        "\n",
        "    for split_name, X, y in [\n",
        "        (\"train\", X_train, y_train),\n",
        "        (\"val\", X_val, y_val),\n",
        "        (\"test\", X_test, y_test),\n",
        "    ]:\n",
        "        if X.empty or y.empty:\n",
        "            continue\n",
        "\n",
        "        y_pred = model.predict(X)\n",
        "        if is_classifier:\n",
        "            y_pred_proba = model.predict_proba(X)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
        "            results[split_name] = {\n",
        "                \"accuracy\": float(accuracy_score(y, y_pred)),\n",
        "                \"auc_roc\": float(roc_auc_score(y, y_pred_proba)) if len(np.unique(y)) > 1 else 0.0,\n",
        "            }\n",
        "        else:\n",
        "            results[split_name] = {\n",
        "                \"rmse\": float(np.sqrt(mean_squared_error(y, y_pred))),\n",
        "                \"mae\": float(mean_absolute_error(y, y_pred)),\n",
        "                \"r2\": float(r2_score(y, y_pred)),\n",
        "            }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def calculate_gaps(metrics: dict[str, dict[str, float]]) -> dict[str, dict[str, float]]:\n",
        "    \"\"\"Calcula gaps entre splits.\"\"\"\n",
        "    gaps: dict[str, dict[str, float]] = {}\n",
        "\n",
        "    if \"train\" in metrics and \"val\" in metrics:\n",
        "        train_val_gaps = {}\n",
        "        for metric in metrics[\"train\"]:\n",
        "            if metric in metrics[\"val\"]:\n",
        "                train_val_gaps[f\"train_val_gap_{metric}\"] = (\n",
        "                    metrics[\"train\"][metric] - metrics[\"val\"][metric]\n",
        "                )\n",
        "        gaps[\"train_val\"] = train_val_gaps\n",
        "\n",
        "    if \"val\" in metrics and \"test\" in metrics:\n",
        "        val_test_gaps = {}\n",
        "        for metric in metrics[\"val\"]:\n",
        "            if metric in metrics[\"test\"]:\n",
        "                val_test_gaps[f\"val_test_gap_{metric}\"] = (\n",
        "                    metrics[\"val\"][metric] - metrics[\"test\"][metric]\n",
        "                )\n",
        "        gaps[\"val_test\"] = val_test_gaps\n",
        "\n",
        "    if \"train\" in metrics and \"test\" in metrics:\n",
        "        train_test_gaps = {}\n",
        "        for metric in metrics[\"train\"]:\n",
        "            if metric in metrics[\"test\"]:\n",
        "                train_test_gaps[f\"train_test_gap_{metric}\"] = (\n",
        "                    metrics[\"train\"][metric] - metrics[\"test\"][metric]\n",
        "                )\n",
        "        gaps[\"train_test\"] = train_test_gaps\n",
        "\n",
        "    return gaps\n",
        "\n",
        "\n",
        "def prepare_pkl_features(\n",
        "    df: pd.DataFrame,\n",
        "    model_path: Path,\n",
        "    feature_config: dict | None = None,\n",
        "    model_name: str | None = None,\n",
        ") -> tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Prepara features y target para modelos .pkl.\"\"\"\n",
        "    # Obtener features del modelo\n",
        "    expected_features = None\n",
        "    if model_path.exists():\n",
        "        try:\n",
        "            model = joblib.load(model_path)\n",
        "            if hasattr(model, \"feature_names_in_\"):\n",
        "                expected_features = list(model.feature_names_in_)\n",
        "            elif hasattr(model, \"get_booster\"):\n",
        "                try:\n",
        "                    booster = model.get_booster()\n",
        "                    if hasattr(booster, \"feature_names\"):\n",
        "                        expected_features = booster.feature_names\n",
        "                except:\n",
        "                    pass\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Si no se pueden obtener del modelo, usar feature_config\n",
        "    if expected_features is None and feature_config:\n",
        "        if model_name == \"quality_model\":\n",
        "            expected_features = feature_config.get(\"quality_features\", [])\n",
        "        elif model_name == \"valuation_model\":\n",
        "            expected_features = feature_config.get(\"valuation_features\", [])\n",
        "        elif model_name == \"momentum_model\":\n",
        "            expected_features = feature_config.get(\"momentum_features\", [])\n",
        "\n",
        "    # Si aún no tenemos features, usar todas las numéricas disponibles\n",
        "    if expected_features is None:\n",
        "        exclude_cols = {\n",
        "            \"ticker\",\n",
        "            \"date\",\n",
        "            \"target\",\n",
        "            \"target_return_20d\",\n",
        "            \"target_drawdown_prob\",\n",
        "            \"target_regime\",\n",
        "            \"features\",\n",
        "            \"feature_columns\",\n",
        "            \"regime_name\",\n",
        "            \"period\",\n",
        "        }\n",
        "        expected_features = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "    # Filtrar solo las features que existen\n",
        "    available_features = [f for f in expected_features if f in df.columns]\n",
        "    missing_features = [f for f in expected_features if f not in df.columns]\n",
        "\n",
        "    if missing_features:\n",
        "        print(f\"    [WARNING] Features faltantes: {missing_features[:5]}...\")\n",
        "\n",
        "    if not available_features:\n",
        "        raise ValueError(f\"No hay features disponibles. Esperadas: {expected_features[:10]}...\")\n",
        "\n",
        "    # Seleccionar y convertir a float32 para ahorrar memoria\n",
        "    X = df[available_features].copy()\n",
        "    for col in X.columns:\n",
        "        if X[col].dtype in [\"float64\", \"float32\"]:\n",
        "            X[col] = X[col].fillna(0.0).astype(\"float32\")\n",
        "        else:\n",
        "            X[col] = X[col].fillna(0)\n",
        "\n",
        "    y = df[\"target\"].copy()\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determinar qué columnas necesitamos para todos los modelos\n",
        "models_dir = BASE_DIR / \"models\"\n",
        "all_needed_cols = set([\"target\", \"date\", \"roic\"])  # roic para quality_model labels\n",
        "\n",
        "for model_name in [\"quality_model\", \"valuation_model\", \"momentum_model\"]:\n",
        "    model_path = models_dir / f\"{model_name}.pkl\"\n",
        "    if model_path.exists():\n",
        "        try:\n",
        "            model = joblib.load(model_path)\n",
        "            if hasattr(model, \"feature_names_in_\"):\n",
        "                all_needed_cols.update(model.feature_names_in_)\n",
        "            elif hasattr(model, \"get_booster\"):\n",
        "                try:\n",
        "                    booster = model.get_booster()\n",
        "                    if hasattr(booster, \"feature_names\"):\n",
        "                        all_needed_cols.update(booster.feature_names)\n",
        "                except:\n",
        "                    pass\n",
        "        except Exception as e:\n",
        "            print(f\"Error cargando {model_name}: {e}\")\n",
        "\n",
        "print(f\"Columnas necesarias identificadas: {len(all_needed_cols)}\")\n",
        "print(f\"Columnas: {sorted(list(all_needed_cols))[:10]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leer archivos parquet (solo las columnas necesarias)\n",
        "gold_path = BASE_DIR / \"data\" / \"gold\"\n",
        "\n",
        "print(\"Cargando datos...\")\n",
        "try:\n",
        "    # Leer metadata primero para verificar columnas disponibles\n",
        "    import pyarrow.parquet as pq\n",
        "    parquet_file = pq.ParquetFile(gold_path / \"train.parquet\")\n",
        "    all_columns = parquet_file.schema.names\n",
        "    \n",
        "    # Filtrar solo columnas que existen\n",
        "    cols_to_read = [c for c in all_needed_cols if c in all_columns]\n",
        "    \n",
        "    print(f\"Leyendo {len(cols_to_read)} columnas de {len(all_columns)} disponibles...\")\n",
        "    \n",
        "    train_df = pd.read_parquet(gold_path / \"train.parquet\", columns=cols_to_read)\n",
        "    val_df = pd.read_parquet(gold_path / \"val.parquet\", columns=cols_to_read)\n",
        "    test_df = pd.read_parquet(gold_path / \"test.parquet\", columns=cols_to_read)\n",
        "    \n",
        "    print(f\"✓ train cargado: {len(train_df)} filas, {len(train_df.columns)} columnas\")\n",
        "    print(f\"✓ val cargado: {len(val_df)} filas, {len(val_df.columns)} columnas\")\n",
        "    print(f\"✓ test cargado: {len(test_df)} filas, {len(test_df.columns)} columnas\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error cargando datos: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar feature config si existe\n",
        "feature_config_path = BASE_DIR / \"models\" / \"feature_config.pkl\"\n",
        "feature_config = None\n",
        "if feature_config_path.exists():\n",
        "    feature_config = joblib.load(feature_config_path)\n",
        "    print(\"✓ feature_config cargado\")\n",
        "else:\n",
        "    print(\"⚠ feature_config no encontrado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluar Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializar reporte\n",
        "report: dict[str, any] = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"models\": {},\n",
        "}\n",
        "\n",
        "# Evaluar modelos .pkl\n",
        "models_dir = BASE_DIR / \"models\"\n",
        "for model_name in [\"quality_model\", \"valuation_model\", \"momentum_model\"]:\n",
        "    model_path = models_dir / f\"{model_name}.pkl\"\n",
        "    if not model_path.exists():\n",
        "        print(f\"\\n⚠ {model_name} no encontrado, omitiendo...\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluando {model_name}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Preparar features\n",
        "    X_train, y_train = prepare_pkl_features(\n",
        "        train_df.copy(), model_path=model_path, feature_config=feature_config, model_name=model_name\n",
        "    )\n",
        "    X_val, y_val = prepare_pkl_features(\n",
        "        val_df.copy(), model_path=model_path, feature_config=feature_config, model_name=model_name\n",
        "    )\n",
        "    X_test, y_test = prepare_pkl_features(\n",
        "        test_df.copy(), model_path=model_path, feature_config=feature_config, model_name=model_name\n",
        "    )\n",
        "\n",
        "    print(f\"  Features usadas: {len(X_train.columns)} ({', '.join(X_train.columns[:5])}...)\")\n",
        "\n",
        "    # Determinar si es clasificador\n",
        "    is_classifier = model_name in [\"quality_model\", \"momentum_model\"]\n",
        "\n",
        "    if is_classifier:\n",
        "        # Para clasificadores, recrear labels\n",
        "        if model_name == \"quality_model\":\n",
        "            train_df_copy = train_df.copy()\n",
        "            train_df_copy[\"roic_percentile\"] = train_df_copy.groupby(\"date\")[\"roic\"].rank(pct=True)\n",
        "            train_df_copy[\"is_quality\"] = (train_df_copy[\"roic_percentile\"] > 0.80).astype(int)\n",
        "            y_train = train_df_copy[\"is_quality\"]\n",
        "\n",
        "            val_df_copy = val_df.copy()\n",
        "            val_df_copy[\"roic_percentile\"] = val_df_copy.groupby(\"date\")[\"roic\"].rank(pct=True)\n",
        "            val_df_copy[\"is_quality\"] = (val_df_copy[\"roic_percentile\"] > 0.80).astype(int)\n",
        "            y_val = val_df_copy[\"is_quality\"]\n",
        "\n",
        "            test_df_copy = test_df.copy()\n",
        "            test_df_copy[\"roic_percentile\"] = test_df_copy.groupby(\"date\")[\"roic\"].rank(pct=True)\n",
        "            test_df_copy[\"is_quality\"] = (test_df_copy[\"roic_percentile\"] > 0.80).astype(int)\n",
        "            y_test = test_df_copy[\"is_quality\"]\n",
        "        elif model_name == \"momentum_model\":\n",
        "            y_train = (train_df[\"target\"] > 0).astype(int)\n",
        "            y_val = (val_df[\"target\"] > 0).astype(int)\n",
        "            y_test = (test_df[\"target\"] > 0).astype(int)\n",
        "\n",
        "    # Evaluar modelo\n",
        "    metrics = evaluate_pkl_model(\n",
        "        model_path, X_train, y_train, X_val, y_val, X_test, y_test, is_classifier=is_classifier\n",
        "    )\n",
        "    gaps = calculate_gaps(metrics)\n",
        "\n",
        "    report[\"models\"][model_name] = {\n",
        "        \"metrics\": metrics,\n",
        "        \"gaps\": gaps,\n",
        "        \"is_classifier\": is_classifier,\n",
        "    }\n",
        "\n",
        "    # Mostrar resultados\n",
        "    print(f\"\\nResultados {model_name}:\")\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        if split in metrics:\n",
        "            print(f\"  {split.upper()}:\")\n",
        "            for metric, value in metrics[split].items():\n",
        "                print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "    if \"train_val\" in gaps:\n",
        "        print(f\"  GAPS train-val:\")\n",
        "        for gap_name, gap_value in gaps[\"train_val\"].items():\n",
        "            print(f\"    {gap_name}: {gap_value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Guardar y Mostrar Reporte\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar reporte\n",
        "output_dir = BASE_DIR / \"artifacts\" / \"diagnostics\"\n",
        "timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "report_path = output_dir / f\"overfitting_report_{timestamp_str}.json\"\n",
        "\n",
        "with report_path.open(\"w\") as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Reporte guardado en: {report_path}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Mostrar resumen completo\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RESUMEN DE OVERFITTING\")\n",
        "print(\"=\" * 60)\n",
        "for model_name, model_data in report[\"models\"].items():\n",
        "    print(f\"\\n{model_name.upper()}:\")\n",
        "    metrics = model_data[\"metrics\"]\n",
        "    gaps = model_data[\"gaps\"]\n",
        "\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        if split in metrics:\n",
        "            print(f\"  {split.upper()}:\")\n",
        "            for metric, value in metrics[split].items():\n",
        "                print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "    if \"train_val\" in gaps:\n",
        "        print(\"  GAPS train-val:\")\n",
        "        for gap_name, gap_value in gaps[\"train_val\"].items():\n",
        "            print(f\"    {gap_name}: {gap_value:.4f}\")\n",
        "\n",
        "    if \"val_test\" in gaps:\n",
        "        print(\"  GAPS val-test:\")\n",
        "        for gap_name, gap_value in gaps[\"val_test\"].items():\n",
        "            print(f\"    {gap_name}: {gap_value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualización (Opcional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar gaps de overfitting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if len(report[\"models\"]) > 0:\n",
        "    fig, axes = plt.subplots(len(report[\"models\"]), 1, figsize=(12, 4 * len(report[\"models\"])))\n",
        "    if len(report[\"models\"]) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, (model_name, model_data) in enumerate(report[\"models\"].items()):\n",
        "        metrics = model_data[\"metrics\"]\n",
        "        \n",
        "        # Seleccionar métrica principal según tipo de modelo\n",
        "        if model_data[\"is_classifier\"]:\n",
        "            main_metric = \"accuracy\"\n",
        "        else:\n",
        "            main_metric = \"r2\"\n",
        "        \n",
        "        if main_metric in metrics.get(\"train\", {}) and main_metric in metrics.get(\"val\", {}) and main_metric in metrics.get(\"test\", {}):\n",
        "            splits = [\"train\", \"val\", \"test\"]\n",
        "            values = [metrics[split][main_metric] for split in splits]\n",
        "            \n",
        "            axes[idx].bar(splits, values, color=[\"blue\", \"orange\", \"red\"])\n",
        "            axes[idx].set_title(f\"{model_name} - {main_metric.upper()}\")\n",
        "            axes[idx].set_ylabel(main_metric.upper())\n",
        "            axes[idx].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Agregar valores en las barras\n",
        "            for i, v in enumerate(values):\n",
        "                axes[idx].text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Guardar figura\n",
        "    fig_path = output_dir / f\"overfitting_plot_{timestamp_str}.png\"\n",
        "    plt.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n",
        "    print(f\"\\nGráfico guardado en: {fig_path}\")\n",
        "else:\n",
        "    print(\"No hay modelos para visualizar\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Copiar Resultados a Drive (Opcional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copiar resultados de vuelta a Drive\n",
        "drive_results_path = Path(DRIVE_BASE_PATH) / \"artifacts\" / \"diagnostics\"\n",
        "drive_results_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "shutil.copy2(report_path, drive_results_path / report_path.name)\n",
        "if 'fig_path' in locals() and fig_path.exists():\n",
        "    shutil.copy2(fig_path, drive_results_path / fig_path.name)\n",
        "\n",
        "print(f\"✓ Resultados copiados a Drive: {drive_results_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
