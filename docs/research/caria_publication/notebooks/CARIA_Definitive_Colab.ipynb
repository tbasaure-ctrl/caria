{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üî¨ CARIA-SR Definitive Validation\n",
                "\n",
                "**CONSISTENT METHODOLOGY:**\n",
                "1. Download S&P 500 constituent prices (500+ stocks)\n",
                "2. Calculate AR + Entropy from cross-sectional covariance\n",
                "3. Z-score + Peak Memory (60-day)\n",
                "4. Quantile Regression validation\n",
                "5. Minsky Hedge (pure structural: peak > 1.5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "# @title 1. Setup\n",
                "!pip install -q yfinance pandas numpy scipy scikit-learn statsmodels seaborn matplotlib pyarrow requests\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import yfinance as yf\n",
                "import requests\n",
                "import warnings\n",
                "from datetime import datetime\n",
                "import statsmodels.formula.api as smf\n",
                "from sklearn.covariance import LedoitWolf\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "np.random.seed(42)\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "WORK_DIR = '/content/drive/MyDrive/CARIA'\n",
                "os.makedirs(WORK_DIR, exist_ok=True)\n",
                "os.makedirs(f'{WORK_DIR}/figures', exist_ok=True)\n",
                "os.makedirs(f'{WORK_DIR}/tables', exist_ok=True)\n",
                "\n",
                "FMP_API_KEY = \"79fY9wvC9qtCJHcn6Yelf4ilE9TkRMoq\"\n",
                "START_DATE = \"2000-01-01\"\n",
                "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
                "\n",
                "print(f\"‚úÖ Output: {WORK_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download_constituents"
            },
            "outputs": [],
            "source": [
                "# @title 2. Download S&P 500 Constituent Prices (500+ stocks)\n",
                "\n",
                "# Get current S&P 500 tickers\n",
                "url = f\"https://financialmodelingprep.com/api/v3/sp500_constituent?apikey={FMP_API_KEY}\"\n",
                "resp = requests.get(url)\n",
                "sp500_tickers = [x['symbol'] for x in resp.json()] if resp.status_code == 200 else []\n",
                "print(f\"S&P 500 constituents: {len(sp500_tickers)}\")\n",
                "\n",
                "# Download prices in batches\n",
                "print(f\"\\nDownloading prices for {len(sp500_tickers)} stocks...\")\n",
                "print(\"‚è≥ This takes 5-10 minutes...\")\n",
                "\n",
                "all_prices = []\n",
                "batch_size = 50\n",
                "\n",
                "for i in range(0, len(sp500_tickers), batch_size):\n",
                "    batch = sp500_tickers[i:i+batch_size]\n",
                "    try:\n",
                "        data = yf.download(batch, start=START_DATE, end=END_DATE, progress=False, auto_adjust=True)['Close']\n",
                "        if isinstance(data, pd.Series):\n",
                "            data = data.to_frame()\n",
                "        all_prices.append(data)\n",
                "        print(f\"   Batch {i//batch_size + 1}/{(len(sp500_tickers)-1)//batch_size + 1} OK\")\n",
                "    except Exception as e:\n",
                "        print(f\"   Batch {i//batch_size + 1} error: {e}\")\n",
                "\n",
                "# Combine all batches\n",
                "prices = pd.concat(all_prices, axis=1)\n",
                "prices = prices.dropna(axis=1, how='all')\n",
                "prices.to_csv(f'{WORK_DIR}/sp500_prices.csv')\n",
                "\n",
                "print(f\"\\n‚úÖ Downloaded {prices.shape[1]} stocks, {len(prices)} days\")\n",
                "print(f\"   Period: {prices.index.min().date()} to {prices.index.max().date()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download_market"
            },
            "outputs": [],
            "source": [
                "# @title 3. Download Market Data (VIX, SPY, TLT, 10Y Yield)\n",
                "\n",
                "print(\"Downloading market data...\")\n",
                "market = yf.download(['^VIX', 'SPY', 'TLT', '^TNX'], start=START_DATE, end=END_DATE, progress=False)\n",
                "\n",
                "market_df = pd.DataFrame({\n",
                "    'volatility': market['Close']['^VIX'],\n",
                "    'price': market['Close']['SPY'],\n",
                "    'tlt': market['Close']['TLT'],\n",
                "    'treasury_10y': market['Close']['^TNX']\n",
                "}).dropna()\n",
                "market_df.index.name = 'Date'\n",
                "market_df.to_csv(f'{WORK_DIR}/market_validation_data.csv')\n",
                "\n",
                "print(f\"\\n‚úÖ Market data: {len(market_df)} days\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "structural_calc"
            },
            "outputs": [],
            "source": [
                "# @title 4. Calculate Structural Metrics (AR + Entropy) ‚è≥ ~15 min\n",
                "\n",
                "def cov_to_corr(S):\n",
                "    d = np.sqrt(np.diag(S))\n",
                "    d = np.where(d == 0, 1e-10, d)\n",
                "    C = S / np.outer(d, d)\n",
                "    return np.nan_to_num((C + C.T) / 2)\n",
                "\n",
                "def eig_metrics(C, k_frac=0.2):\n",
                "    w = np.sort(np.linalg.eigvalsh(C))[::-1]\n",
                "    w = np.maximum(w, 1e-10)  # Avoid negative eigenvalues\n",
                "    k = max(1, int(np.ceil(k_frac * len(w))))\n",
                "    ar = np.sum(w[:k]) / np.sum(w)\n",
                "    p = w / np.sum(w)\n",
                "    ent = -np.sum(p * np.log(p + 1e-10)) / np.log(len(w)) if len(w) > 1 else 0.5\n",
                "    return float(ar), float(ent)\n",
                "\n",
                "# Calculate returns\n",
                "returns = np.log(prices).diff()\n",
                "good_coverage = returns.notna().mean() >= 0.9\n",
                "returns = returns.loc[:, good_coverage]\n",
                "print(f\"Using {returns.shape[1]} stocks with >90% coverage\")\n",
                "\n",
                "# Rolling structural metrics\n",
                "window = 252\n",
                "step = 5\n",
                "lw = LedoitWolf()\n",
                "\n",
                "struct = pd.DataFrame(index=returns.index, columns=['absorption_ratio', 'entropy'], dtype=float)\n",
                "\n",
                "total_steps = (len(returns) - window) // step\n",
                "print(f\"\\nCalculating AR + Entropy ({total_steps} steps)...\")\n",
                "\n",
                "for idx, t in enumerate(range(window, len(returns), step)):\n",
                "    W = returns.iloc[t-window:t]\n",
                "    W = W.loc[:, W.notna().mean() >= 0.9]\n",
                "    \n",
                "    if W.shape[1] < 100:\n",
                "        continue\n",
                "    \n",
                "    W = W.apply(lambda s: s.fillna(s.mean()))\n",
                "    X = W.values - np.nanmean(W.values, axis=0)\n",
                "    \n",
                "    try:\n",
                "        S = lw.fit(X).covariance_\n",
                "        C = cov_to_corr(S)\n",
                "    except:\n",
                "        C = np.corrcoef(X, rowvar=False)\n",
                "        C = np.nan_to_num((C + C.T) / 2)\n",
                "    \n",
                "    ar, ent = eig_metrics(C)\n",
                "    struct.iloc[t] = [ar, ent]\n",
                "    \n",
                "    if (idx + 1) % 100 == 0:\n",
                "        print(f\"   {idx + 1}/{total_steps} ({(idx+1)/total_steps*100:.0f}%)\")\n",
                "\n",
                "struct = struct.ffill().bfill()\n",
                "struct.index.name = 'date'\n",
                "struct.to_csv(f'{WORK_DIR}/caria_structural_metrics.csv')\n",
                "\n",
                "print(f\"\\n‚úÖ Structural metrics saved\")\n",
                "print(f\"   AR mean: {struct['absorption_ratio'].mean():.4f}\")\n",
                "print(f\"   Entropy mean: {struct['entropy'].mean():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "merge_data"
            },
            "outputs": [],
            "source": [
                "# @title 5. Merge Data and Calculate Signals\n",
                "\n",
                "print(\"Loading and merging data...\")\n",
                "\n",
                "# Load from saved files (in case rerunning)\n",
                "struct_df = pd.read_csv(f'{WORK_DIR}/caria_structural_metrics.csv', index_col='date', parse_dates=True)\n",
                "market_df = pd.read_csv(f'{WORK_DIR}/market_validation_data.csv', index_col='Date', parse_dates=True)\n",
                "\n",
                "# Merge\n",
                "df = struct_df.join(market_df, how='inner').sort_index()\n",
                "\n",
                "# Calculate Z-Scores\n",
                "window_z = 252\n",
                "df['absorp_z'] = (df['absorption_ratio'] - df['absorption_ratio'].rolling(window_z).mean()) / df['absorption_ratio'].rolling(window_z).std()\n",
                "\n",
                "# Peak Memory (60 days) - THE KEY FEATURE\n",
                "window_memory = 60\n",
                "df['caria_peak'] = df['absorp_z'].rolling(window_memory).max()\n",
                "\n",
                "# Future returns for prediction\n",
                "df['future_ret_22'] = df['price'].pct_change(22).shift(-22)\n",
                "\n",
                "df = df.dropna()\n",
                "print(f\"\\n‚úÖ Dataset: {len(df)} observations\")\n",
                "print(f\"   Period: {df.index.min().date()} to {df.index.max().date()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "phase8"
            },
            "outputs": [],
            "source": [
                "# @title Phase 8: Regime & Memory Test (Quantile Regression)\n",
                "\n",
                "low_vol_df = df[df['volatility'] < 20].copy()\n",
                "print(f\"Testing on 'Calm Markets' (VIX < 20). N={len(low_vol_df)}\")\n",
                "\n",
                "# Model A: VIX Only\n",
                "mod_vix = smf.quantreg('future_ret_22 ~ volatility', low_vol_df)\n",
                "res_vix = mod_vix.fit(q=0.05)\n",
                "\n",
                "# Model B: VIX + Peak Memory\n",
                "mod_struct = smf.quantreg('future_ret_22 ~ volatility + caria_peak', low_vol_df)\n",
                "res_struct = mod_struct.fit(q=0.05)\n",
                "\n",
                "print(res_struct.summary())\n",
                "\n",
                "print(f\"\\nBase Model (VIX Only) Pseudo R¬≤:      {res_vix.prsquared:.5f}\")\n",
                "print(f\"Structural Model (+Peak) Pseudo R¬≤:   {res_struct.prsquared:.5f}\")\n",
                "imp = ((res_struct.prsquared - res_vix.prsquared)/res_vix.prsquared)*100\n",
                "print(f\"üî• Improvement in Low-Vol Regime:     {imp:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "phase9"
            },
            "outputs": [],
            "source": [
                "# @title Phase 9: Robustness Heatmap\n",
                "\n",
                "windows = [20, 40, 60, 90, 120]\n",
                "vix_caps = [15, 18, 20, 22, 25]\n",
                "results_matrix = np.zeros((len(windows), len(vix_caps)))\n",
                "\n",
                "print(\"Running Sensitivity Grid...\")\n",
                "\n",
                "for w in windows:\n",
                "    df[f'peak_{w}'] = df['absorp_z'].rolling(window=w).max()\n",
                "\n",
                "for i, w in enumerate(windows):\n",
                "    for j, v in enumerate(vix_caps):\n",
                "        subset = df[df['volatility'] < v].copy().dropna()\n",
                "        if len(subset) > 500:\n",
                "            try:\n",
                "                r_b = smf.quantreg('future_ret_22 ~ volatility', subset).fit(q=0.05).prsquared\n",
                "                r_s = smf.quantreg(f'future_ret_22 ~ volatility + peak_{w}', subset).fit(q=0.05).prsquared\n",
                "                results_matrix[i, j] = ((r_s - r_b)/r_b)*100 if r_b > 0 else 0\n",
                "            except: pass\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(results_matrix, annot=True, fmt=\".1f\", cmap=\"RdYlGn\", xticklabels=vix_caps, yticklabels=windows)\n",
                "plt.title(\"Robustness: Improvement in Tail Risk Prediction (%)\")\n",
                "plt.xlabel(\"VIX Threshold\")\n",
                "plt.ylabel(\"Memory Window (Days)\")\n",
                "plt.savefig(f'{WORK_DIR}/figures/Robustness_Heatmap.png', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "phase11"
            },
            "outputs": [],
            "source": [
                "# @title Phase 11: Pure Structural Hedge (Cash)\n",
                "\n",
                "print(\"Running Pure Structural Hedge...\")\n",
                "\n",
                "backtest_df = df.copy().dropna()\n",
                "backtest_df['daily_ret'] = backtest_df['price'].pct_change()\n",
                "\n",
                "# THE SIGNAL: Peak > 1.5 sigma = Unsafe\n",
                "# Lag by 1 day (trade at next open)\n",
                "backtest_df['unsafe_state'] = (backtest_df['caria_peak'].shift(1) > 1.5)\n",
                "\n",
                "# Strategy: If Unsafe -> 0 (Cash). If Safe -> S&P 500.\n",
                "backtest_df['strat_ret'] = np.where(backtest_df['unsafe_state'], 0, backtest_df['daily_ret'])\n",
                "\n",
                "# Cumulative\n",
                "backtest_df['cum_bnh'] = (1 + backtest_df['daily_ret'].fillna(0)).cumprod()\n",
                "backtest_df['cum_strat'] = (1 + backtest_df['strat_ret'].fillna(0)).cumprod()\n",
                "\n",
                "def get_max_drawdown(series):\n",
                "    roll_max = series.cummax()\n",
                "    return ((series - roll_max) / roll_max).min()\n",
                "\n",
                "dd_bnh = get_max_drawdown(backtest_df['cum_bnh'])\n",
                "dd_strat = get_max_drawdown(backtest_df['cum_strat'])\n",
                "\n",
                "days = (backtest_df.index[-1] - backtest_df.index[0]).days\n",
                "years = days / 365.25\n",
                "cagr_bnh = (backtest_df['cum_bnh'].iloc[-1])**(1/years) - 1\n",
                "cagr_strat = (backtest_df['cum_strat'].iloc[-1])**(1/years) - 1\n",
                "\n",
                "print(f\"\\n--- PHASE 11 RESULTS ---\")\n",
                "print(f\"Time in Cash/Hedge: {backtest_df['unsafe_state'].mean()*100:.1f}%\")\n",
                "print(f\"Buy & Hold:   Max DD = {dd_bnh:.1%}, CAGR = {cagr_bnh:.1%}\")\n",
                "print(f\"Minsky Cash:  Max DD = {dd_strat:.1%}, CAGR = {cagr_strat:.1%}\")\n",
                "print(f\"DD Reduction: {dd_bnh - dd_strat:.1%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "phase12"
            },
            "outputs": [],
            "source": [
                "# @title Phase 12: Smart Hedge (Treasuries) + Vol-Targeting\n",
                "\n",
                "print(\"Running Smart Hedge...\")\n",
                "\n",
                "# Treasury return from yield\n",
                "backtest_df['treasury_daily_ret'] = (backtest_df['treasury_10y'] / 100) / 252\n",
                "\n",
                "# Strategy: Unsafe -> Treasuries. Safe -> S&P 500.\n",
                "backtest_df['smart_ret'] = np.where(\n",
                "    backtest_df['unsafe_state'],\n",
                "    backtest_df['treasury_daily_ret'],\n",
                "    backtest_df['daily_ret']\n",
                ")\n",
                "\n",
                "# Levered version (1.5x when safe)\n",
                "leverage = 1.5\n",
                "backtest_df['lev_ret'] = np.where(\n",
                "    backtest_df['unsafe_state'],\n",
                "    backtest_df['treasury_daily_ret'],\n",
                "    backtest_df['daily_ret'] * leverage - (0.05/252 * (leverage-1))\n",
                ")\n",
                "\n",
                "# Cumulative\n",
                "backtest_df['cum_smart'] = (1 + backtest_df['smart_ret'].fillna(0)).cumprod()\n",
                "backtest_df['cum_lev'] = (1 + backtest_df['lev_ret'].fillna(0)).cumprod()\n",
                "\n",
                "dd_smart = get_max_drawdown(backtest_df['cum_smart'])\n",
                "cagr_smart = (backtest_df['cum_smart'].iloc[-1])**(1/years) - 1\n",
                "\n",
                "dd_lev = get_max_drawdown(backtest_df['cum_lev'])\n",
                "cagr_lev = (backtest_df['cum_lev'].iloc[-1])**(1/years) - 1\n",
                "\n",
                "print(f\"\\n--- PHASE 12 RESULTS ---\")\n",
                "print(f\"Benchmark (S&P 500):    DD = {dd_bnh:.1%}, CAGR = {cagr_bnh:.1%}\")\n",
                "print(f\"Minsky (Cash Hedge):    DD = {dd_strat:.1%}, CAGR = {cagr_strat:.1%}\")\n",
                "print(f\"Minsky (Smart/Bond):    DD = {dd_smart:.1%}, CAGR = {cagr_smart:.1%}\")\n",
                "print(f\"Minsky (1.5x Levered):  DD = {dd_lev:.1%}, CAGR = {cagr_lev:.1%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "plot_final"
            },
            "outputs": [],
            "source": [
                "# @title Visualization\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# Equity Curves\n",
                "ax = axes[0, 0]\n",
                "ax.plot(backtest_df.index, backtest_df['cum_bnh'], label='S&P 500', color='gray', alpha=0.5)\n",
                "ax.plot(backtest_df.index, backtest_df['cum_smart'], label=f'Minsky Bond (DD:{dd_smart:.0%})', color='blue', linewidth=2)\n",
                "ax.plot(backtest_df.index, backtest_df['cum_lev'], label=f'Minsky 1.5x (DD:{dd_lev:.0%})', color='darkgreen', linewidth=2)\n",
                "ax.set_yscale('log')\n",
                "ax.set_title('Equity Curves')\n",
                "ax.legend(loc='upper left')\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "# Signal\n",
                "ax = axes[0, 1]\n",
                "ax.plot(backtest_df.index, backtest_df['caria_peak'], color='darkred')\n",
                "ax.axhline(y=1.5, color='orange', linestyle='--', label='Threshold (1.5œÉ)')\n",
                "ax.fill_between(backtest_df.index, 0, 4, where=backtest_df['unsafe_state'], alpha=0.2, color='red')\n",
                "ax.set_title('CARIA Peak Memory Signal')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "# Drawdowns\n",
                "ax = axes[1, 0]\n",
                "dd_bnh_series = backtest_df['cum_bnh'] / backtest_df['cum_bnh'].cummax() - 1\n",
                "dd_smart_series = backtest_df['cum_smart'] / backtest_df['cum_smart'].cummax() - 1\n",
                "ax.fill_between(backtest_df.index, dd_bnh_series, 0, alpha=0.3, color='gray', label='S&P 500')\n",
                "ax.plot(backtest_df.index, dd_smart_series, color='blue', label='Minsky')\n",
                "ax.set_title('Drawdowns')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "# 2020 COVID zoom\n",
                "ax = axes[1, 1]\n",
                "subset = backtest_df.loc['2019-01-01':'2020-06-01']\n",
                "ax.plot(subset.index, subset['volatility'], color='gray', linestyle='--', label='VIX')\n",
                "ax2 = ax.twinx()\n",
                "ax2.plot(subset.index, subset['caria_peak'], color='darkred', linewidth=2, label='Peak Memory')\n",
                "ax2.axhline(y=1.5, color='orange', linestyle='--')\n",
                "ax.set_title('COVID-19 Warning')\n",
                "ax2.legend(loc='upper left')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{WORK_DIR}/figures/Final_Results.png', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "summary"
            },
            "outputs": [],
            "source": [
                "# @title Final Summary\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"üî¨ CARIA-SR DEFINITIVE VALIDATION\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(f\"\\nüìä DATA:\")\n",
                "print(f\"   Universe: {returns.shape[1]} S&P 500 stocks\")\n",
                "print(f\"   Period: {df.index.min().date()} to {df.index.max().date()}\")\n",
                "print(f\"   Observations: {len(df)}\")\n",
                "\n",
                "print(f\"\\nüî¨ QUANTILE REGRESSION (VIX < 20):\")\n",
                "print(f\"   VIX-only Pseudo R¬≤: {res_vix.prsquared:.5f}\")\n",
                "print(f\"   VIX+Peak Pseudo R¬≤: {res_struct.prsquared:.5f}\")\n",
                "print(f\"   Improvement: {imp:.1f}%\")\n",
                "print(f\"   Peak Memory p-value: {res_struct.pvalues['caria_peak']:.4f}\")\n",
                "\n",
                "print(f\"\\nüí∞ MINSKY HEDGE:\")\n",
                "print(f\"   Time in Hedge: {backtest_df['unsafe_state'].mean()*100:.1f}%\")\n",
                "print(f\"   {'Strategy':<20} {'Max DD':>10} {'CAGR':>10}\")\n",
                "print(f\"   {'-'*42}\")\n",
                "print(f\"   {'S&P 500':<20} {dd_bnh:>10.1%} {cagr_bnh:>10.1%}\")\n",
                "print(f\"   {'Minsky Cash':<20} {dd_strat:>10.1%} {cagr_strat:>10.1%}\")\n",
                "print(f\"   {'Minsky Bond':<20} {dd_smart:>10.1%} {cagr_smart:>10.1%}\")\n",
                "print(f\"   {'Minsky 1.5x':<20} {dd_lev:>10.1%} {cagr_lev:>10.1%}\")\n",
                "\n",
                "print(f\"\\nüìÅ Files saved to: {WORK_DIR}/\")\n",
                "print(\"\\n‚úÖ DONE!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}