{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CARIA — Real Data Validation (FMP)\n",
        "\n",
        "Este notebook (Google Colab) permite validar CARIA con **datos reales** descargados desde **Financial Modeling Prep (FMP)**, ejecutando **celda por celda**.\n",
        "\n",
        "### Qué valida\n",
        "- **Entropía de forma**: \\(H_{Sh,z}\\) = Shannon entropy sobre **z-returns** \\(z_t=(r_t-\\mu_t)/\\sigma_t\\)\n",
        "- **Sincronización**: \\(S_{PLV}\\) = Phase Locking Value multi-escala (wavelet → fase → PLV)\n",
        "- **Surrogates** (mínimo publicable): `shuffle`, `time_shift`, `phase_randomize`, con criterio **p < 0.01**\n",
        "- **Ground truth (tu definición C′)**:\n",
        "\n",
        "\\[\n",
        "Crisis_{t+5}=1 \\iff \\sum_{m\\in M} \\mathbb{1}\\{m_{t+1:t+5}=1\\} \\ge 2,\\quad M=\\{\\text{EVT},\\ \\text{Drawdown},\\ \\text{Jump(BNS)}\\}\n",
        "\\]\n",
        "\n",
        "### Output\n",
        "- Probabilidades por cuadrante (Q1–Q4) + **bootstrap CI**\n",
        "- Gráficos: series temporales + espacio de fase\n",
        "\n",
        "> Nota: FMP tiene límites de rate. El notebook incluye pausa entre requests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Setup (instalación + imports)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab: instala dependencias\n",
        "# (statsmodels/yfinance/pyarrow se usan en la sección CARIA-SR estructural)\n",
        "%pip install -q PyWavelets pandas numpy scipy scikit-learn requests matplotlib seaborn statsmodels yfinance pyarrow\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.signal import hilbert\n",
        "import pywt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) FMP API key (ingrésala sin exponerla)\n",
        "\n",
        "Recomendado: usar `getpass()` para que no quede guardada en el notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "# Opción A: variable de entorno (si ya la seteaste)\n",
        "FMP_API_KEY = os.environ.get(\"FMP_API_KEY\", \"\").strip()\n",
        "\n",
        "# Opción B: input seguro\n",
        "if not FMP_API_KEY:\n",
        "    FMP_API_KEY = getpass(\"Pega tu FMP_API_KEY (no se mostrará): \").strip()\n",
        "\n",
        "assert FMP_API_KEY, \"FMP_API_KEY vacío\"\n",
        "\n",
        "FMP_BASE_URL = \"https://financialmodelingprep.com/api/v3\"\n",
        "\n",
        "print(\"✅ API key cargada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Descarga de datos (FMP)\n",
        "\n",
        "Notas:\n",
        "- Para índices, FMP a veces no soporta `^GSPC`. Si falla, usa `SPY` como proxy del S&P 500.\n",
        "- Si algún símbolo falla, el notebook sigue con los demás.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fmp_get_historical(symbol: str, start: str, end: str, api_key: str) -> pd.DataFrame:\n",
        "    url = f\"{FMP_BASE_URL}/historical-price-full/{symbol}\"\n",
        "    params = {\"from\": start, \"to\": end, \"apikey\": api_key}\n",
        "    r = requests.get(url, params=params, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    j = r.json()\n",
        "    hist = j.get(\"historical\", [])\n",
        "    if not hist:\n",
        "        return pd.DataFrame()\n",
        "    df = pd.DataFrame(hist)\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    df = df.sort_values(\"date\")\n",
        "    # columnas estándar\n",
        "    keep = [c for c in [\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"] if c in df.columns]\n",
        "    df = df[keep].copy()\n",
        "    df[\"symbol\"] = symbol\n",
        "    return df\n",
        "\n",
        "# Ajusta símbolos aquí\n",
        "SYMBOLS = {\n",
        "    \"SP500\": \"SPY\",   # proxy S&P500\n",
        "    \"VIX\": \"^VIX\",    # puede fallar en FMP; si falla, prueba \"VIXY\" o \"VXX\"\n",
        "    \"TLT\": \"TLT\",\n",
        "    \"GLD\": \"GLD\",\n",
        "}\n",
        "\n",
        "# ✅ Pediste más historia: comenzamos en 2000\n",
        "START_DATE = \"2000-01-01\"\n",
        "END_DATE = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "data = {}\n",
        "for name, sym in SYMBOLS.items():\n",
        "    try:\n",
        "        df = fmp_get_historical(sym, START_DATE, END_DATE, FMP_API_KEY)\n",
        "        if df.empty:\n",
        "            print(f\"⚠️ {name} ({sym}): sin datos\")\n",
        "        else:\n",
        "            data[name] = df\n",
        "            print(f\"✅ {name} ({sym}): {len(df)} filas, {df['date'].min().date()} → {df['date'].max().date()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ {name} ({sym}): {e}\")\n",
        "    time.sleep(0.35)  # rate limit\n",
        "\n",
        "assert len(data) >= 1, \"No se descargó ningún dataset. Revisa símbolos / API key.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Pre-procesamiento (returns + volatilidad)\n",
        "\n",
        "- Returns: \\(r_t = \\log(P_t/P_{t-1})\\)\n",
        "- Volatilidad: rolling std (30d) anualizada\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_returns_vol(df: pd.DataFrame, vol_window: int = 30) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(\"date\")\n",
        "    df[\"ret\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
        "    df[\"vol\"] = df[\"ret\"].rolling(vol_window).std() * np.sqrt(252)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "proc = {k: add_returns_vol(v) for k, v in data.items()}\n",
        "for k, df in proc.items():\n",
        "    print(k, df.shape, df['date'].min().date(), df['date'].max().date())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Entropía de forma: \\(H_{Sh,z}\\)\n",
        "\n",
        "Calculamos Shannon entropy sobre **z-returns** para eliminar confusión con amplitud:\n",
        "\\[\n",
        "z_t = \\frac{r_t-\\mu_t}{\\sigma_t}\\quad\\Rightarrow\\quad H_{Sh,z}=H(\\{z_t\\})\n",
        "\\]\n",
        "\n",
        "Usamos histograma (FD por defecto).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shannon_hist(x: np.ndarray, bins='fd', normalize: bool = True) -> float:\n",
        "    x = np.asarray(x).flatten()\n",
        "    x = x[~np.isnan(x)]\n",
        "    if len(x) < 30:\n",
        "        return np.nan\n",
        "    counts, _ = np.histogram(x, bins=bins, density=False)\n",
        "    p = counts / max(counts.sum(), 1)\n",
        "    p = p[p > 0]\n",
        "    h = -np.sum(p * np.log2(p))\n",
        "    if normalize:\n",
        "        hmax = np.log2(len(counts)) if len(counts) > 1 else 1.0\n",
        "        h = h / hmax if hmax > 0 else h\n",
        "    return float(h)\n",
        "\n",
        "def rolling_entropy_z(ret: pd.Series, window: int = 30, bins='fd') -> pd.Series:\n",
        "    ret = ret.reset_index(drop=True)\n",
        "    mu = ret.rolling(window).mean()\n",
        "    sig = ret.rolling(window).std().replace(0, np.nan)\n",
        "    z = (ret - mu) / sig\n",
        "\n",
        "    out = pd.Series(np.nan, index=ret.index)\n",
        "    for i in range(window, len(ret)):\n",
        "        out.iloc[i] = shannon_hist(z.iloc[i-window+1:i+1].values, bins=bins, normalize=True)\n",
        "    return out\n",
        "\n",
        "# demo rápido\n",
        "k = list(proc.keys())[0]\n",
        "df0 = proc[k]\n",
        "Hz = rolling_entropy_z(df0['ret'], window=30)\n",
        "print(k, \"Hz mean/std:\", float(np.nanmean(Hz)), float(np.nanstd(Hz)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Sincronización multi-escala: Wavelet → fase → PLV + surrogates\n",
        "\n",
        "- Descomponemos en bandas (por periodos) con wavelets Morlet (CWT).\n",
        "- Extraemos fase con Hilbert.\n",
        "- Calculamos PLV promedio entre pares de bandas.\n",
        "- Validamos contra surrogates (`shuffle`, `time_shift`, `phase_randomize`).\n",
        "\n",
        "Criterio: **significativo si worst-case p < 0.01**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nota: esta sección PLV es costosa. Para correr rápido el notebook,\n",
        "# deja RUN_PLV=False y usa la sección CARIA-SR estructural más abajo.\n",
        "RUN_PLV = False\n",
        "\n",
        "BANDS = {\n",
        "    \"ultra_fast\": (1, 5),\n",
        "    \"short\": (5, 20),\n",
        "    \"medium\": (20, 60),\n",
        "    \"long\": (60, 252),\n",
        "    \"ultra_long\": (252, 504),\n",
        "}\n",
        "\n",
        "# Use an explicit parameterized complex Morlet to avoid FutureWarning:\n",
        "# format: \"cmorB-C\" (bandwidth B, center frequency C)\n",
        "WAVELET = \"cmor1.5-1.0\"\n",
        "\n",
        "def cwt_band_signal(\n",
        "    x: np.ndarray,\n",
        "    low_period: int,\n",
        "    high_period: int,\n",
        "    dt: float = 1.0,\n",
        "    wavelet: str = WAVELET,\n",
        "    n_scales: int = 24,\n",
        "    min_scale: float = 1.0,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Band signal = mean real(CWT) across scales matching periods [low_period, high_period].\n",
        "\n",
        "    Fixes:\n",
        "    - avoids PyWavelets \"Selected scale too small\" by enforcing min_scale\n",
        "    - uses scale2frequency mapping (more correct than hand-made omega0 mapping)\n",
        "    - uses parameterized cmor wavelet (no deprecation warning)\n",
        "    \"\"\"\n",
        "    x = np.asarray(x).astype(float).flatten()\n",
        "    x = np.nan_to_num(x, nan=0.0)\n",
        "\n",
        "    f_low = 1.0 / float(high_period)  # lower freq edge\n",
        "    f_high = 1.0 / float(low_period)  # higher freq edge\n",
        "    fc = float(pywt.scale2frequency(wavelet, 1.0))\n",
        "\n",
        "    # scale = fc / (f * dt)\n",
        "    s_low = fc / (f_high * dt)\n",
        "    s_high = fc / (f_low * dt)\n",
        "\n",
        "    s_low = max(float(min_scale), float(s_low))\n",
        "    s_high = max(s_low + 1e-9, float(s_high))\n",
        "\n",
        "    scales = np.linspace(s_low, s_high, int(max(8, n_scales)))\n",
        "    coef, _freqs = pywt.cwt(x, scales, wavelet, sampling_period=dt)\n",
        "    return np.real(coef).mean(axis=0)\n",
        "\n",
        "def detrend_mean(x: np.ndarray) -> np.ndarray:\n",
        "    x = np.asarray(x).astype(float)\n",
        "    t = np.arange(len(x))\n",
        "    a, b = np.polyfit(t, x, 1)\n",
        "    return x - (a*t + b) - np.mean(x)\n",
        "\n",
        "def phase_series(x: np.ndarray) -> np.ndarray:\n",
        "    x = detrend_mean(x)\n",
        "    ph = np.unwrap(np.angle(hilbert(x)))\n",
        "    return ph\n",
        "\n",
        "def plv(ph1: np.ndarray, ph2: np.ndarray) -> float:\n",
        "    d = ph1 - ph2\n",
        "    return float(np.abs(np.mean(np.exp(1j*d))))\n",
        "\n",
        "def plv_multiscale(price: np.ndarray, bands=BANDS) -> Tuple[float, Dict[str, np.ndarray]]:\n",
        "    price = np.asarray(price).astype(float)\n",
        "    # trabajar con returns para estabilidad\n",
        "    r = np.diff(np.log(price))\n",
        "    r = np.nan_to_num(r, nan=0.0)\n",
        "\n",
        "    band_sig = {}\n",
        "    for name, (lp, hp) in bands.items():\n",
        "        band_sig[name] = cwt_band_signal(r, lp, hp)\n",
        "\n",
        "    phases = {k: phase_series(v) for k, v in band_sig.items()}\n",
        "    keys = list(phases.keys())\n",
        "    vals = []\n",
        "    for i in range(len(keys)):\n",
        "        for j in range(i+1, len(keys)):\n",
        "            vals.append(plv(phases[keys[i]], phases[keys[j]]))\n",
        "\n",
        "    return float(np.mean(vals)), band_sig\n",
        "\n",
        "def surrogate_series(x: np.ndarray, method: str) -> np.ndarray:\n",
        "    x = np.asarray(x)\n",
        "    n = len(x)\n",
        "    if method == 'shuffle':\n",
        "        return np.random.permutation(x)\n",
        "    if method == 'time_shift':\n",
        "        shift = np.random.randint(1, n)\n",
        "        return np.roll(x, shift)\n",
        "    if method == 'phase_randomize':\n",
        "        X = np.fft.fft(x)\n",
        "        amp = np.abs(X)\n",
        "        ph = np.angle(X)\n",
        "        rnd = ph.copy()\n",
        "        if n > 2:\n",
        "            rnd[1:-1] = np.random.uniform(0, 2*np.pi, n-2)\n",
        "        Xs = amp * np.exp(1j*rnd)\n",
        "        return np.real(np.fft.ifft(Xs))\n",
        "    raise ValueError(method)\n",
        "\n",
        "def plv_surrogate_test(price: np.ndarray, n_surrogates: int = 50, alpha: float = 0.01, methods=None) -> dict:\n",
        "    if methods is None:\n",
        "        methods = ['time_shift','phase_randomize','shuffle']\n",
        "\n",
        "    obs, _ = plv_multiscale(price)\n",
        "    per = {}\n",
        "    p_list = []\n",
        "    for m in methods:\n",
        "        s_vals = []\n",
        "        for _ in range(n_surrogates):\n",
        "            s_price = surrogate_series(np.asarray(price), m)\n",
        "            s_plv, _ = plv_multiscale(s_price)\n",
        "            s_vals.append(s_plv)\n",
        "        s_vals = np.asarray(s_vals)\n",
        "        mu = float(np.mean(s_vals))\n",
        "        sd = float(np.std(s_vals))\n",
        "        z = (obs - mu) / sd if sd > 0 else (np.inf if obs > mu else -np.inf)\n",
        "        p = float(1 - stats.norm.cdf(z))  # one-tailed\n",
        "        per[m] = {'mean': mu, 'std': sd, 'p': p, 'z': float(z), 'sig': p < alpha}\n",
        "        p_list.append(p)\n",
        "\n",
        "    worst_p = float(np.max(p_list))\n",
        "    return {\n",
        "        'observed_plv': obs,\n",
        "        'per_method': per,\n",
        "        'worst_p': worst_p,\n",
        "        'significant_all': all(per[m]['sig'] for m in per)\n",
        "    }\n",
        "\n",
        "# quick sanity check (opcional)\n",
        "if RUN_PLV:\n",
        "    k = list(proc.keys())[0]\n",
        "    df0 = proc[k]\n",
        "    res = plv_surrogate_test(df0['close'].values, n_surrogates=25, alpha=0.01)\n",
        "    print('Observed PLV:', res['observed_plv'])\n",
        "    print('Worst p:', res['worst_p'])\n",
        "    print('Significant ALL?:', res['significant_all'])\n",
        "    print('Per-method:', {m: round(v['p'],4) for m,v in res['per_method'].items()})\n",
        "else:\n",
        "    print('RUN_PLV=False → saltando sanity check PLV')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Ground truth \\(Crisis_{t+5}\\) (definición C′)\n",
        "\n",
        "Construimos 3 detectores base:\n",
        "- **EVT (cola)**: evento extremo en retornos (VaR 1% por defecto)\n",
        "- **Drawdown estructural**: drawdown bajo umbral (ej. -15%)\n",
        "- **Jump (BNS)**: test de saltos (Barndorff-Nielsen & Shephard)\n",
        "\n",
        "Definición:\n",
        "- Para cada método \\(m\\): \\(m_{t+1:t+5}=1\\) si hay **al menos 1** evento en \\(t+1..t+5\\)\n",
        "- \\(Crisis_{t+5}=1\\) si **≥ 2 de 3** métodos disparan en ese horizonte\n",
        "\n",
        "\\[\n",
        "Crisis_{t+5}=1 \\iff \\sum_{m\\in\\{EVT,Drawdown,Jump\\}} \\mathbb{1}\\{m_{t+1:t+5}=1\\} \\ge 2\n",
        "\\]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evt_tail_events(ret: pd.Series, q: float = 0.01) -> pd.Series:\n",
        "    \"\"\"Evento EVT simple: ret < quantile(q).\"\"\"\n",
        "    thr = ret.quantile(q)\n",
        "    return (ret < thr).astype(int)\n",
        "\n",
        "def drawdown_events(close: pd.Series, dd_threshold: float = -0.15) -> pd.Series:\n",
        "    \"\"\"Evento drawdown: drawdown <= dd_threshold.\"\"\"\n",
        "    peak = close.cummax()\n",
        "    dd = (close / peak) - 1.0\n",
        "    return (dd <= dd_threshold).astype(int)\n",
        "\n",
        "def bipower_variation(ret: pd.Series, window: int = 30) -> pd.Series:\n",
        "    r = ret.dropna()\n",
        "    abs_r = r.abs()\n",
        "    mu1 = np.sqrt(2/np.pi)\n",
        "    scaling = 1/(mu1**2)\n",
        "    contrib = abs_r * abs_r.shift(1)\n",
        "    return scaling * contrib.rolling(window).sum()\n",
        "\n",
        "def bns_jump_events(ret: pd.Series, window: int = 30, alpha: float = 0.01) -> pd.Series:\n",
        "    \"\"\"BNS jump test (simplificado) → evento jump si Z > z_alpha.\"\"\"\n",
        "    r = ret.dropna()\n",
        "    rv = (r**2).rolling(window).sum()\n",
        "    bv = bipower_variation(r, window)\n",
        "\n",
        "    mu1 = np.sqrt(2/np.pi)\n",
        "    theta = (np.pi**2/4 + np.pi - 5) * (mu1 ** -4)\n",
        "    abs_r = r.abs()\n",
        "    qp = (abs_r ** (4/3)).rolling(window).sum() ** 3\n",
        "    var_est = theta * np.maximum(qp - bv**2, 1e-12)\n",
        "\n",
        "    z = (rv - bv) / np.sqrt(var_est)\n",
        "    z_crit = stats.norm.ppf(1 - alpha)\n",
        "    return (z > z_crit).astype(int).reindex(ret.index).fillna(0).astype(int)\n",
        "\n",
        "def forward_any(x: pd.Series, horizon: int = 5) -> pd.Series:\n",
        "    \"\"\"Devuelve 1 en t si hay algún 1 en t+1..t+horizon.\"\"\"\n",
        "    arr = x.values\n",
        "    out = np.zeros_like(arr)\n",
        "    for i in range(len(arr)):\n",
        "        j0 = i+1\n",
        "        j1 = min(len(arr), i+1+horizon)\n",
        "        out[i] = 1 if (j0 < j1 and arr[j0:j1].max() > 0) else 0\n",
        "    return pd.Series(out, index=x.index)\n",
        "\n",
        "def crisis_ground_truth(ret: pd.Series, close: pd.Series, horizon: int = 5) -> pd.Series:\n",
        "    \"\"\"Tu definición C′: ≥2 de {EVT, Drawdown, Jump} en t+1..t+5.\"\"\"\n",
        "    evt = evt_tail_events(ret, q=0.01)\n",
        "    dd = drawdown_events(close, dd_threshold=-0.15)\n",
        "    jump = bns_jump_events(ret, window=30, alpha=0.01)\n",
        "\n",
        "    evt_f = forward_any(evt, horizon)\n",
        "    dd_f = forward_any(dd, horizon)\n",
        "    jump_f = forward_any(jump, horizon)\n",
        "\n",
        "    s = evt_f + dd_f + jump_f\n",
        "    return (s >= 2).astype(int)\n",
        "\n",
        "# demo en un activo\n",
        "k = list(proc.keys())[0]\n",
        "df0 = proc[k].copy()\n",
        "ret = df0['ret']\n",
        "close = df0['close']\n",
        "cr = crisis_ground_truth(ret, close, horizon=5)\n",
        "print(k, 'crisis positives:', int(cr.sum()), 'out of', len(cr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Pipeline por activo: \\(H_{Sh,z}\\), \\(S_{PLV}\\), crisis ground truth\n",
        "\n",
        "Para acelerar, calculamos sincronización en modo rolling con `step` y `n_surrogates` moderados.\n",
        "\n",
        "Puedes subir rigor aumentando:\n",
        "- `n_surrogates`\n",
        "- reduciendo `step`\n",
        "- ampliando lookback de sincronización\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rolling_plv(price: np.ndarray, lookback: int = 252, step: int = 10, n_surrogates: int = 25, alpha: float = 0.01) -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"Devuelve (S_plv, p_worst) alineados al índice de prices (longitud len(price)).\"\"\"\n",
        "    n = len(price)\n",
        "    s = np.full(n, np.nan)\n",
        "    p = np.full(n, np.nan)\n",
        "    for i in range(lookback, n, step):\n",
        "        w = price[i-lookback:i+1]\n",
        "        out = plv_surrogate_test(w, n_surrogates=n_surrogates, alpha=alpha)\n",
        "        s[i] = out['observed_plv']\n",
        "        p[i] = out['worst_p']\n",
        "        if i+step < n:\n",
        "            s[i:i+step] = s[i]\n",
        "            p[i:i+step] = p[i]\n",
        "    s = pd.Series(s).ffill().bfill()\n",
        "    p = pd.Series(p).ffill().bfill()\n",
        "    return s, p\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, df in proc.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    Hz = rolling_entropy_z(df['ret'], window=30)\n",
        "    crisis = crisis_ground_truth(df['ret'], df['close'], horizon=5)\n",
        "\n",
        "    # sincronización rolling (MUY costoso). Por defecto, se salta.\n",
        "    if RUN_PLV:\n",
        "        s_plv, p_worst = rolling_plv(df['close'].values, lookback=252, step=10, n_surrogates=20, alpha=0.01)\n",
        "    else:\n",
        "        s_plv = pd.Series(np.nan, index=np.arange(len(df)))\n",
        "        p_worst = pd.Series(np.nan, index=np.arange(len(df)))\n",
        "\n",
        "    out = df[['date','close','ret','vol']].copy().reset_index(drop=True)\n",
        "    out['Hz'] = Hz.values\n",
        "    out['S_plv'] = s_plv.values\n",
        "    out['p_worst'] = p_worst.values\n",
        "    out['crisis_t5'] = crisis.values\n",
        "\n",
        "    # stats\n",
        "    print('Hz mean/std:', float(np.nanmean(out['Hz'])), float(np.nanstd(out['Hz'])))\n",
        "    print('S_plv mean/std:', float(np.nanmean(out['S_plv'])), float(np.nanstd(out['S_plv'])))\n",
        "    print('Signif windows (%):', float((out['p_worst'] < 0.01).mean()*100))\n",
        "    print('Crisis_t5 positives:', int(out['crisis_t5'].sum()))\n",
        "\n",
        "    results[name] = out\n",
        "\n",
        "list(results.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Phase Space + Super-Criticality (Q1–Q4) con bootstrap CI\n",
        "\n",
        "Definimos umbrales por medianas:\n",
        "- High/Low entropy: vs mediana de \\(H_{Sh,z}\\)\n",
        "- High/Low sync: vs mediana de \\(S_{PLV}\\)\n",
        "\n",
        "Luego estimamos:\n",
        "\\[\n",
        "P(Crisis_{t+5}=1\\mid Q_k)\n",
        "\\]\n",
        "\n",
        "Y añadimos **bootstrap CI (95%)**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quadrant_probs(df: pd.DataFrame, hz_col='Hz', s_col='S_plv', y_col='crisis_t5'):\n",
        "    d = df[[hz_col, s_col, y_col]].dropna().copy()\n",
        "    h_thr = d[hz_col].median()\n",
        "    s_thr = d[s_col].median()\n",
        "\n",
        "    q = pd.Series(index=d.index, dtype=str)\n",
        "    q[(d[hz_col] >= h_thr) & (d[s_col] < s_thr)] = 'Q1'\n",
        "    q[(d[hz_col] >= h_thr) & (d[s_col] >= s_thr)] = 'Q2'\n",
        "    q[(d[hz_col] < h_thr) & (d[s_col] >= s_thr)] = 'Q3'\n",
        "    q[(d[hz_col] < h_thr) & (d[s_col] < s_thr)] = 'Q4'\n",
        "\n",
        "    out = {}\n",
        "    for qq in ['Q1','Q2','Q3','Q4']:\n",
        "        m = (q == qq)\n",
        "        n = int(m.sum())\n",
        "        k = int(d.loc[m, y_col].sum())\n",
        "        p = k/n if n>0 else np.nan\n",
        "        out[qq] = {'p': p, 'n': n, 'k': k}\n",
        "    return out, q, (h_thr, s_thr)\n",
        "\n",
        "def bootstrap_ci(df: pd.DataFrame, qmask: np.ndarray, y: np.ndarray, B: int = 500, alpha: float = 0.05):\n",
        "    idx = np.where(qmask)[0]\n",
        "    if len(idx) < 20:\n",
        "        return (np.nan, np.nan)\n",
        "    ps = []\n",
        "    for _ in range(B):\n",
        "        s = np.random.choice(idx, size=len(idx), replace=True)\n",
        "        ps.append(float(y[s].mean()))\n",
        "    lo = float(np.quantile(ps, alpha/2))\n",
        "    hi = float(np.quantile(ps, 1-alpha/2))\n",
        "    return lo, hi\n",
        "\n",
        "for name, df in results.items():\n",
        "    print(f\"\\n=== Quadrants: {name} ===\")\n",
        "    probs, q, (h_thr, s_thr) = quadrant_probs(df)\n",
        "    print('thresholds:', 'Hz=', round(h_thr,4), 'S=', round(s_thr,4))\n",
        "\n",
        "    d = df[['Hz','S_plv','crisis_t5']].dropna().copy()\n",
        "    y = d['crisis_t5'].values\n",
        "    # rebuild q for aligned d\n",
        "    probs, q, _ = quadrant_probs(df)\n",
        "    \n",
        "    for qq in ['Q1','Q2','Q3','Q4']:\n",
        "        # recompute mask on d\n",
        "        h_thr = d['Hz'].median(); s_thr = d['S_plv'].median()\n",
        "        if qq=='Q1': m = (d['Hz']>=h_thr) & (d['S_plv']<s_thr)\n",
        "        if qq=='Q2': m = (d['Hz']>=h_thr) & (d['S_plv']>=s_thr)\n",
        "        if qq=='Q3': m = (d['Hz']<h_thr) & (d['S_plv']>=s_thr)\n",
        "        if qq=='Q4': m = (d['Hz']<h_thr) & (d['S_plv']<s_thr)\n",
        "        p = float(d.loc[m,'crisis_t5'].mean()) if m.sum()>0 else np.nan\n",
        "        lo, hi = bootstrap_ci(d, m.values, d['crisis_t5'].values, B=300)\n",
        "        print(f\"{qq}: p={p:.4f}  n={int(m.sum())}  CI95=[{lo:.4f},{hi:.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Gráficos (ejemplo: SP500)\n",
        "\n",
        "Ajusta `asset` si quieres graficar otro.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "asset = 'SP500' if 'SP500' in results else list(results.keys())[0]\n",
        "df = results[asset].dropna().copy()\n",
        "\n",
        "fig, ax = plt.subplots(4,1, figsize=(14,10), sharex=True)\n",
        "ax[0].plot(df['date'], df['close'], color='black'); ax[0].set_title(f\"{asset} price\")\n",
        "ax[1].plot(df['date'], df['Hz'], color='purple'); ax[1].set_title(\"H_{Sh,z}\")\n",
        "ax[2].plot(df['date'], df['S_plv'], color='green'); ax[2].set_title(\"S_PLV\")\n",
        "ax[3].plot(df['date'], df['crisis_t5'], color='red'); ax[3].set_title(\"Crisis_{t+5} (ground truth)\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Phase space\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(df['Hz'], df['S_plv'], c=df['crisis_t5'], cmap='coolwarm', s=10, alpha=0.6)\n",
        "plt.xlabel('H_{Sh,z}'); plt.ylabel('S_PLV'); plt.title(f\"Phase space: {asset}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Hysteresis / Path-dependence tests (your claim)\n",
        "\n",
        "Vamos a probar formalmente tres cosas:\n",
        "\n",
        "### (A) Phase-space trajectory + loop area\n",
        "Calculamos el área firmada (trapezoidal) en el plano \n",
        "\\((H_t,S_t)\\) alrededor de eventos:\n",
        "\n",
        "\\[\n",
        "A=\\sum_{t=t_0}^{t_1-1}(H_{t+1}-H_t)\\,\\frac{S_{t+1}+S_t}{2}\n",
        "\\]\n",
        "\n",
        "Si hay histéresis consistente, la distribución de \\(A\\) alrededor de eventos debería ser sistemáticamente distinta de 0.\n",
        "\n",
        "### (B) Event-aligned curves (pre vs post)\n",
        "Promediamos \\(H\\) y \\(S\\) en tiempo-evento \\(k\\in[-K,K]\\) y medimos asimetría pre/post.\n",
        "\n",
        "### (C) “Release-before-rupture”\n",
        "Testea tu hipótesis específica:\n",
        "- peak local de \\(S\\) con \\(H\\) bajo\n",
        "- luego \\(S\\) cae (release)\n",
        "- y después ocurre \\(Crisis_{t+5}=1\\)\n",
        "\n",
        "**Controles:**\n",
        "- Usar solo puntos con \\(p_{worst}<0.01\\) (ventanas con coupling significativo)\n",
        "- Repetir con \\(S_\\perp\\) (residualizando \\(S\\) contra volatilidad \\(\\sigma\\))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy.linalg import lstsq\n",
        "\n",
        "def event_starts_from_groundtruth(crisis_t5: pd.Series, min_gap: int = 30) -> List[int]:\n",
        "    \"\"\"Indices where crisis_t5 flips 0→1, with a minimum gap between events.\"\"\"\n",
        "    x = crisis_t5.values.astype(int)\n",
        "    starts = np.where((x[1:] == 1) & (x[:-1] == 0))[0] + 1\n",
        "    if len(starts) == 0:\n",
        "        return []\n",
        "    filtered = [int(starts[0])]\n",
        "    for s in starts[1:]:\n",
        "        if int(s) - filtered[-1] >= min_gap:\n",
        "            filtered.append(int(s))\n",
        "    return filtered\n",
        "\n",
        "def loop_area(H: np.ndarray, S: np.ndarray) -> float:\n",
        "    \"\"\"Signed area proxy via trapezoids in HS plane.\"\"\"\n",
        "    H = np.asarray(H); S = np.asarray(S)\n",
        "    dH = np.diff(H)\n",
        "    Sm = (S[1:] + S[:-1]) / 2.0\n",
        "    return float(np.nansum(dH * Sm))\n",
        "\n",
        "def residualize_S_against_vol(S: pd.Series, vol: pd.Series) -> pd.Series:\n",
        "    \"\"\"S_perp = S - beta*vol (simple control).\"\"\"\n",
        "    d = pd.DataFrame({'S': S, 'vol': vol}).dropna()\n",
        "    if len(d) < 50:\n",
        "        return S * np.nan\n",
        "    X = np.column_stack([np.ones(len(d)), d['vol'].values])\n",
        "    y = d['S'].values\n",
        "    beta, *_ = lstsq(X, y, rcond=None)\n",
        "    yhat = X @ beta\n",
        "    resid = y - yhat\n",
        "    out = pd.Series(index=S.index, dtype=float)\n",
        "    out.loc[d.index] = resid\n",
        "    return out\n",
        "\n",
        "def event_aligned_means(series: pd.Series, events: List[int], K: int = 60) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Return k-grid and mean curve for k in [-K,K].\"\"\"\n",
        "    vals = []\n",
        "    arr = series.values\n",
        "    for e in events:\n",
        "        if e-K < 0 or e+K >= len(arr):\n",
        "            continue\n",
        "        vals.append(arr[e-K:e+K+1])\n",
        "    if len(vals) == 0:\n",
        "        return np.arange(-K, K+1), np.full(2*K+1, np.nan)\n",
        "    M = np.vstack(vals)\n",
        "    return np.arange(-K, K+1), np.nanmean(M, axis=0)\n",
        "\n",
        "def asymmetry(curve: np.ndarray, K: int) -> float:\n",
        "    \"\"\"Delta = sum_{k=1..K} post - sum_{k=1..K} pre.\"\"\"\n",
        "    pre = np.nansum(curve[:K])\n",
        "    post = np.nansum(curve[K+1:])\n",
        "    return float(post - pre)\n",
        "\n",
        "def local_peaks(x: np.ndarray, m: int = 10) -> np.ndarray:\n",
        "    \"\"\"Boolean mask of local maxima in +/- m window.\"\"\"\n",
        "    x = np.asarray(x)\n",
        "    peaks = np.zeros_like(x, dtype=bool)\n",
        "    for i in range(m, len(x)-m):\n",
        "        w = x[i-m:i+m+1]\n",
        "        if np.isfinite(x[i]) and x[i] == np.nanmax(w):\n",
        "            peaks[i] = True\n",
        "    return peaks\n",
        "\n",
        "def release_before_rupture(df: pd.DataFrame, m: int = 10, qS: float = 0.80, qH: float = 0.40, L: int = 20, delta: float = 0.02,\n",
        "                           use_resid_S: bool = False, require_sig: bool = True) -> Dict:\n",
        "    \"\"\"Test P(Crisis|Peak(S)&H low&Release) vs baseline.\"\"\"\n",
        "    d = df[['Hz','S_plv','p_worst','vol','crisis_t5']].dropna().copy()\n",
        "    if len(d) < 500:\n",
        "        return {'n': 0}\n",
        "\n",
        "    S = d['S_plv'].values\n",
        "    if use_resid_S:\n",
        "        S_perp = residualize_S_against_vol(d['S_plv'], d['vol']).loc[d.index].values\n",
        "        S = S_perp\n",
        "\n",
        "    H = d['Hz'].values\n",
        "    Y = d['crisis_t5'].values.astype(int)\n",
        "\n",
        "    if require_sig:\n",
        "        sig_mask = (d['p_worst'].values < 0.01)\n",
        "    else:\n",
        "        sig_mask = np.ones(len(d), dtype=bool)\n",
        "\n",
        "    S_thr = np.nanquantile(S[sig_mask], qS)\n",
        "    H_thr = np.nanquantile(H[sig_mask], qH)\n",
        "\n",
        "    pk = local_peaks(S, m=m)\n",
        "    lowH = H < H_thr\n",
        "    highS = S > S_thr\n",
        "\n",
        "    # release: within L days, S drops by at least delta\n",
        "    rel = np.zeros(len(S), dtype=bool)\n",
        "    for i in range(len(S)):\n",
        "        j1 = min(len(S), i+L+1)\n",
        "        if i+1 < j1 and np.nanmin(S[i+1:j1]) < (S[i] - delta):\n",
        "            rel[i] = True\n",
        "\n",
        "    cond = pk & lowH & highS & rel & sig_mask\n",
        "\n",
        "    baseline = float(np.mean(Y))\n",
        "    p_cond = float(np.mean(Y[cond])) if cond.sum() > 0 else np.nan\n",
        "\n",
        "    return {\n",
        "        'baseline': baseline,\n",
        "        'p_cond': p_cond,\n",
        "        'n_cond': int(cond.sum()),\n",
        "        'S_thr': float(S_thr),\n",
        "        'H_thr': float(H_thr)\n",
        "    }\n",
        "\n",
        "# Run hysteresis suite for SP500\n",
        "asset = 'SP500' if 'SP500' in results else list(results.keys())[0]\n",
        "df = results[asset].dropna().reset_index(drop=True)\n",
        "\n",
        "# Use only significant windows if you want (p_worst < 0.01)\n",
        "mask_sig = (df['p_worst'] < 0.01)\n",
        "print('Significant windows %:', float(mask_sig.mean()*100))\n",
        "\n",
        "# Define events from Crisis_t5\n",
        "events = event_starts_from_groundtruth(df['crisis_t5'], min_gap=30)\n",
        "print('Event starts:', len(events))\n",
        "\n",
        "# Loop areas around each event\n",
        "K = 60\n",
        "areas = []\n",
        "for e in events:\n",
        "    if e-K < 0 or e+K >= len(df):\n",
        "        continue\n",
        "    seg = df.iloc[e-K:e+K+1]\n",
        "    areas.append(loop_area(seg['Hz'].values, seg['S_plv'].values))\n",
        "\n",
        "areas = np.asarray(areas)\n",
        "print('Loop area A: mean=', float(np.nanmean(areas)), 'median=', float(np.nanmedian(areas)), 'n=', int(np.isfinite(areas).sum()))\n",
        "\n",
        "# Event-aligned curves\n",
        "kgrid, Hbar = event_aligned_means(df['Hz'], events, K=K)\n",
        "kgrid, Sbar = event_aligned_means(df['S_plv'], events, K=K)\n",
        "\n",
        "print('Asymmetry ΔH:', asymmetry(Hbar, K))\n",
        "print('Asymmetry ΔS:', asymmetry(Sbar, K))\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(kgrid, Hbar, label='H (mean)')\n",
        "plt.axvline(0, color='k', linestyle='--')\n",
        "plt.title(f'Event-aligned H around Crisis_{t+5} starts — {asset}')\n",
        "plt.legend(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(kgrid, Sbar, label='S_PLV (mean)', color='green')\n",
        "plt.axvline(0, color='k', linestyle='--')\n",
        "plt.title(f'Event-aligned S_PLV around Crisis_{t+5} starts — {asset}')\n",
        "plt.legend(); plt.show()\n",
        "\n",
        "# Release-before-rupture test\n",
        "r1 = release_before_rupture(df, use_resid_S=False, require_sig=True)\n",
        "r2 = release_before_rupture(df, use_resid_S=True, require_sig=True)\n",
        "print('\\nRelease-before-rupture (raw S):', r1)\n",
        "print('Release-before-rupture (S residualized vs vol):', r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) CARIA-SR (Structural): sincronización espectral cross-sectional + memoria (histeresis)\n",
        "\n",
        "**Motivación:** PLV multi-escala dentro de una sola serie puede ser frágil en datos diarios. Para capturar la idea de *crowding / colapso de grados de libertad* de forma más robusta, medimos la estructura de la **matriz de correlación** entre muchos activos.\n",
        "\n",
        "### Métricas\n",
        "- **Absorption Ratio (AR)**: fracción de varianza explicada por los primeros componentes (eigenvalues)\n",
        "- **Eigen-Entropy**: diversidad estructural de los eigenvalues\n",
        "- **CARIA-SR**: combinación estandarizada de **AR alto** + **(1 − entropy) alto**\n",
        "- **Peak Memory**: máximo rolling de CARIA-SR en horizonte H (plasticidad)\n",
        "\n",
        "### Deep Calm\n",
        "Usamos **VIX** para definir calma: `VIX < 15/18/20` y evaluamos si Peak Memory explica cola izquierda futura.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# === 11A) Load cross-sectional SP500 panel (local parquet) + VIX ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "DATA_PATH = \"data/sp500_universe_fmp.parquet\"  # repo path; in Colab upload/mount to match\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(\n",
        "        f\"No encuentro {DATA_PATH}.\\n\"\n",
        "        \"En Colab: sube el parquet a /content/data/ o monta Google Drive y ajusta DATA_PATH.\"\n",
        "    )\n",
        "\n",
        "# Load wide panel: index=date, columns=tickers (prices)\n",
        "px = pd.read_parquet(DATA_PATH)\n",
        "if \"date\" in px.columns:\n",
        "    px[\"date\"] = pd.to_datetime(px[\"date\"])\n",
        "    px = px.set_index(\"date\")\n",
        "px.index = pd.to_datetime(px.index)\n",
        "px = px.sort_index()\n",
        "\n",
        "# Use 2000+\n",
        "px = px.loc[\"2000-01-01\":].copy()\n",
        "\n",
        "# Log-returns\n",
        "ret = np.log(px).diff()\n",
        "\n",
        "# Stable universe selection (coverage)\n",
        "coverage = 1.0 - ret.isna().mean()\n",
        "COVERAGE_MIN = 0.90\n",
        "keep = coverage[coverage >= COVERAGE_MIN].index.tolist()\n",
        "ret = ret[keep]\n",
        "\n",
        "print(\"Panel dates:\", ret.index.min().date(), \"→\", ret.index.max().date())\n",
        "print(\"Universe size (coverage>=\", COVERAGE_MIN, \"):\", ret.shape[1])\n",
        "\n",
        "# VIX from yfinance (robust)\n",
        "START = str(ret.index.min().date())\n",
        "END = str(ret.index.max().date() + pd.Timedelta(days=1))\n",
        "vix_df = yf.download(\"^VIX\", start=START, end=END, progress=False, auto_adjust=False)\n",
        "if vix_df.empty:\n",
        "    raise RuntimeError(\"No pude bajar ^VIX con yfinance.\")\n",
        "vix = vix_df[\"Adj Close\"].rename(\"VIX\").dropna()\n",
        "\n",
        "# Align indices\n",
        "common_idx = ret.index.intersection(vix.index)\n",
        "ret_cs = ret.loc[common_idx].copy()\n",
        "vix_cs = vix.loc[common_idx].copy()\n",
        "\n",
        "print(\"Aligned:\", ret_cs.shape, \"VIX:\", vix_cs.shape, \"from\", common_idx.min().date(), \"to\", common_idx.max().date())\n",
        "\n",
        "# Target series for tail-risk (prefer SPY if available)\n",
        "if \"proc\" in globals() and \"SP500\" in proc:\n",
        "    spy = proc[\"SP500\"].copy().set_index(\"date\").sort_index()\n",
        "    spy_close = spy[\"close\"].reindex(common_idx).astype(float)\n",
        "    spy_ret = np.log(spy_close).diff().rename(\"spy_ret\")\n",
        "    print(\"Using SPY from FMP download as target\")\n",
        "else:\n",
        "    spy_df = yf.download(\"SPY\", start=START, end=END, progress=False, auto_adjust=False)\n",
        "    spy_close = spy_df[\"Adj Close\"].rename(\"close\").reindex(common_idx).astype(float)\n",
        "    spy_ret = np.log(spy_close).diff().rename(\"spy_ret\")\n",
        "    print(\"Using SPY from yfinance as target\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# === 11B) CARIA-SR structural metrics + Peak Memory + Deep Calm robustness ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "def eig_metrics_from_corr(C: np.ndarray, k_frac: float = 0.2):\n",
        "    w = np.linalg.eigvalsh(C)\n",
        "    w = np.sort(w)[::-1]\n",
        "    n = len(w)\n",
        "    k = max(1, int(np.ceil(k_frac * n)))\n",
        "    ar = float(np.sum(w[:k]) / np.sum(w))\n",
        "    p = w / np.sum(w)\n",
        "    p = p[p > 0]\n",
        "    ent = -np.sum(p * np.log(p))\n",
        "    ent_norm = float(ent / np.log(n)) if n > 1 else np.nan\n",
        "    return ar, ent_norm\n",
        "\n",
        "def rolling_structural_metrics(ret_mat: pd.DataFrame, window: int = 252, k_frac: float = 0.2, min_assets: int = 120, step: int = 5):\n",
        "    idx = ret_mat.index\n",
        "    out = pd.DataFrame(index=idx, columns=[\"AR\", \"E_eig\", \"N_assets\"], dtype=float)\n",
        "\n",
        "    for t in range(window, len(idx), step):\n",
        "        W = ret_mat.iloc[t-window+1:t+1]\n",
        "        good = W.notna().mean() >= 0.90\n",
        "        W = W.loc[:, good]\n",
        "        if W.shape[1] < min_assets:\n",
        "            continue\n",
        "        W = W.apply(lambda s: s.fillna(s.mean()), axis=0)\n",
        "        C = np.corrcoef(W.values, rowvar=False)\n",
        "        C = np.nan_to_num(C, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        ar, ent = eig_metrics_from_corr(C, k_frac=k_frac)\n",
        "        out.iloc[t] = [ar, ent, W.shape[1]]\n",
        "\n",
        "    # forward-fill to daily index\n",
        "    out = out.ffill().bfill()\n",
        "    return out\n",
        "\n",
        "def zscore(s: pd.Series, w: int = 252):\n",
        "    mu = s.rolling(w).mean()\n",
        "    sd = s.rolling(w).std().replace(0, np.nan)\n",
        "    return (s - mu) / sd\n",
        "\n",
        "def forward_min(x: pd.Series, h: int = 22) -> pd.Series:\n",
        "    a = x.to_numpy()\n",
        "    out = np.full(len(a), np.nan)\n",
        "    for i in range(len(a)):\n",
        "        j0, j1 = i+1, min(len(a), i+1+h)\n",
        "        if j0 < j1:\n",
        "            out[i] = np.nanmin(a[j0:j1])\n",
        "    return pd.Series(out, index=x.index)\n",
        "\n",
        "# Compute structural metrics (fast via step)\n",
        "struct = rolling_structural_metrics(ret_cs, window=252, k_frac=0.2, min_assets=120, step=5)\n",
        "struct[\"AR_z\"] = zscore(struct[\"AR\"], 252)\n",
        "struct[\"E_low_z\"] = zscore(1.0 - struct[\"E_eig\"], 252)\n",
        "struct[\"CARIA_SR\"] = struct[\"AR_z\"] + struct[\"E_low_z\"]\n",
        "\n",
        "# Peak memory horizons\n",
        "H_list = [20, 40, 60, 90, 120]\n",
        "for H in H_list:\n",
        "    struct[f\"Peak{H}\"] = struct[\"CARIA_SR\"].rolling(H).max()\n",
        "\n",
        "# Target outcomes on SPY\n",
        "spy_ret_al = spy_ret.reindex(struct.index).astype(float)\n",
        "spy_close_al = spy_close.reindex(struct.index).astype(float)\n",
        "\n",
        "y = forward_min(spy_ret_al, h=22).rename(\"future_min_22d\")\n",
        "crisis_t5 = crisis_ground_truth(spy_ret_al, spy_close_al, horizon=5).rename(\"crisis_t5\")\n",
        "\n",
        "# Plot Peak60 vs VIX (quick sanity)\n",
        "plt.figure(figsize=(14,5))\n",
        "ax1 = plt.gca()\n",
        "ax1.plot(struct.index, struct[\"Peak60\"], color=\"crimson\", lw=2, label=\"Peak60 (Caria-SR memory)\")\n",
        "ax1.set_ylabel(\"Peak60 (z units)\")\n",
        "ax1.legend(loc=\"upper left\")\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(vix_cs.index, vix_cs.values, color=\"grey\", alpha=0.5, lw=1.5, label=\"VIX\")\n",
        "ax2.axhline(15, color=\"grey\", ls=\"--\", lw=1)\n",
        "ax2.axhline(18, color=\"grey\", ls=\":\", lw=1)\n",
        "ax2.axhline(20, color=\"grey\", ls=\"-.\", lw=1)\n",
        "ax2.set_ylabel(\"VIX\")\n",
        "plt.title(\"Structural Memory (Peak60) vs VIX\")\n",
        "plt.show()\n",
        "\n",
        "# Δq05 heatmap: HIGH PeakMemory (top20%) − LOW\n",
        "thr_list = [15, 18, 20, 22, 25]\n",
        "heat_q = pd.DataFrame(index=H_list, columns=thr_list, dtype=float)\n",
        "heat_n = pd.DataFrame(index=H_list, columns=thr_list, dtype=float)\n",
        "\n",
        "for H in H_list:\n",
        "    peakH = struct[f\"Peak{H}\"]\n",
        "    for thr in thr_list:\n",
        "        calm = (vix_cs < thr).reindex(struct.index).fillna(False)\n",
        "        df = pd.concat([y, peakH.rename(\"peak\")], axis=1).loc[calm].dropna()\n",
        "        heat_n.loc[H, thr] = float(len(df))\n",
        "        if len(df) < 400:\n",
        "            heat_q.loc[H, thr] = np.nan\n",
        "            continue\n",
        "        cut = df[\"peak\"].quantile(0.80)\n",
        "        hi = df[df[\"peak\"] >= cut][\"future_min_22d\"]\n",
        "        lo = df[df[\"peak\"] < cut][\"future_min_22d\"]\n",
        "        heat_q.loc[H, thr] = float(hi.quantile(0.05) - lo.quantile(0.05))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(heat_q.astype(float), annot=True, fmt=\".4f\", cmap=\"RdYlGn\", center=0)\n",
        "plt.title(\"Δq05(future_min_22d): HIGH PeakMemory − LOW (top20% vs rest)\\nNegative = worse tail under structural memory\")\n",
        "plt.xlabel(\"Deep Calm Threshold (VIX < X)\")\n",
        "plt.ylabel(\"Memory Window H (days)\")\n",
        "plt.show()\n",
        "\n",
        "print(\"n per cell (Deep Calm samples):\")\n",
        "display(heat_n)\n",
        "\n",
        "# Optional: crisis probability difference in Deep Calm for Peak60 high vs low\n",
        "for thr in [15, 18, 20]:\n",
        "    calm = (vix_cs < thr).reindex(struct.index).fillna(False)\n",
        "    df = pd.concat([crisis_t5, struct[\"Peak60\"].rename(\"peak\")], axis=1).loc[calm].dropna()\n",
        "    if len(df) < 400:\n",
        "        continue\n",
        "    cut = df[\"peak\"].quantile(0.80)\n",
        "    p_hi = float(df[df[\"peak\"] >= cut][\"crisis_t5\"].mean())\n",
        "    p_lo = float(df[df[\"peak\"] < cut][\"crisis_t5\"].mean())\n",
        "    print(f\"Deep Calm VIX<{thr}: P(crisis_t+5) highPeak={p_hi:.4f} lowPeak={p_lo:.4f} Δ={p_hi-p_lo:+.4f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
