{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# ðŸ”¬ CARIA-SR Hysteresis Validation\n",
                "**\"Hysteresis in Financial Fragility: Structural Memory and Tail Risk in Low-Volatility Regimes\"**\n",
                "\n",
                "Phases:\n",
                "- **8**: Quantile Regression + Peak Memory\n",
                "- **9**: Robustness Heatmap\n",
                "- **12**: Minsky Hedge Backtest\n",
                "- **13**: Structural Alpha Grid\n",
                "- **14**: Bootstrap CI + Sharpe/Sortino\n",
                "\n",
                "**FMP API**: `79fY9wvC9qtCJHcn6Yelf4ilE9TkRMoq`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "# @title 1. Setup\n",
                "!pip install -q yfinance pandas numpy scipy scikit-learn statsmodels seaborn matplotlib pyarrow requests\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import yfinance as yf\n",
                "import requests\n",
                "import warnings\n",
                "from datetime import datetime\n",
                "import statsmodels.formula.api as smf\n",
                "from sklearn.covariance import LedoitWolf\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "np.random.seed(42)\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "# Directories\n",
                "WORK_DIR = '/content/drive/MyDrive/CARIA'\n",
                "os.makedirs(WORK_DIR, exist_ok=True)\n",
                "os.makedirs(f'{WORK_DIR}/figures', exist_ok=True)\n",
                "os.makedirs(f'{WORK_DIR}/tables', exist_ok=True)\n",
                "\n",
                "FMP_API_KEY = \"79fY9wvC9qtCJHcn6Yelf4ilE9TkRMoq\"\n",
                "START_DATE, END_DATE = \"2000-01-01\", datetime.now().strftime(\"%Y-%m-%d\")\n",
                "print(f\"âœ… Output: {WORK_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download"
            },
            "outputs": [],
            "source": [
                "# @title 2. Download Data (S&P 500 + Market)\n",
                "\n",
                "# Get S&P 500 tickers\n",
                "url = f\"https://financialmodelingprep.com/api/v3/sp500_constituent?apikey={FMP_API_KEY}\"\n",
                "resp = requests.get(url)\n",
                "sp500_tickers = [x['symbol'] for x in resp.json()] if resp.status_code == 200 else []\n",
                "print(f\"Downloading {len(sp500_tickers)} stocks...\")\n",
                "\n",
                "# Download in batches\n",
                "all_prices = []\n",
                "for i in range(0, len(sp500_tickers), 50):\n",
                "    batch = sp500_tickers[i:i+50]\n",
                "    try:\n",
                "        data = yf.download(batch, start=START_DATE, end=END_DATE, progress=False, auto_adjust=True)['Close']\n",
                "        all_prices.append(data)\n",
                "        print(f\"   Batch {i//50 + 1}/{(len(sp500_tickers)-1)//50 + 1}\")\n",
                "    except Exception as e:\n",
                "        print(f\"   Error batch {i//50 + 1}: {e}\")\n",
                "\n",
                "prices = pd.concat(all_prices, axis=1).dropna(axis=1, how='all')\n",
                "prices.to_csv(f'{WORK_DIR}/sp500_prices.csv')\n",
                "\n",
                "# Market data (VIX, SPY, TLT, 10Y Treasury)\n",
                "print(\"Downloading market data...\")\n",
                "market = yf.download(['^VIX', 'SPY', 'TLT', '^TNX'], start=START_DATE, end=END_DATE, progress=False)\n",
                "\n",
                "market_df = pd.DataFrame({\n",
                "    'volatility': market['Close']['^VIX'],\n",
                "    'price': market['Close']['SPY'],\n",
                "    'tlt': market['Close']['TLT'],\n",
                "    'treasury_10y': market['Close']['^TNX']\n",
                "}).dropna()\n",
                "market_df.index.name = 'Date'\n",
                "market_df.to_csv(f'{WORK_DIR}/market_validation_data.csv')\n",
                "\n",
                "print(f\"\\nâœ… Prices: {prices.shape[1]} stocks, {len(prices)} days\")\n",
                "print(f\"âœ… Market: {len(market_df)} days\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "structural"
            },
            "outputs": [],
            "source": [
                "# @title 3. Calculate Structural Metrics (AR + Entropy) â³ ~15 min\n",
                "\n",
                "def cov_to_corr(S):\n",
                "    d = np.sqrt(np.diag(S))\n",
                "    d = np.where(d == 0, 1e-10, d)\n",
                "    C = S / np.outer(d, d)\n",
                "    return np.nan_to_num((C + C.T) / 2)\n",
                "\n",
                "def eig_metrics(C, k_frac=0.2):\n",
                "    w = np.sort(np.linalg.eigvalsh(C))[::-1]\n",
                "    w = np.maximum(w, 1e-10)  # Avoid negative eigenvalues\n",
                "    k = max(1, int(np.ceil(k_frac * len(w))))\n",
                "    ar = np.sum(w[:k]) / np.sum(w)\n",
                "    p = w / np.sum(w)\n",
                "    ent = -np.sum(p * np.log(p + 1e-10)) / np.log(len(w)) if len(w) > 1 else 0.5\n",
                "    return float(ar), float(ent)\n",
                "\n",
                "# Calculate returns\n",
                "returns = np.log(prices).diff()\n",
                "good_coverage = returns.notna().mean() >= 0.9\n",
                "returns = returns.loc[:, good_coverage]\n",
                "print(f\"Using {returns.shape[1]} stocks with >90% coverage\")\n",
                "\n",
                "# Rolling structural metrics\n",
                "window = 252\n",
                "step = 5\n",
                "lw = LedoitWolf()\n",
                "\n",
                "struct = pd.DataFrame(index=returns.index, columns=['absorption_ratio', 'entropy'], dtype=float)\n",
                "\n",
                "total_steps = (len(returns) - window) // step\n",
                "for idx, t in enumerate(range(window, len(returns), step)):\n",
                "    W = returns.iloc[t-window:t]\n",
                "    W = W.loc[:, W.notna().mean() >= 0.9]\n",
                "    if W.shape[1] < 100:\n",
                "        continue\n",
                "    W = W.apply(lambda s: s.fillna(s.mean()))\n",
                "    X = W.values - np.nanmean(W.values, axis=0)\n",
                "    try:\n",
                "        C = cov_to_corr(lw.fit(X).covariance_)\n",
                "    except:\n",
                "        C = np.corrcoef(X, rowvar=False)\n",
                "        C = np.nan_to_num((C + C.T) / 2)\n",
                "    ar, ent = eig_metrics(C)\n",
                "    struct.iloc[t] = [ar, ent]\n",
                "    if (idx + 1) % 100 == 0:\n",
                "        print(f\"   {idx + 1}/{total_steps} ({(idx+1)/total_steps*100:.0f}%)\")\n",
                "\n",
                "struct = struct.ffill().bfill()\n",
                "struct.index.name = 'date'\n",
                "struct.to_csv(f'{WORK_DIR}/caria_structural_metrics.csv')\n",
                "\n",
                "print(f\"\\nâœ… Structural metrics saved\")\n",
                "print(f\"   AR mean: {struct['absorption_ratio'].mean():.4f}\")\n",
                "print(f\"   Entropy mean: {struct['entropy'].mean():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "phase8"
            },
            "outputs": [],
            "source": [
                "# @title Phase 8: The \"Regime & Memory\" Test (Fixing the Signal)\n",
                "\n",
                "print(\"Loading and merging data for Phase 8...\")\n",
                "\n",
                "# Load data\n",
                "struct_df = pd.read_csv(f'{WORK_DIR}/caria_structural_metrics.csv', index_col='date', parse_dates=True)\n",
                "market_df = pd.read_csv(f'{WORK_DIR}/market_validation_data.csv', index_col='Date', parse_dates=True)\n",
                "\n",
                "# Merge\n",
                "df = struct_df.join(market_df, how='inner').sort_index()\n",
                "\n",
                "# Calculate Z-Scores and Memory\n",
                "window_z = 252\n",
                "window_memory = 60\n",
                "\n",
                "rolling_mean = df['absorption_ratio'].rolling(window=window_z).mean()\n",
                "rolling_std = df['absorption_ratio'].rolling(window=window_z).std()\n",
                "df['absorp_z'] = (df['absorption_ratio'] - rolling_mean) / rolling_std\n",
                "\n",
                "# Peak Memory (THE KEY INNOVATION)\n",
                "df['caria_peak'] = df['absorp_z'].rolling(window=window_memory).max()\n",
                "\n",
                "df = df.dropna()\n",
                "print(f\"Data prepared. Shape: {df.shape}\")\n",
                "\n",
                "# --- REGIME FILTERING ---\n",
                "low_vol_df = df[df['volatility'] < 20].copy()\n",
                "low_vol_df['future_ret_22'] = low_vol_df['price'].pct_change(22).shift(-22)\n",
                "low_vol_df = low_vol_df.dropna()\n",
                "\n",
                "print(f\"Testing on 'Calm Markets' (VIX < 20). Observations: {len(low_vol_df)}\")\n",
                "\n",
                "# --- QUANTILE REGRESSION ---\n",
                "print(\"\\nRunning Quantile Regression on 'Hidden Risks'...\")\n",
                "\n",
                "mod_vix = smf.quantreg('future_ret_22 ~ volatility', low_vol_df)\n",
                "res_vix = mod_vix.fit(q=0.05)\n",
                "\n",
                "mod_struct = smf.quantreg('future_ret_22 ~ volatility + caria_peak', low_vol_df)\n",
                "res_struct = mod_struct.fit(q=0.05)\n",
                "\n",
                "print(res_struct.summary())\n",
                "\n",
                "print(f\"\\nBase Model (VIX Only) Pseudo RÂ²:      {res_vix.prsquared:.5f}\")\n",
                "print(f\"Structural Model (+Peak) Pseudo RÂ²:   {res_struct.prsquared:.5f}\")\n",
                "imp = ((res_struct.prsquared - res_vix.prsquared)/res_vix.prsquared)*100\n",
                "print(f\"ðŸ”¥ Improvement in Low-Vol Regime:     {imp:.1f}%\")\n",
                "\n",
                "# --- VISUALIZATION ---\n",
                "subset = df.loc['2019-01-01':'2020-06-01']\n",
                "\n",
                "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
                "ax1.plot(subset.index, subset['volatility'], color='gray', linestyle='--', label='VIX')\n",
                "ax1.set_ylabel('VIX', color='gray')\n",
                "\n",
                "ax2 = ax1.twinx()\n",
                "ax2.plot(subset.index, subset['absorp_z'], color='salmon', alpha=0.5, label='Original Signal (Fades)')\n",
                "ax2.plot(subset.index, subset['caria_peak'], color='darkred', linewidth=3, label='Peak Signal (Memory)')\n",
                "ax2.set_ylabel('Structure (Z-Score)', color='darkred')\n",
                "\n",
                "plt.title('Fixing the \"Fall\": Using Peak Memory to Sustain the Warning')\n",
                "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{WORK_DIR}/figures/Figure_PeakMemory.png', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "phase9"
            },
            "outputs": [],
            "source": [
                "# @title Phase 9: Robustness Heatmap (Sensitivity Analysis)\n",
                "\n",
                "windows = [20, 40, 60, 90, 120]\n",
                "vix_caps = [15, 18, 20, 22, 25]\n",
                "results_matrix = np.zeros((len(windows), len(vix_caps)))\n",
                "\n",
                "print(\"Running Sensitivity Grid...\")\n",
                "\n",
                "# Pre-compute peak signals for all windows\n",
                "for w in windows:\n",
                "    df[f'peak_{w}'] = df['absorp_z'].rolling(window=w).max()\n",
                "\n",
                "for i, w in enumerate(windows):\n",
                "    for j, v in enumerate(vix_caps):\n",
                "        subset = df[df['volatility'] < v].copy()\n",
                "        subset['ret_future'] = subset['price'].pct_change(22).shift(-22)\n",
                "        subset = subset.dropna()\n",
                "        \n",
                "        if len(subset) > 500:\n",
                "            try:\n",
                "                mod_base = smf.quantreg('ret_future ~ volatility', subset)\n",
                "                res_base = mod_base.fit(q=0.05)\n",
                "                \n",
                "                mod_struct = smf.quantreg(f'ret_future ~ volatility + peak_{w}', subset)\n",
                "                res_struct = mod_struct.fit(q=0.05)\n",
                "                \n",
                "                imp = ((res_struct.prsquared - res_base.prsquared)/res_base.prsquared) * 100\n",
                "                results_matrix[i, j] = imp\n",
                "            except:\n",
                "                results_matrix[i, j] = 0\n",
                "        else:\n",
                "            results_matrix[i, j] = 0\n",
                "\n",
                "# Save table\n",
                "sensitivity_df = pd.DataFrame(results_matrix, index=windows, columns=vix_caps)\n",
                "sensitivity_df.to_csv(f'{WORK_DIR}/tables/Table_Sensitivity.csv')\n",
                "\n",
                "# Visualization\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(results_matrix, annot=True, fmt=\".1f\", cmap=\"RdYlGn\", \n",
                "            xticklabels=vix_caps, yticklabels=windows)\n",
                "plt.title(\"Robustness Check: Improvement in Tail Risk Prediction (%)\\n(Y=Memory Window, X=VIX Threshold)\")\n",
                "plt.xlabel(\"Low-Volatility Threshold (VIX < X)\")\n",
                "plt.ylabel(\"Memory Window (Days)\")\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{WORK_DIR}/figures/Figure_RobustnessHeatmap.png', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "phase12"
            },
            "outputs": [],
            "source": [
                "# @title Phase 12: The \"Smart Hedge\" (SPY â†” TLT Switching)\n",
                "\n",
                "print(\"Running 'Smart Switch' Backtest...\")\n",
                "\n",
                "def get_max_drawdown(equity):\n",
                "    peak = equity.cummax()\n",
                "    dd = equity / peak - 1.0\n",
                "    return float(dd.min())\n",
                "\n",
                "# Prepare backtest dataframe\n",
                "backtest_df = df.copy()\n",
                "backtest_df['daily_ret'] = np.log(backtest_df['price']).diff()\n",
                "backtest_df['tlt_ret'] = np.log(backtest_df['tlt']).diff()\n",
                "\n",
                "# Treasury daily return from yield\n",
                "backtest_df['treasury_daily_ret'] = (backtest_df['treasury_10y'] / 100) / 252\n",
                "\n",
                "# Unsafe state: Low VIX but high structural stress (Peak Z > 1.5)\n",
                "peak_z = (backtest_df['caria_peak'] - backtest_df['caria_peak'].rolling(252).mean()) / backtest_df['caria_peak'].rolling(252).std()\n",
                "backtest_df['unsafe_state'] = (backtest_df['volatility'] < 20) & (peak_z > 1.5)\n",
                "\n",
                "# Strategy returns\n",
                "backtest_df['smart_ret'] = np.where(backtest_df['unsafe_state'], \n",
                "                                    backtest_df['treasury_daily_ret'], \n",
                "                                    backtest_df['daily_ret'])\n",
                "\n",
                "leverage = 1.5\n",
                "backtest_df['lev_ret'] = np.where(backtest_df['unsafe_state'], \n",
                "                                  backtest_df['treasury_daily_ret'],\n",
                "                                  backtest_df['daily_ret'] * leverage - (0.05/252 * (leverage-1)))\n",
                "\n",
                "# Cumulative returns\n",
                "backtest_df['cum_bnh'] = (1 + backtest_df['daily_ret'].fillna(0)).cumprod()\n",
                "backtest_df['cum_smart'] = (1 + backtest_df['smart_ret'].fillna(0)).cumprod()\n",
                "backtest_df['cum_lev'] = (1 + backtest_df['lev_ret'].fillna(0)).cumprod()\n",
                "\n",
                "# Performance metrics\n",
                "years = len(backtest_df) / 252\n",
                "\n",
                "dd_bnh = get_max_drawdown(backtest_df['cum_bnh'])\n",
                "cagr_bnh = (backtest_df['cum_bnh'].iloc[-1])**(1/years) - 1\n",
                "\n",
                "dd_smart = get_max_drawdown(backtest_df['cum_smart'])\n",
                "cagr_smart = (backtest_df['cum_smart'].iloc[-1])**(1/years) - 1\n",
                "\n",
                "dd_lev = get_max_drawdown(backtest_df['cum_lev'])\n",
                "cagr_lev = (backtest_df['cum_lev'].iloc[-1])**(1/years) - 1\n",
                "\n",
                "print(f\"\\n--- RESULTS ---\")\n",
                "print(f\"Benchmark (S&P 500):    DD = {dd_bnh:.1%}, CAGR = {cagr_bnh:.1%}\")\n",
                "print(f\"Minsky (Smart/Bond):    DD = {dd_smart:.1%}, CAGR = {cagr_smart:.1%}\")\n",
                "print(f\"Minsky (1.5x Levered):  DD = {dd_lev:.1%}, CAGR = {cagr_lev:.1%}\")\n",
                "print(f\"\\nTime in Hedge: {backtest_df['unsafe_state'].mean()*100:.1f}%\")\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(backtest_df.index, backtest_df['cum_bnh'], label='S&P 500', color='gray', alpha=0.4)\n",
                "plt.plot(backtest_df.index, backtest_df['cum_smart'], label=f'Smart Hedge CAGR:{cagr_smart:.1%}', color='blue')\n",
                "plt.plot(backtest_df.index, backtest_df['cum_lev'], label=f'Levered Minsky CAGR:{cagr_lev:.1%}', color='darkgreen', linewidth=2)\n",
                "plt.title('Turning Safety into Alpha (Smart Switching & Leverage)', fontsize=14)\n",
                "plt.yscale('log')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.2)\n",
                "plt.savefig(f'{WORK_DIR}/figures/Figure_MinskyHedge.png', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "phase13"
            },
            "outputs": [],
            "source": [
                "# @title Phase 13: Structural Alpha Heatmap (Entropy vs Sync)\n",
                "\n",
                "entropy_pcts = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.40, 0.50]\n",
                "sync_pcts = [0.50, 0.60, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
                "results_grid = np.zeros((len(sync_pcts), len(entropy_pcts)))\n",
                "\n",
                "print(\"Running Grid Search on Structure (Entropy vs Sync)...\")\n",
                "\n",
                "subset = df[df['volatility'] < 25].copy()\n",
                "subset['ret_future'] = subset['price'].pct_change(22).shift(-22)\n",
                "subset = subset.dropna()\n",
                "\n",
                "mod_base = smf.quantreg('ret_future ~ volatility', subset)\n",
                "res_base = mod_base.fit(q=0.05)\n",
                "r2_base = res_base.prsquared\n",
                "\n",
                "for i, s_pct in enumerate(sync_pcts):\n",
                "    s_thresh = subset['absorption_ratio'].quantile(s_pct)\n",
                "    \n",
                "    for j, e_pct in enumerate(entropy_pcts):\n",
                "        e_thresh = subset['entropy'].quantile(e_pct)\n",
                "        \n",
                "        subset['structural_signal'] = ((subset['absorption_ratio'] > s_thresh) & \n",
                "                                       (subset['entropy'] < e_thresh)).astype(int)\n",
                "        \n",
                "        if subset['structural_signal'].sum() > 10:\n",
                "            try:\n",
                "                mod_struct = smf.quantreg('ret_future ~ volatility + structural_signal', subset)\n",
                "                res_struct = mod_struct.fit(q=0.05)\n",
                "                imp = ((res_struct.prsquared - r2_base) / r2_base) * 100\n",
                "                results_grid[i, j] = imp\n",
                "            except:\n",
                "                results_grid[i, j] = 0\n",
                "        else:\n",
                "            results_grid[i, j] = 0\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(12, 9))\n",
                "x_labels = [f\"Bottom {int(p*100)}%\" for p in entropy_pcts]\n",
                "y_labels = [f\"Top {int(p*100)}%\" for p in sync_pcts]\n",
                "\n",
                "sns.heatmap(results_grid, annot=True, fmt=\".1f\", cmap=\"vlag\", center=0,\n",
                "            xticklabels=x_labels, yticklabels=y_labels,\n",
                "            cbar_kws={'label': 'Improvement over VIX Baseline (%)'})\n",
                "\n",
                "plt.title('Relative Improvement of CARIA-SR over Benchmark (%)', fontsize=16, fontweight='bold')\n",
                "plt.xlabel('Entropy Threshold (Low Diversity)', fontsize=12)\n",
                "plt.ylabel('Synchronization Threshold (High Rigidity)', fontsize=12)\n",
                "plt.gca().invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{WORK_DIR}/figures/Figure_StructuralAlpha.png', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "phase14"
            },
            "outputs": [],
            "source": [
                "# @title Phase 14: JF Empirical Suite (Sharpe, Sortino, Bootstrap)\n",
                "\n",
                "def calc_risk_metrics(series, rf=0.04):\n",
                "    rf_daily = (1 + rf)**(1/252) - 1\n",
                "    excess_ret = series - rf_daily\n",
                "    \n",
                "    ann_ret = np.mean(series) * 252\n",
                "    ann_vol = np.std(series) * np.sqrt(252)\n",
                "    sharpe = np.mean(excess_ret) / np.std(series) * np.sqrt(252)\n",
                "    \n",
                "    downside = excess_ret[excess_ret < 0]\n",
                "    downside_std = np.std(downside) * np.sqrt(252)\n",
                "    sortino = np.mean(excess_ret) / downside_std * np.sqrt(252) if downside_std > 0 else 0\n",
                "    \n",
                "    return ann_ret, ann_vol, sharpe, sortino\n",
                "\n",
                "# Calculate metrics\n",
                "metrics_bench = calc_risk_metrics(backtest_df['daily_ret'].dropna())\n",
                "metrics_smart = calc_risk_metrics(backtest_df['smart_ret'].dropna())\n",
                "metrics_lev = calc_risk_metrics(backtest_df['lev_ret'].dropna())\n",
                "\n",
                "risk_table = pd.DataFrame({\n",
                "    'Metric': ['Ann. Return', 'Ann. Volatility', 'Sharpe Ratio', 'Sortino Ratio'],\n",
                "    'S&P 500': [f\"{metrics_bench[0]:.1%}\", f\"{metrics_bench[1]:.1%}\", f\"{metrics_bench[2]:.2f}\", f\"{metrics_bench[3]:.2f}\"],\n",
                "    'Minsky Hedge': [f\"{metrics_smart[0]:.1%}\", f\"{metrics_smart[1]:.1%}\", f\"{metrics_smart[2]:.2f}\", f\"{metrics_smart[3]:.2f}\"],\n",
                "    'Minsky 1.5x': [f\"{metrics_lev[0]:.1%}\", f\"{metrics_lev[1]:.1%}\", f\"{metrics_lev[2]:.2f}\", f\"{metrics_lev[3]:.2f}\"]\n",
                "})\n",
                "\n",
                "print(\"\\n--- TABLE: ECONOMIC PERFORMANCE ---\")\n",
                "print(risk_table.to_string(index=False))\n",
                "risk_table.to_csv(f'{WORK_DIR}/tables/Table_RiskMetrics.csv', index=False)\n",
                "\n",
                "# --- BOOTSTRAP ---\n",
                "print(\"\\nRunning Bootstrap (1000 iterations)...\")\n",
                "n_boot = 1000\n",
                "improvements = []\n",
                "\n",
                "subset = df[df['volatility'] < 20].copy()\n",
                "subset['ret_future'] = subset['price'].pct_change(22).shift(-22)\n",
                "subset = subset.dropna()\n",
                "\n",
                "for i in range(n_boot):\n",
                "    sample = subset.sample(n=len(subset), replace=True)\n",
                "    try:\n",
                "        mod_base = smf.quantreg('ret_future ~ volatility', sample)\n",
                "        r2_base = mod_base.fit(q=0.05).prsquared\n",
                "        \n",
                "        mod_struct = smf.quantreg('ret_future ~ volatility + caria_peak', sample)\n",
                "        r2_struct = mod_struct.fit(q=0.05).prsquared\n",
                "        \n",
                "        if r2_base > 0:\n",
                "            imp = (r2_struct - r2_base) / r2_base\n",
                "            improvements.append(imp)\n",
                "    except:\n",
                "        continue\n",
                "\n",
                "mean_imp = np.mean(improvements)\n",
                "ci_lower = np.percentile(improvements, 2.5)\n",
                "ci_upper = np.percentile(improvements, 97.5)\n",
                "\n",
                "print(f\"\\n--- BOOTSTRAP RESULTS ---\")\n",
                "print(f\"Mean Improvement: {mean_imp:.1%}\")\n",
                "print(f\"95% CI: [{ci_lower:.1%}, {ci_upper:.1%}]\")\n",
                "print(f\"P(Improvement > 0): {np.mean(np.array(improvements) > 0):.1%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "summary"
            },
            "outputs": [],
            "source": [
                "# @title Final Summary\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"CARIA-SR HYSTERESIS VALIDATION COMPLETE\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(f\"\\nðŸ“Š Data:\")\n",
                "print(f\"   Universe: {returns.shape[1]} S&P 500 stocks\")\n",
                "print(f\"   Period: {df.index.min().date()} to {df.index.max().date()}\")\n",
                "print(f\"   Observations: {len(df)}\")\n",
                "\n",
                "print(f\"\\nðŸ”¬ Key Finding (Phase 8):\")\n",
                "print(f\"   VIX < 20 Regime Improvement: {imp:.1f}%\")\n",
                "\n",
                "print(f\"\\nðŸ’° Minsky Hedge (Phase 12):\")\n",
                "print(f\"   Max DD Reduction: {dd_bnh - dd_smart:.1%}\")\n",
                "print(f\"   Calmar Improvement: {(cagr_smart/abs(dd_smart))/(cagr_bnh/abs(dd_bnh)):.1f}x\")\n",
                "\n",
                "print(f\"\\nðŸ“ Saved to: {WORK_DIR}\")\n",
                "print(f\"   /figures/Figure_*.png\")\n",
                "print(f\"   /tables/Table_*.csv\")\n",
                "\n",
                "# Save final dataset\n",
                "df.to_csv(f'{WORK_DIR}/full_validation_dataset.csv')\n",
                "print(\"\\nâœ… DONE!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}