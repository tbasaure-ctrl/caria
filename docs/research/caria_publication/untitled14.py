# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mwp7rIiIlAMSepZRwiAxtPfe9c-IIMQp
"""

import numpy as np
import pandas as pd

from scipy.ndimage import gaussian_filter1d
from scipy import signal

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import FactorAnalysis, PCA

from scipy.optimize import minimize

from google.colab import drive
drive.mount('/content/drive')

import os
print([f for f in os.listdir(".") if f.lower().endswith((".csv",".parquet"))][:30])

!pip -q install pandas numpy requests tqdm

import os, time, requests
import pandas as pd
import numpy as np
from tqdm.auto import tqdm

# --- PON TU KEY AQUÍ (o mejor: Colab Secrets / env var) ---
FMP_API_KEY = os.environ.get("FMP_API_KEY", "79fY9wvC9qtCJHcn6Yelf4ilE9TkRMoq")

# Universe "countries" vía ETFs (puedes editar/expandir)
TICKERS = [
  # Core USA
  "SPY","QQQ","IWM","DIA",
  # Developed
  "EFA","EWJ","EWG","EWU","EWC","EWA","EWH","EWK","EWT","EWS","EWL",
  # EM / LatAm / Asia
  "EEM","EWZ","EWW","EPU","ECH","ECO","ARGT",
  "EWI","TUR","EZA","RSX","THD","IDX","EPHE","VNM","EIDO",
  "INDA","EWY","FXI",
  # Middle East
  "EIS","KSA","QAT","UAE",
]

START = "2005-01-01"
END   = None  # None = hasta hoy

def fetch_fmp_prices(symbol, start="2005-01-01", end=None, apikey=None, sleep_s=0.25):
    url = f"https://financialmodelingprep.com/api/v3/historical-price-full/{symbol}"
    params = {"apikey": apikey, "from": start}
    if end is not None:
        params["to"] = end

    r = requests.get(url, params=params, timeout=60)
    if r.status_code != 200:
        raise RuntimeError(f"{symbol}: HTTP {r.status_code} {r.text[:200]}")

    js = r.json()
    hist = js.get("historical", None)
    if not hist:
        return None

    df = pd.DataFrame(hist)
    # FMP suele traer 'date' y 'adjClose' / 'close'
    if "adjClose" in df.columns:
        px = df[["date","adjClose"]].rename(columns={"adjClose":"price"})
    elif "close" in df.columns:
        px = df[["date","close"]].rename(columns={"close":"price"})
    else:
        return None

    px["date"] = pd.to_datetime(px["date"])
    px = px.sort_values("date").set_index("date")["price"].astype(float)

    time.sleep(sleep_s)
    return px

prices = {}
failed = []

for sym in tqdm(TICKERS):
    try:
        s = fetch_fmp_prices(sym, start=START, end=END, apikey=FMP_API_KEY)
        if s is None or s.dropna().shape[0] < 500:
            failed.append(sym)
        else:
            prices[sym] = s
    except Exception as e:
        failed.append(sym)

print("OK:", len(prices), "Failed:", len(failed), "Failed list:", failed[:20])

prices_df = pd.DataFrame(prices).sort_index()

# Coverage filter (evita series muy incompletas)
coverage = prices_df.notna().mean().sort_values(ascending=False)
keep = coverage[coverage >= 0.8].index  # 80% cobertura (ajusta si quieres)
prices_df = prices_df[keep]

# Forward-fill solo gaps pequeños (evita lookahead: NO bfill)
prices_df = prices_df.ffill(limit=3)

print("prices_df:", prices_df.shape, "range:", prices_df.index.min(), "->", prices_df.index.max())
prices_df.to_csv("prices_dataset1.csv")
prices_df.head()

print(prices_df.shape)
print(prices_df.columns[:10])
print(prices_df.index.min(), prices_df.index.max())
print(prices_df.notna().mean().median())

prices = prices_df.copy()

# Si hay NaN aislados (por feriados), forward-fill pequeño
prices = prices.ffill(limit=3)

# Drop columnas que igual quedaron con gaps relevantes (por si acaso)
keep = prices.columns[prices.notna().mean() >= 0.98]
prices = prices[keep]

print("Final panel:", prices.shape, "median coverage:", float(prices.notna().mean().median()))

ret = np.log(prices).diff().dropna(how="all")

from sklearn.covariance import LedoitWolf

def cov_to_corr(S):
    d = np.sqrt(np.diag(S))
    d = np.where(d == 0, 1e-10, d)
    C = S / np.outer(d, d)
    return np.nan_to_num((C + C.T) / 2)

def eig_metrics(C, k_frac=0.2):
    w = np.sort(np.linalg.eigvalsh(C))[::-1]
    w = np.maximum(w, 1e-10)
    k = max(1, int(np.ceil(k_frac * len(w))))
    ar = np.sum(w[:k]) / np.sum(w)
    p = w / np.sum(w)
    ent = -np.sum(p * np.log(p + 1e-10)) / np.log(len(w)) if len(w) > 1 else 0.5
    return float(ar), float(ent)

window = 252
step = 5
lw = LedoitWolf()

struct = pd.DataFrame(index=ret.index, columns=["absorption_ratio","entropy"], dtype=float)

min_assets = max(20, int(0.7 * ret.shape[1]))  # para 27 -> 20 aprox
total_steps = (len(ret) - window) // step
print("Assets:", ret.shape[1], "min_assets:", min_assets, "steps:", total_steps)

for i, t in enumerate(range(window, len(ret), step)):
    W = ret.iloc[t-window:t].copy()

    # quedate con columnas con buena cobertura dentro del window
    good = W.notna().mean() >= 0.9
    W = W.loc[:, good]

    if W.shape[1] < min_assets:
        continue

    # imputación LOCAL (solo dentro de ventana): mean
    W = W.apply(lambda s: s.fillna(s.mean()))
    X = W.values - np.nanmean(W.values, axis=0)

    try:
        S = lw.fit(X).covariance_
        C = cov_to_corr(S)
    except:
        C = np.corrcoef(X, rowvar=False)
        C = np.nan_to_num((C + C.T) / 2)

    ar, ent = eig_metrics(C, k_frac=0.2)
    struct.iloc[t] = [ar, ent]

struct = struct.ffill()  # NO bfill
struct.index.name = "date"
struct.dropna().head(), struct.dropna().tail()

import numpy as np, pandas as pd
from scipy.ndimage import gaussian_filter1d
from scipy import signal
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import FactorAnalysis
from scipy.optimize import minimize

# usa el panel final que ocupaste para AR/Entropy:
prices = prices_df.copy().ffill(limit=3)
prices = prices.dropna(axis=1, how="any")  # ya te quedó 22 assets
ret = np.log(prices).diff().dropna(how="all")

def rolling_avg_corr(r, window=60):
    out = []
    idx = r.index
    for i in range(window, len(r)):
        c = r.iloc[i-window:i].corr().values
        n = c.shape[0]
        avg = (c.sum() - n) / (n * (n - 1))
        out.append(avg)
    return pd.Series(out, index=idx[window:])

def compute_cf(r, w=20):
    avg_corr = rolling_avg_corr(r, window=w)
    avg_std  = r.rolling(w).std().mean(axis=1).loc[avg_corr.index]
    return (avg_corr * avg_std * 100).rename("cf")

CF   = compute_cf(ret, w=20)
CURV = rolling_avg_corr(ret, 60).rename("curv")  # “correlación promedio”

def extract_phase_safe(series):
    s = series.dropna()
    baseline = pd.Series(gaussian_filter1d(s.values, sigma=60), index=s.index)
    detr = s - baseline
    analytic = signal.hilbert(detr.values)
    return pd.Series(np.angle(analytic), index=s.index)

phases = {c: extract_phase_safe(ret[c]) for c in ret.columns}
common = None
for s in phases.values():
    common = s.index if common is None else common.intersection(s.index)
phases_df = pd.DataFrame({k: v.loc[common] for k, v in phases.items()}).dropna()

def kuramoto_order(phases_df, window=60):
    r = []
    idx = phases_df.index
    for i in range(window, len(phases_df)):
        z = np.exp(1j * phases_df.iloc[i].values).mean()
        r.append(np.abs(z))
    return pd.Series(r, index=idx[window:], name="sync")

SYNC = kuramoto_order(phases_df, window=60)

def compute_ews(series, window=120):
    return pd.DataFrame({
        "acf1": series.rolling(window).apply(lambda x: pd.Series(x).autocorr(1), raw=False),
        "var":  series.rolling(window).var(),
        "skew": series.rolling(window).skew(),
    }, index=series.index)

EWS = compute_ews(CF, 120)

print("CF:", CF.dropna().shape, "SYNC:", SYNC.dropna().shape, "CURV:", CURV.dropna().shape, "EWS:", EWS.dropna().shape)

def rolling_z(x, win=252):
    m = x.rolling(win, min_periods=win).mean()
    s = x.rolling(win, min_periods=win).std(ddof=0)
    return (x - m) / s

struct_feat = struct.copy()
struct_feat["absorp_z"] = rolling_z(struct_feat["absorption_ratio"], 252)
struct_feat["ent_z"]    = rolling_z(struct_feat["entropy"], 252)
struct_feat["peak_60"]  = struct_feat["absorp_z"].rolling(60, min_periods=60).mean()
struct_feat["topology"] = struct_feat["entropy"]  # o (1-entropy) si prefieres “orden”

struct_feat = struct_feat.dropna(subset=["absorp_z","ent_z","peak_60","topology"])
print("struct_feat:", struct_feat.shape, struct_feat.index.min(), struct_feat.index.max())

def align_all(CF, SYNC, EWS, CURV, struct_feat):
    sdict = {
        "cf": CF,
        "sync": SYNC,
        "acf1": EWS["acf1"],
        "var": EWS["var"],
        "skew": EWS["skew"].abs(),
        "curv": CURV,
        "absorp_z": struct_feat["absorp_z"],
        "ent_z": struct_feat["ent_z"],
        "peak_60": struct_feat["peak_60"],
        "topology": struct_feat["topology"],
    }
    common = None
    for s in sdict.values():
        idx = s.dropna().index
        common = idx if common is None else common.intersection(idx)
    df_sig = pd.DataFrame({k: v.loc[common] for k, v in sdict.items()}).dropna().sort_index()
    return df_sig

signals_df = align_all(CF, SYNC, EWS, CURV, struct_feat)
print("signals_df:", signals_df.shape, signals_df.index.min(), signals_df.index.max())

Xz = StandardScaler().fit_transform(signals_df.values)
fa = FactorAnalysis(n_components=1, random_state=0)
F = fa.fit_transform(Xz).reshape(-1)

load_df = pd.DataFrame({"signal": signals_df.columns, "loading": fa.components_.T.reshape(-1)}).sort_values("loading", ascending=False)

# orientar: fragilidad ↑ con cf y absorp_z
if load_df.set_index("signal").loc["cf","loading"] < 0:
    F = -F
    load_df["loading"] = -load_df["loading"]

Ft = pd.Series(F, index=signals_df.index, name="F_t")

display(load_df)
print("Ft:", Ft.describe())

Z = pd.DataFrame(StandardScaler().fit_transform(signals_df),
                 index=signals_df.index, columns=signals_df.columns)

a_df = Z[["acf1","skew","ent_z"]]          # “asymmetry control”
b_df = Z[["cf","sync","absorp_z","curv"]]  # “bifurcation push”

def stable_state_from_ab(a, b):
    roots = np.roots([1.0, 0.0, float(a), float(b)])
    real_roots = np.real(roots[np.isclose(np.imag(roots), 0.0, atol=1e-8)])
    if len(real_roots) == 0:
        return np.nan
    def V(x): return (x**4)/4.0 + (a*(x**2))/2.0 + b*x
    return float(real_roots[np.argmin([V(r) for r in real_roots])])

def fit_cusp(Ft, a_df, b_df):
    y = Ft.values.astype(float)
    Za = a_df.values.astype(float)
    Zb = b_df.values.astype(float)

    p = 1 + Za.shape[1] + 1 + Zb.shape[1]
    x0 = np.zeros(p)

    def predict(params):
        alpha0 = params[0]
        alpha  = params[1:1+Za.shape[1]]
        beta0  = params[1+Za.shape[1]]
        beta   = params[2+Za.shape[1]:]
        a = alpha0 + Za @ alpha
        b = beta0  + Zb @ beta
        xhat = np.array([stable_state_from_ab(ai, bi) for ai, bi in zip(a, b)], dtype=float)
        return xhat, a, b

    def loss(params):
        xhat, _, _ = predict(params)
        m = np.isfinite(xhat) & np.isfinite(y)
        return np.mean((y[m] - xhat[m])**2)

    res = minimize(loss, x0, method="Powell", options={"maxiter": 2000, "disp": True})
    xhat, a, b = predict(res.x)
    return res, pd.Series(xhat, index=Ft.index, name="xhat"), pd.Series(a, index=Ft.index, name="a_t"), pd.Series(b, index=Ft.index, name="b_t")

res, xhat, a_t, b_t = fit_cusp(Ft, a_df, b_df)
out_dyn = pd.concat([Ft, xhat, a_t, b_t], axis=1).dropna()

disc = 4*(out_dyn["a_t"]**3) + 27*(out_dyn["b_t"]**2)
out_dyn["bistable"] = disc < 0

print("Cusp success:", res.success, "loss:", res.fun)
print("Corr(Ft,xhat):", float(out_dyn["F_t"].corr(out_dyn["xhat"])))
print("Bistable fraction:", float(out_dyn["bistable"].mean()))

import numpy as np, pandas as pd

H = 22  # horizonte (días de trading)
mkt = "SPY"

# Target: retorno futuro del mercado
mkt_px = prices[mkt].dropna()
mkt_ret = np.log(mkt_px).diff()

future_ret = (mkt_px.shift(-H) / mkt_px - 1.0).rename(f"fut_ret_{H}")
realized_vol = (mkt_ret.rolling(H).std() * np.sqrt(252)).rename("rv_ann")

# Dataframe base (features en t, target en t+H)
df_eval = pd.concat([signals_df, Ft.rename("F_t"), realized_vol, future_ret], axis=1).dropna()
df_eval = df_eval.iloc[:-H]  # quita el tail sin futuro

print(df_eval.shape, df_eval.index.min(), "->", df_eval.index.max())
df_eval[["F_t","rv_ann",f"fut_ret_{H}"]].head()

y = df_eval[f"fut_ret_{H}"].copy()
x = df_eval["F_t"].copy()

# deciles
df_eval["F_decile"] = pd.qcut(x, 10, labels=False) + 1

# tail event: peor 10% de retornos futuros (definición no paramétrica)
tail_thr = y.quantile(0.10)
df_eval["tail10"] = (y <= tail_thr).astype(int)

summary = df_eval.groupby("F_decile").agg(
    n=("tail10","size"),
    mean_ret=(f"fut_ret_{H}","mean"),
    p_tail10=("tail10","mean"),
    q05=(f"fut_ret_{H}", lambda s: s.quantile(0.05)),
    q50=(f"fut_ret_{H}", "median"),
    q95=(f"fut_ret_{H}", lambda s: s.quantile(0.95)),
).reset_index()

print("Tail threshold (10%):", float(tail_thr))
summary

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, brier_score_loss

H = 22
purge = H  # clave: purgar al menos H días
train_n = 8*252
test_n  = 126
step_n  = 126

dfm = df_eval.copy()

# features sets (ajusta si cambias nombres)
X1 = ["rv_ann"]
X2 = ["rv_ann","peak_60","absorp_z","ent_z"]
X3 = ["rv_ann","F_t"]

def make_folds_idx(n, train_n, test_n, purge_n, step_n):
    folds = []
    start_test = train_n + purge_n
    while (start_test + test_n) <= n:
        train_end = start_test - purge_n
        test_end  = start_test + test_n
        folds.append((0, train_end, start_test, test_end))
        start_test += step_n
    return folds

folds = make_folds_idx(len(dfm), train_n, test_n, purge, step_n)
print("Folds:", len(folds))

def fit_predict_proba(train, test, feats):
    Xtr, ytr = train[feats].values, train["tail10"].values
    Xte, yte = test[feats].values,  test["tail10"].values

    clf = Pipeline([
        ("scaler", StandardScaler()),
        ("lr", LogisticRegression(max_iter=500, class_weight="balanced"))
    ])
    clf.fit(Xtr, ytr)
    p = clf.predict_proba(Xte)[:,1]
    return yte, p

rows = []
for i,(a,b,c,d) in enumerate(folds, 1):
    tr = dfm.iloc[a:b].dropna()
    te = dfm.iloc[c:d].dropna()
    if len(te) < 80:
        continue

    yte, p1 = fit_predict_proba(tr, te, X1)
    _,   p2 = fit_predict_proba(tr, te, X2)
    _,   p3 = fit_predict_proba(tr, te, X3)

    rows.append({
        "fold": i,
        "start": te.index.min(),
        "end": te.index.max(),
        "auc_rv": roc_auc_score(yte, p1),
        "auc_struct": roc_auc_score(yte, p2),
        "auc_Ft": roc_auc_score(yte, p3),
        "brier_rv": brier_score_loss(yte, p1),
        "brier_struct": brier_score_loss(yte, p2),
        "brier_Ft": brier_score_loss(yte, p3),
    })
    if i % 5 == 0:
        print("done fold", i)

res = pd.DataFrame(rows)
res, res[["auc_rv","auc_struct","auc_Ft"]].mean(), res[["brier_rv","brier_struct","brier_Ft"]].mean()

import numpy as np

# daily market returns para equity curve
daily_ret = prices[mkt].pct_change().reindex(df_eval.index).fillna(0.0)

# señal: risk-off si F_t > threshold (ej: percentil 80 in-sample global)
thr = df_eval["F_t"].quantile(0.80)
risk_off = (df_eval["F_t"] > thr).astype(int)

# exposición: 1 cuando risk_on, 0 cuando off
expo = (1 - risk_off).astype(float)

# equity curves
eq_mkt = (1 + daily_ret).cumprod()
eq_strat = (1 + daily_ret * expo).cumprod()

def max_dd(eq):
    peak = eq.cummax()
    dd = eq/peak - 1
    return float(dd.min())

cagr = lambda eq: float(eq.iloc[-1]**(252/len(eq)) - 1)

print("Threshold:", float(thr))
print("CAGR mkt:", cagr(eq_mkt), "MaxDD mkt:", max_dd(eq_mkt))
print("CAGR strat:", cagr(eq_strat), "MaxDD strat:", max_dd(eq_strat))

from sklearn.decomposition import FactorAnalysis
from sklearn.preprocessing import StandardScaler

cols = ["cf","sync","acf1","var","skew","curv","absorp_z","ent_z","peak_60"]  # deja fuera topology si es redundante
Z = signals_df[cols].dropna()

win = 252*5  # 5 años
step = 21    # mensual
loadings = []

for t in range(win, len(Z), step):
    W = Z.iloc[t-win:t]
    X = StandardScaler().fit_transform(W.values)
    fa = FactorAnalysis(n_components=1, random_state=0)
    fa.fit(X)
    ld = fa.components_.T.reshape(-1)
    # orienta por absorp_z positivo
    if ld[cols.index("absorp_z")] < 0:
        ld = -ld
    loadings.append(pd.Series(ld, index=cols, name=Z.index[t]))

L = pd.DataFrame(loadings)
L.describe().T.sort_values("std", ascending=False).head(12), L.tail()

import numpy as np
import pandas as pd

dfh = df_eval.copy()
H = 22
ycol = f"fut_ret_{H}"

# evento cola por definición global (por ahora)
tail_thr = dfh[ycol].quantile(0.10)
dfh["tail10"] = (dfh[ycol] <= tail_thr).astype(int)

# estado: dirección del “camino”
dfh["dF"] = dfh["F_t"].diff()
dfh["path"] = np.where(dfh["dF"] >= 0, "rising", "falling")

dfh["F_decile"] = pd.qcut(dfh["F_t"], 10, labels=False) + 1

tab_hyst = dfh.groupby(["F_decile","path"]).agg(
    n=("tail10","size"),
    mean_ret=(ycol,"mean"),
    p_tail10=("tail10","mean"),
    q05=(ycol, lambda s: s.quantile(0.05)),
    q50=(ycol,"median"),
).reset_index()

tab_hyst.sort_values(["F_decile","path"])

import pandas as pd

tab = tab_hyst.copy()  # tu tabla
wide = tab.pivot(index="F_decile", columns="path", values=["p_tail10","q05","mean_ret","n"])

wide["Δp_tail10(fall-rise)"] = wide[("p_tail10","falling")] - wide[("p_tail10","rising")]
wide["Δq05(fall-rise)"]      = wide[("q05","falling")]      - wide[("q05","rising")]
wide["Δmean(fall-rise)"]     = wide[("mean_ret","falling")] - wide[("mean_ret","rising")]

wide[["Δp_tail10(fall-rise)","Δq05(fall-rise)","Δmean(fall-rise)"]]

counts = dfp.pivot_table(index="dF_bin", columns="F_bin", values="tail10", aggfunc="size")
heat = dfp.pivot_table(index="dF_bin", columns="F_bin", values="tail10", aggfunc="mean")

counts, heat.where(counts >= 50)   # ajusta 50 según tu n

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import average_precision_score, log_loss
import numpy as np
import pandas as pd

H = 22
ycol = f"fut_ret_{H}"
df = df_eval.copy().dropna()

df["dF"] = df["F_t"].diff()
df = df.dropna(subset=["dF"])

# etiqueta tail definida globalmente (si quieres OOS: define umbral por fold)
thr = df[ycol].quantile(0.10)
df["tail10"] = (df[ycol] <= thr).astype(int)

# features “histeresis”: Ft, dF, interacción y opcional |dF|
df["FxdF"] = df["F_t"] * df["dF"]
X = df[["rv_ann","F_t","dF","FxdF"]].replace([np.inf,-np.inf], np.nan).dropna()
y = df.loc[X.index, "tail10"].values

clf = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(max_iter=800, class_weight="balanced"))
])
clf.fit(X.values, y)
p = clf.predict_proba(X.values)[:,1]

print("PR-AUC:", average_precision_score(y, p))
print("LogLoss:", log_loss(y, p))
print("coef:", dict(zip(X.columns, clf.named_steps["lr"].coef_[0])))

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

H = 22
ycol = f"fut_ret_{H}"
df = df_eval.copy().dropna()

df["dF"] = df["F_t"].diff()
df = df.dropna(subset=["dF"])

thr = df[ycol].quantile(0.10)           # luego lo hacemos por fold (OOS)
df["tail10"] = (df[ycol] <= thr).astype(int)

df["FxdF"] = df["F_t"] * df["dF"]
X = df[["F_t","dF","FxdF"]].values
y = df["tail10"].values

clf = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(max_iter=800, class_weight="balanced"))
])
clf.fit(X, y)
p = clf.predict_proba(X)[:,1]
df["p_tail"] = p

# señal: salgo del mercado cuando p_tail supera percentil q
def backtest_prob(q=0.80, mkt="SPY"):
    daily_ret = prices[mkt].pct_change().reindex(df.index).fillna(0.0)
    cut = df["p_tail"].quantile(q)
    expo = (df["p_tail"] <= cut).astype(float)  # risk-on cuando prob baja
    eq = (1 + daily_ret*expo).cumprod()
    peak = eq.cummax()
    maxdd = float((eq/peak - 1).min())
    cagr = float(eq.iloc[-1]**(252/len(eq)) - 1)
    mar = cagr/abs(maxdd)
    return {"q":q, "cut":float(cut), "CAGR":cagr, "MaxDD":maxdd, "MAR":mar, "avg_expo":float(expo.mean())}

grid = pd.DataFrame([backtest_prob(q) for q in np.linspace(0.70,0.95,11)])
grid.sort_values("MAR", ascending=False).head(10)

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import average_precision_score, log_loss, roc_auc_score

H = 22
ycol = f"fut_ret_{H}"

dfm = df_eval.copy().dropna()
dfm["dF"]  = dfm["F_t"].diff()
dfm["FxdF"] = dfm["F_t"] * dfm["dF"]
dfm = dfm.dropna(subset=["dF","FxdF","rv_ann"])

def make_folds_idx(n, train_n=8*252, test_n=252, purge_n=22, step_n=126):
    folds=[]
    start_test = train_n + purge_n
    while (start_test + test_n) <= n:
        train_end = start_test - purge_n
        test_end  = start_test + test_n
        folds.append((0, train_end, start_test, test_end))
        start_test += step_n
    return folds

folds = make_folds_idx(len(dfm), train_n=8*252, test_n=252, purge_n=H, step_n=126)

def fit_predict_fold(tr, te, q_tail=0.10):
    # tail threshold from TRAIN ONLY
    thr = tr[ycol].quantile(q_tail)
    ytr = (tr[ycol] <= thr).astype(int).values
    yte = (te[ycol] <= thr).astype(int).values

    Xtr = tr[["rv_ann","F_t","dF","FxdF"]].values
    Xte = te[["rv_ann","F_t","dF","FxdF"]].values

    clf = Pipeline([
        ("scaler", StandardScaler()),
        ("lr", LogisticRegression(max_iter=800, class_weight="balanced"))
    ])
    clf.fit(Xtr, ytr)
    p = clf.predict_proba(Xte)[:,1]

    out = {
        "event_rate": float(yte.mean()),
        "prauc": np.nan,
        "logloss": np.nan,
    }
    # Calculate metrics only if both classes exist in yte
    if len(np.unique(yte)) == 2:
        out["prauc"] = average_precision_score(yte, p)
        out["logloss"] = log_loss(yte, p)
        out["auc"] = roc_auc_score(yte, p)
    else:
        out["auc"] = np.nan
    return out

rows=[]
for i,(a,b,c,d) in enumerate(folds, 1):
    tr = dfm.iloc[a:b].dropna()
    te = dfm.iloc[c:d].dropna()
    if len(te) < 150:
        continue

    r = fit_predict_fold(tr, te, q_tail=0.10)
    rows.append({
        "fold": i,
        "start": te.index.min(),
        "end": te.index.max(),
        **r
    })

oos = pd.DataFrame(rows)
print(oos[["auc","prauc","logloss"]].mean(numeric_only=True))
oos.head()

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

H = 22
ycol = f"fut_ret_{H}"
mkt = "SPY"

dfm = df_eval.copy().dropna()
dfm["dF"]   = dfm["F_t"].diff()
dfm["FxdF"] = dfm["F_t"] * dfm["dF"]
dfm = dfm.dropna(subset=["dF","FxdF","rv_ann"])

daily_ret = prices[mkt].pct_change().reindex(dfm.index).fillna(0.0)

def make_folds_idx(n, train_n=8*252, test_n=252, purge_n=22, step_n=126):
    folds=[]
    start_test = train_n + purge_n
    while (start_test + test_n) <= n:
        train_end = start_test - purge_n
        test_end  = start_test + test_n
        folds.append((0, train_end, start_test, test_end))
        start_test += step_n
    return folds

def max_dd(eq):
    peak = eq.cummax()
    return float((eq/peak - 1).min())

def cagr(eq):
    return float(eq.iloc[-1]**(252/len(eq)) - 1)

def fit_model(tr):
    thr = tr[ycol].quantile(0.10)
    ytr = (tr[ycol] <= thr).astype(int).values
    Xtr = tr[["rv_ann","F_t","dF","FxdF"]].values
    clf = Pipeline([
        ("scaler", StandardScaler()),
        ("lr", LogisticRegression(max_iter=800, class_weight="balanced"))
    ])
    clf.fit(Xtr, ytr)
    return clf

def predict_p(clf, df):
    X = df[["rv_ann","F_t","dF","FxdF"]].values
    return clf.predict_proba(X)[:,1]

def oos_strategy(q=0.875, train_n=8*252, test_n=252, purge_n=H, step_n=126):
    folds = make_folds_idx(len(dfm), train_n=train_n, test_n=test_n, purge_n=purge_n, step_n=step_n)
    eq = pd.Series(1.0, index=dfm.index)
    expo_all = pd.Series(np.nan, index=dfm.index)

    rows=[]
    for i,(a,b,c,d) in enumerate(folds, 1):
        tr = dfm.iloc[a:b].copy()
        te = dfm.iloc[c:d].copy()

        clf = fit_model(tr)

        p_tr = predict_p(clf, tr)
        cut  = np.quantile(p_tr, q)          # cut en TRAIN

        p_te = predict_p(clf, te)
        expo = (p_te <= cut).astype(float)   # risk-on cuando prob baja

        expo_all.iloc[c:d] = expo
        # equity update within test
        eq_te = (1 + daily_ret.iloc[c:d] * expo).cumprod()
        eq.iloc[c:d] = eq.iloc[c-1] * eq_te.values

        rows.append({
            "fold": i,
            "start": te.index.min(),
            "end": te.index.max(),
            "avg_expo": float(expo.mean()),
            "cut": float(cut),
        })

    eq = eq.ffill()
    out = {
        "CAGR": cagr(eq),
        "MaxDD": max_dd(eq),
        "MAR": cagr(eq)/abs(max_dd(eq)),
    }
    return out, pd.DataFrame(rows), eq, expo_all

out, fold_info, eq_oos, expo_oos = oos_strategy(q=0.875, test_n=252)
out, fold_info.head()

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

H = 22
ycol = f"fut_ret_{H}"
mkt = "SPY"

dfm = df_eval.copy().dropna()
dfm["dF"]   = dfm["F_t"].diff()
dfm["FxdF"] = dfm["F_t"] * dfm["dF"]
dfm = dfm.dropna(subset=["rv_ann","F_t","dF","FxdF", ycol])

daily_ret = prices[mkt].pct_change().reindex(dfm.index).fillna(0.0)

def make_folds_nonoverlap(n, train_n=8*252, test_n=252, purge_n=22):
    folds=[]
    start_test = train_n + purge_n
    while (start_test + test_n) <= n:
        train_end = start_test - purge_n
        test_end  = start_test + test_n
        folds.append((0, train_end, start_test, test_end))
        start_test += test_n  # <- clave: NO overlap
    return folds

def fit_model(tr):
    thr = tr[ycol].quantile(0.10)
    ytr = (tr[ycol] <= thr).astype(int).values
    Xtr = tr[["rv_ann","F_t","dF","FxdF"]].values
    clf = Pipeline([
        ("scaler", StandardScaler()),
        ("lr", LogisticRegression(max_iter=800, class_weight="balanced"))
    ])
    clf.fit(Xtr, ytr)
    return clf

def predict_p(clf, df):
    X = df[["rv_ann","F_t","dF","FxdF"]].values
    return clf.predict_proba(X)[:,1]

def max_dd(eq):
    peak = eq.cummax()
    return float((eq/peak - 1).min())

def cagr(eq):
    return float(eq.iloc[-1]**(252/len(eq)) - 1)

def oos_strategy_prob(q=0.875, train_n=8*252, test_n=252, purge_n=H):
    folds = make_folds_nonoverlap(len(dfm), train_n=train_n, test_n=test_n, purge_n=purge_n)

    expo = pd.Series(np.nan, index=dfm.index)
    fold_rows = []

    for i,(a,b,c,d) in enumerate(folds, 1):
        tr = dfm.iloc[a:b].copy()
        te = dfm.iloc[c:d].copy()

        clf = fit_model(tr)
        p_tr = predict_p(clf, tr)
        cut  = np.quantile(p_tr, q)      # cut se fija SOLO con train

        p_te = predict_p(clf, te)
        expo.iloc[c:d] = (p_te <= cut).astype(float)

        fold_rows.append({
            "fold": i,
            "start": te.index.min(),
            "end": te.index.max(),
            "avg_expo": float(expo.iloc[c:d].mean()),
            "cut": float(cut),
        })

    # usa solo el tramo OOS realmente asignado
    oos_mask = expo.notna()
    expo_oos = expo[oos_mask].astype(float)
    ret_oos  = daily_ret.loc[expo_oos.index]

    eq_strat = (1 + ret_oos * expo_oos).cumprod()
    eq_mkt   = (1 + ret_oos).cumprod()

    out = {
        "CAGR_strat": cagr(eq_strat),
        "MaxDD_strat": max_dd(eq_strat),
        "MAR_strat": cagr(eq_strat)/abs(max_dd(eq_strat)),
        "CAGR_mkt": cagr(eq_mkt),
        "MaxDD_mkt": max_dd(eq_mkt),
        "MAR_mkt": cagr(eq_mkt)/abs(max_dd(eq_mkt)),
        "avg_expo": float(expo_oos.mean()),
        "n_days": int(len(expo_oos)),
    }
    return out, pd.DataFrame(fold_rows), eq_strat, eq_mkt, expo_oos

out, fold_info, eq_strat, eq_mkt, expo_oos = oos_strategy_prob(q=0.875, test_n=252)
out, fold_info.head()

qs = np.linspace(0.75, 0.95, 9)
rows=[]
for q in qs:
    out, _, _, _, _ = oos_strategy_prob(q=q, test_n=252)
    rows.append({"q": q, **out})
pd.DataFrame(rows).sort_values("MAR_strat", ascending=False)

import matplotlib.pyplot as plt

plt.figure()
plt.plot(eq_mkt.index, eq_mkt.values, label="Market")
plt.plot(eq_strat.index, eq_strat.values, label="Hysteresis-2D")
plt.legend()
plt.title("OOS Equity Curve")
plt.show()

df_eval.columns.tolist()

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

H = 22
ycol = "fut_ret_22"
mkt = "SPY"

dfm = df_eval.copy().dropna(subset=["F_t", ycol, "rv_ann"])
dfm["dF"]   = dfm["F_t"].diff()
dfm["FxdF"] = dfm["F_t"] * dfm["dF"]
dfm = dfm.dropna(subset=["dF","FxdF", "rv_ann"])

daily_ret = prices[mkt].pct_change().reindex(dfm.index).fillna(0.0)

def make_folds_nonoverlap(n, train_n=8*252, test_n=252, purge_n=22):
    folds=[]
    start_test = train_n + purge_n
    while (start_test + test_n) <= n:
        train_end = start_test - purge_n
        test_end  = start_test + test_n
        folds.append((0, train_end, start_test, test_end))
        start_test += test_n
    return folds

def max_dd(eq):
    peak = eq.cummax()
    return float((eq/peak - 1).min())

def cagr(eq):
    return float(eq.iloc[-1]**(252/len(eq)) - 1)

def fit_model(tr, features, q_tail=0.10):
    thr = tr[ycol].quantile(q_tail)
    ytr = (tr[ycol] <= thr).astype(int).values
    Xtr = tr[features].values
    clf = Pipeline([
        ("scaler", StandardScaler()),
        ("lr", LogisticRegression(max_iter=1200, class_weight="balanced"))
    ])
    clf.fit(Xtr, ytr)
    return clf

def predict_p(clf, df, features):
    return clf.predict_proba(df[features].values)[:,1]

def oos_strategy_prob_features(q=0.80, features=None, q_tail=0.10,
                              train_n=8*252, test_n=252, purge_n=H):
    assert features is not None and len(features) > 0

    # remove rows with missing in any feature
    dff = dfm.dropna(subset=features).copy()
    dret = daily_ret.reindex(dff.index).fillna(0.0)

    folds = make_folds_nonoverlap(len(dff), train_n=train_n, test_n=test_n, purge_n=purge_n)

    expo = pd.Series(np.nan, index=dff.index)
    fold_rows = []

    for i,(a,b,c,d) in enumerate(folds, 1):
        tr = dff.iloc[a:b].copy()
        te = dff.iloc[c:d].copy()

        clf = fit_model(tr, features, q_tail=q_tail)

        p_tr = predict_p(clf, tr, features)
        cut  = np.quantile(p_tr, q)

        p_te = predict_p(clf, te, features)
        expo.iloc[c:d] = (p_te <= cut).astype(float)

        fold_rows.append({
            "fold": i,
            "start": te.index.min(),
            "end": te.index.max(),
            "avg_expo": float(expo.iloc[c:d].mean()),
            "cut": float(cut),
        })

    oos_mask = expo.notna()
    expo_oos = expo[oos_mask].astype(float)
    ret_oos  = dret.loc[expo_oos.index]

    eq_strat = (1 + ret_oos * expo_oos).cumprod()
    eq_mkt   = (1 + ret_oos).cumprod()

    out = {
        "CAGR_strat": cagr(eq_strat),
        "MaxDD_strat": max_dd(eq_strat),
        "MAR_strat": cagr(eq_strat)/abs(max_dd(eq_strat)),
        "CAGR_mkt": cagr(eq_mkt),
        "MaxDD_mkt": max_dd(eq_mkt),
        "MAR_mkt": cagr(eq_mkt)/abs(max_dd(eq_mkt)),
        "avg_expo": float(expo_oos.mean()),
        "n_days": int(len(expo_oos)),
    }
    return out

# ---- feature sets ----
BASE = ["rv_ann", "F_t", "dF", "FxdF"]
PLUS_ABS = BASE + ["absorp_z"]
PLUS_ENT = BASE + ["ent_z"]
PLUS_BOTH = BASE + ["absorp_z", "ent_z"]

feature_sets = {
    "BASE": BASE,
    "+absorp_z": PLUS_ABS,
    "+ent_z": PLUS_ENT,
    "+abs+ent": PLUS_BOTH,
}

# ---- sweep q ----
qs = [0.75, 0.775, 0.80, 0.825, 0.85, 0.875, 0.90, 0.925, 0.95]

rows = []
for name, feats in feature_sets.items():
    for q in qs:
        out = oos_strategy_prob_features(q=q, features=feats, q_tail=0.10, test_n=252)
        rows.append({"model": name, "q": q, **out})

res = pd.DataFrame(rows)
res.sort_values(["model","q"]).head(), res.sort_values("MAR_strat", ascending=False).head(15)

best = res.sort_values("MAR_strat", ascending=False).groupby("model").head(1)
best[["model","q","CAGR_strat","MaxDD_strat","MAR_strat","avg_expo"]]

for qt in [0.05, 0.10, 0.15]:
    out = oos_strategy_prob_features(q=0.80, features=BASE+["absorp_z"], q_tail=qt, test_n=252)
    print(qt, out["CAGR_strat"], out["MaxDD_strat"], out["MAR_strat"])

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# ---------- REQUIRED OBJECTS ----------
# df_eval: DataFrame indexed by date with at least:
# ['F_t','rv_ann','fut_ret_22','absorp_z','ent_z']  (others optional)
# prices: DataFrame indexed by date with column 'SPY' (or your chosen market)

assert "F_t" in df_eval.columns and "rv_ann" in df_eval.columns and "fut_ret_22" in df_eval.columns
assert "absorp_z" in df_eval.columns and "ent_z" in df_eval.columns
assert "SPY" in prices.columns

# align indexes
df_eval = df_eval.copy()
if not isinstance(df_eval.index, pd.DatetimeIndex):
    df_eval.index = pd.to_datetime(df_eval.index)

prices = prices.copy()
if not isinstance(prices.index, pd.DatetimeIndex):
    prices.index = pd.to_datetime(prices.index)

common_idx = df_eval.index.intersection(prices.index)
df_eval = df_eval.loc[common_idx].sort_index()
prices = prices.loc[common_idx].sort_index()

print("df_eval:", df_eval.shape, "prices:", prices.shape, "dates:", common_idx.min(), "->", common_idx.max())

H = 22
ycol = "fut_ret_22"
mkt = "SPY"

def make_folds_nonoverlap(n, train_n=8*252, test_n=252, purge_n=22):
    folds=[]
    start_test = train_n + purge_n
    while (start_test + test_n) <= n:
        train_end = start_test - purge_n
        test_end  = start_test + test_n
        folds.append((0, train_end, start_test, test_end))
        start_test += test_n
    return folds

def max_dd(eq):
    peak = np.maximum.accumulate(eq)
    return float((eq/peak - 1).min())

def cagr(eq):
    return float(eq[-1]**(252/len(eq)) - 1)

def cvar(x, alpha=0.05):
    # x are daily returns
    q = np.quantile(x, alpha)
    return float(np.mean(x[x <= q])) if np.any(x <= q) else np.nan

def cer_meanvar(daily_ret, gamma=3.0):
    # certainty-equivalent via mean-variance approx: CER ≈ μ - 0.5*γ*σ^2
    mu = np.mean(daily_ret)*252
    sig2 = np.var(daily_ret)*252
    return float(mu - 0.5*gamma*sig2)

def eval_path(daily_ret):
    eq = (1 + daily_ret).cumprod()
    return {
        "CAGR": cagr(eq),
        "MaxDD": max_dd(eq),
        "MAR": cagr(eq)/abs(max_dd(eq)),
        "Vol": float(np.std(daily_ret)*np.sqrt(252)),
        "Sharpe": float((np.mean(daily_ret)*252) / (np.std(daily_ret)*np.sqrt(252) + 1e-12)),
        "CVaR5": cvar(daily_ret.values, 0.05),
        "CER_g1": float(np.exp(np.mean(np.log1p(daily_ret)))**252 - 1),   # log-utility growth
        "CER_g3": cer_meanvar(daily_ret.values, gamma=3.0),
        "CER_g5": cer_meanvar(daily_ret.values, gamma=5.0),
    }, eq

def fit_model(tr, features, q_tail=0.10):
    thr = tr[ycol].quantile(q_tail)
    ytr = (tr[ycol] <= thr).astype(int).values
    Xtr = tr[features].values
    clf = Pipeline([
        ("scaler", StandardScaler()),
        ("lr", LogisticRegression(max_iter=1200, class_weight="balanced"))
    ])
    clf.fit(Xtr, ytr)
    return clf

def predict_p(clf, df, features):
    return clf.predict_proba(df[features].values)[:,1]

def oos_run(features, q=0.80, q_tail=0.10, train_n=8*252, test_n=252, purge_n=H,
            tc_bps=0.0):
    dff = df_eval.copy()
    dff["dF"]   = dff["F_t"].diff()
    dff["FxdF"] = dff["F_t"] * dff["dF"]
    dff = dff.dropna(subset=features + ["dF","FxdF", ycol, "rv_ann", "F_t"])

    daily_mkt = prices[mkt].pct_change().reindex(dff.index).fillna(0.0)

    folds = make_folds_nonoverlap(len(dff), train_n=train_n, test_n=test_n, purge_n=purge_n)

    expo = pd.Series(np.nan, index=dff.index)
    p_hat = pd.Series(np.nan, index=dff.index)
    fold_rows = []

    for i,(a,b,c,d) in enumerate(folds, 1):
        tr = dff.iloc[a:b].copy()
        te = dff.iloc[c:d].copy()

        clf = fit_model(tr, features, q_tail=q_tail)
        p_tr = predict_p(clf, tr, features)
        cut  = np.quantile(p_tr, q)

        p_te = predict_p(clf, te, features)
        expo.iloc[c:d] = (p_te <= cut).astype(float)
        p_hat.iloc[c:d] = p_te

        fold_rows.append({
            "fold": i,
            "start": te.index.min(),
            "end": te.index.max(),
            "avg_expo": float(expo.iloc[c:d].mean()),
            "cut": float(cut),
        })

    # OOS slice
    mask = expo.notna()
    expo_oos = expo[mask].astype(float)
    p_oos    = p_hat[mask].astype(float)
    ret_mkt  = daily_mkt.loc[expo_oos.index]

    # transaction costs on switches (simple)
    if tc_bps > 0:
        switches = expo_oos.diff().abs().fillna(0.0)
        cost = switches * (tc_bps/10000.0)
    else:
        cost = 0.0

    ret_strat = ret_mkt * expo_oos - cost

    perf_strat, eq_strat = eval_path(ret_strat)
    perf_mkt,   eq_mkt   = eval_path(ret_mkt)

    summary = {
        "q": q, "q_tail": q_tail, "tc_bps": tc_bps,
        "avg_expo": float(expo_oos.mean()),
        "n_days": int(len(expo_oos)),
        **{f"{k}_strat": v for k,v in perf_strat.items()},
        **{f"{k}_mkt": v for k,v in perf_mkt.items()},
        "MaxDD_gap": perf_strat["MaxDD"] - perf_mkt["MaxDD"],
        "MAR_gap": perf_strat["MAR"] - perf_mkt["MAR"],
        "CER_g3_gap": perf_strat["CER_g3"] - perf_mkt["CER_g3"],
    }

    return summary, pd.DataFrame(fold_rows), eq_strat, eq_mkt, expo_oos, p_oos

# ---- MODEL DEFINITIONS ----
BASE = ["rv_ann", "F_t", "dF", "FxdF"]
ABS  = BASE + ["absorp_z"]
ENT  = BASE + ["ent_z"]
BOTH = BASE + ["absorp_z", "ent_z"]

# ---- RUN YOUR WINNER (paper default) ----
winner_summary, winner_folds, eq_strat, eq_mkt, expo_oos, p_oos = oos_run(
    features=ABS, q=0.80, q_tail=0.10, test_n=252, tc_bps=0.0
)

pd.DataFrame([winner_summary]).T.head(30), winner_folds.head()

def plot_equity(eq_mkt, eq_strat, title="OOS Equity", save=None):
    plt.figure()
    plt.plot(eq_mkt.index, eq_mkt.values, label="Market")
    plt.plot(eq_strat.index, eq_strat.values, label="Strategy")
    plt.legend()
    plt.title(title)
    plt.xlabel("Date"); plt.ylabel("Equity (cumprod)")
    if save: plt.savefig(save, dpi=200, bbox_inches="tight")
    plt.show()

def plot_drawdown(eq, title="Drawdown", save=None):
    peak = eq.cummax()
    dd = eq/peak - 1
    plt.figure()
    plt.plot(dd.index, dd.values)
    plt.title(title)
    plt.xlabel("Date"); plt.ylabel("Drawdown")
    if save: plt.savefig(save, dpi=200, bbox_inches="tight")
    plt.show()

def plot_exposure(expo, title="Exposure", save=None):
    plt.figure()
    plt.plot(expo.index, expo.values)
    plt.ylim(-0.05, 1.05)
    plt.title(title)
    plt.xlabel("Date"); plt.ylabel("Exposure")
    if save: plt.savefig(save, dpi=200, bbox_inches="tight")
    plt.show()

def plot_prob(p, title="Tail probability (model)", save=None):
    plt.figure()
    plt.plot(p.index, p.values)
    plt.title(title)
    plt.xlabel("Date"); plt.ylabel("p(tail)")
    if save: plt.savefig(save, dpi=200, bbox_inches="tight")
    plt.show()

plot_equity(eq_mkt, eq_strat, title="OOS Equity: +absorp_z (q=0.80, tail10)")
plot_drawdown(eq_mkt, title="Market Drawdown (OOS)")
plot_drawdown(eq_strat, title="Strategy Drawdown (OOS)")
plot_exposure(expo_oos, title="Exposure (OOS)")
plot_prob(p_oos, title="p(tail10) (OOS)")

def hysteresis_heatmap(df, p_series, n_bins=12, min_count=40, save=None):
    tmp = df.loc[p_series.index, ["F_t"]].copy()
    tmp["dF"] = df.loc[p_series.index, "F_t"].diff()
    tmp["p"]  = p_series.values
    tmp = tmp.dropna()

    # quantile bins to balance counts
    tmp["F_bin"]  = pd.qcut(tmp["F_t"], n_bins, labels=False, duplicates="drop")
    tmp["dF_bin"] = pd.qcut(tmp["dF"],  n_bins, labels=False, duplicates="drop")

    grid_p = tmp.pivot_table(index="dF_bin", columns="F_bin", values="p", aggfunc="mean")
    grid_n = tmp.pivot_table(index="dF_bin", columns="F_bin", values="p", aggfunc="size").fillna(0)

    # mask low count
    masked = grid_p.copy()
    masked[grid_n < min_count] = np.nan

    plt.figure()
    plt.imshow(masked.values, aspect="auto", origin="lower")
    plt.colorbar(label="mean p(tail)")
    plt.title(f"Hysteresis surface: p(tail) vs (F, dF) | min_count={min_count}")
    plt.xlabel("F-bin (low → high)")
    plt.ylabel("dF-bin (low → high)")

    # annotate counts lightly
    for i in range(masked.shape[0]):
        for j in range(masked.shape[1]):
            n = int(grid_n.values[i, j])
            if n >= min_count:
                plt.text(j, i, str(n), ha="center", va="center", fontsize=7)

    if save: plt.savefig(save, dpi=220, bbox_inches="tight")
    plt.show()

hysteresis_heatmap(df_eval, p_oos, n_bins=12, min_count=40)

def plot_q_sweep(res, title="q-sweep", save=None):
    # res: output df from run_table()
    plt.figure()
    for m, g in res.groupby("model"):
        g = g.sort_values("q")
        plt.plot(g["q"], g["MAR_strat"], label=m)
    plt.title(f"{title}: MAR vs q")
    plt.xlabel("q (quantile cut on score)")
    plt.ylabel("MAR (CAGR/|MaxDD|)")
    plt.legend()
    if save: plt.savefig(save, dpi=220, bbox_inches="tight")
    plt.show()

    plt.figure()
    for m, g in res.groupby("model"):
        g = g.sort_values("q")
        plt.plot(g["q"], g["avg_expo"], label=m)
    plt.title(f"{title}: avg exposure vs q")
    plt.xlabel("q")
    plt.ylabel("avg exposure")
    plt.legend()
    if save: plt.savefig(save, dpi=220, bbox_inches="tight")
    plt.show()

plot_q_sweep(res, title="OOS (tail10)")

def hysteresis_panel(df, score, n_bins=8, min_count=30, use_qcut=True, save=None):
    tmp = df.loc[score.index, ["F_t"]].copy()
    tmp["dF"] = df.loc[score.index, "F_t"].diff()
    tmp["s"]  = score.values
    tmp = tmp.dropna()

    if use_qcut:
        tmp["F_bin"]  = pd.qcut(tmp["F_t"], n_bins, labels=False, duplicates="drop")
        tmp["dF_bin"] = pd.qcut(tmp["dF"],  n_bins, labels=False, duplicates="drop")
    else:
        # fixed bins on z-scored variables (more comparable across assets)
        Fz  = (tmp["F_t"] - tmp["F_t"].mean()) / tmp["F_t"].std()
        dFz = (tmp["dF"]  - tmp["dF"].mean()) / tmp["dF"].std()
        tmp["F_bin"]  = pd.cut(Fz,  n_bins, labels=False)
        tmp["dF_bin"] = pd.cut(dFz, n_bins, labels=False)

    grid_s = tmp.pivot_table(index="dF_bin", columns="F_bin", values="s", aggfunc="mean")
    grid_n = tmp.pivot_table(index="dF_bin", columns="F_bin", values="s", aggfunc="size").fillna(0)

    masked = grid_s.copy()
    masked[grid_n < min_count] = np.nan

    plt.figure()
    plt.imshow(grid_n.values, aspect="auto", origin="lower")
    plt.colorbar(label="count")
    plt.title(f"Counts | n_bins={n_bins}")
    plt.xlabel("F-bin (low→high)")
    plt.ylabel("dF-bin (low→high)")
    if save: plt.savefig(save.replace(".png","_counts.png"), dpi=220, bbox_inches="tight")
    plt.show()

    plt.figure()
    plt.imshow(masked.values, aspect="auto", origin="lower")
    plt.colorbar(label="mean score")
    plt.title(f"Hysteresis surface: mean score | min_count={min_count}")
    plt.xlabel("F-bin (low→high)")
    plt.ylabel("dF-bin (low→high)")
    if save: plt.savefig(save.replace(".png","_score.png"), dpi=220, bbox_inches="tight")
    plt.show()

hysteresis_panel(df_eval, p_oos, n_bins=8, min_count=30, use_qcut=True)

# ============================================================
# CARIA: End-to-end (signals -> latent fragility F_t -> OOS calibrated risk -> strategy -> plots + tables)
# Works in Google Colab
# ============================================================

# --- Install deps (Colab) ---
!pip -q install yfinance scikit-learn scipy

import warnings, math
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from scipy import signal
from scipy.ndimage import gaussian_filter1d

from sklearn.covariance import LedoitWolf
from sklearn.decomposition import FactorAnalysis
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

import yfinance as yf

# ----------------------------
# 0) CONFIG
# ----------------------------
DATA_SOURCE = "yfinance"  # "yfinance" | "alphavantage" | "fmp"
START = "2005-01-01"
END   = None  # None = today

# Universe (feel free to edit)
TICKERS = [
    "SPY","QQQ","IWM","DIA","EFA",
    "EWJ","EWG","EWU","EWC","EWA","EWH","FXI","EWY","EZA","EWI","EEM","EWW","EWT","EWS","EWQ","EWD","EWN"
]

# Structural metrics settings
STRUCT_WINDOW = 252
STRUCT_STEP   = 5
MIN_ASSETS    = 20
COVERAGE_TH   = 0.90

# EWS & CF settings
CF_W     = 20
SYNC_W   = 60
EWS_W    = 120
CURV_W   = 60
PEAK_W   = 60

# OOS folds (trading-days based)
TRAIN_N = 8*252
TEST_N  = 126
PURGE_N = 60
STEP_N  = 126

# Strategy settings
Q_EXPO   = 0.80      # target exposure fraction (calibration)
Q_TAIL   = 0.10      # tail event definition (bottom q of future returns, calibrated per fold)
TC_BPS   = 0.0       # set to 5.0 for 5 bps switching cost

# ----------------------------------------------------------
# 1) DATA LOADER (yfinance default; AV/FMP hooks included)
# ----------------------------------------------------------
def fetch_prices_yf(tickers, start, end=None):
    px = yf.download(tickers, start=start, end=end, progress=False, auto_adjust=True)["Close"]
    if isinstance(px, pd.Series):
        px = px.to_frame()
    px.index = pd.to_datetime(px.index)
    px = px.sort_index()
    return px

def fetch_prices_alpha_vantage(tickers, api_key, start, end=None):
    # Note: Alpha Vantage has strict rate limits.
    import requests, time
    out = {}
    for t in tickers:
        url = "https://www.alphavantage.co/query"
        params = {"function":"TIME_SERIES_DAILY_ADJUSTED","symbol":t,"outputsize":"full","apikey":api_key}
        r = requests.get(url, params=params, timeout=30).json()
        key = "Time Series (Daily)"
        if key not in r:
            raise RuntimeError(f"AlphaVantage failed for {t}: {list(r.keys())[:5]}")
        s = pd.Series({pd.to_datetime(k): float(v["5. adjusted close"]) for k,v in r[key].items()})
        s = s.sort_index()
        out[t] = s
        time.sleep(12)  # be kind to AV limits
    px = pd.DataFrame(out).loc[pd.to_datetime(start):]
    if end is not None:
        px = px.loc[:pd.to_datetime(end)]
    return px

def fetch_prices_fmp(tickers, api_key, start, end=None):
    # FMP endpoint example: /api/v3/historical-price-full/{symbol}?from=...&to=...&apikey=...
    import requests
    out = {}
    for t in tickers:
        url = f"https://financialmodelingprep.com/api/v3/historical-price-full/{t}"
        params = {"from":start, "to": (end or pd.Timestamp.today().strftime("%Y-%m-%d")), "apikey":api_key}
        r = requests.get(url, params=params, timeout=30).json()
        if "historical" not in r:
            raise RuntimeError(f"FMP failed for {t}: {list(r.keys())[:5]}")
        dfh = pd.DataFrame(r["historical"])
        dfh["date"] = pd.to_datetime(dfh["date"])
        dfh = dfh.sort_values("date")
        out[t] = pd.Series(dfh["adjClose"].values, index=dfh["date"].values)
    px = pd.DataFrame(out).sort_index()
    px = px.loc[pd.to_datetime(start):]
    if end is not None:
        px = px.loc[:pd.to_datetime(end)]
    return px

if DATA_SOURCE == "yfinance":
    prices = fetch_prices_yf(TICKERS, START, END)
elif DATA_SOURCE == "alphavantage":
    AV_KEY = "PUT_YOUR_ALPHA_VANTAGE_KEY_HERE"
    prices = fetch_prices_alpha_vantage(TICKERS, AV_KEY, START, END)
elif DATA_SOURCE == "fmp":
    FMP_KEY = "PUT_YOUR_FMP_KEY_HERE"
    prices = fetch_prices_fmp(TICKERS, FMP_KEY, START, END)
else:
    raise ValueError("DATA_SOURCE must be yfinance|alphavantage|fmp")

# Basic cleaning
prices = prices.dropna(how="all")
prices = prices.loc[:, prices.notna().sum() > 200]  # keep non-empty series

print("Prices:", prices.shape, "|", prices.index.min(), "->", prices.index.max())
assert "SPY" in prices.columns, "Need SPY in tickers (used as market proxy)."

# Daily log-returns (more stable for cov)
ret = np.log(prices).diff()

# ----------------------------------------------------------
# 2) SIGNALS: CF, SYNC, EWS(acf/var/skew), CURV, PEAK_60
# ----------------------------------------------------------
def avg_pairwise_corr(df):
    C = df.corr().values
    n = C.shape[0]
    if n < 2:
        return np.nan
    return (C.sum() - n) / (n*(n-1))

def compute_cf(r, w=20):
    cf = []
    idx = []
    for i in range(w, len(r)):
        W = r.iloc[i-w:i].dropna(axis=1, how="any")
        if W.shape[1] < MIN_ASSETS:
            cf.append(np.nan); idx.append(r.index[i]); continue
        ac = avg_pairwise_corr(W)
        vol = W.std().mean()
        cf.append(ac * vol * 100.0)
        idx.append(r.index[i])
    return pd.Series(cf, index=idx, name="cf").astype(float)

def extract_phase_series(x, detrend_sigma=60):
    x = pd.Series(x).astype(float).fillna(method="ffill").fillna(method="bfill").values
    detr = x - gaussian_filter1d(x, sigma=detrend_sigma)
    ph = np.angle(signal.hilbert(detr))
    return ph

def kuramoto_order(phases_df, window=60):
    out = []
    idx = []
    for i in range(window, len(phases_df)):
        ph = phases_df.iloc[i].values
        r = np.abs(np.exp(1j*ph).mean())
        out.append(r); idx.append(phases_df.index[i])
    return pd.Series(out, index=idx, name="sync").astype(float)

def compute_ews(series, window=120):
    s = pd.Series(series).astype(float)
    acf1 = s.rolling(window).apply(lambda x: pd.Series(x).autocorr(lag=1), raw=False)
    var  = s.rolling(window).var()
    skew = s.rolling(window).skew()
    return pd.DataFrame({"acf1": acf1, "var": var, "skew": skew}).astype(float)

def rolling_avg_corr(r, window=60):
    out = []
    idx = []
    for i in range(window, len(r)):
        W = r.iloc[i-window:i].dropna(axis=1, how="any")
        if W.shape[1] < MIN_ASSETS:
            out.append(np.nan); idx.append(r.index[i]); continue
        out.append(avg_pairwise_corr(W))
        idx.append(r.index[i])
    return pd.Series(out, index=idx, name="avg_corr").astype(float)

CF   = compute_cf(ret, w=CF_W)

# SYNC: phases of each asset return series
phases = pd.DataFrame({c: extract_phase_series(ret[c]) for c in ret.columns}, index=ret.index)
SYNC = kuramoto_order(phases, window=SYNC_W)

EWS = compute_ews(CF, window=EWS_W)

AVG_CORR = rolling_avg_corr(ret, window=CURV_W)     # topology proxy (more correlated = more rigid)
CURV     = (1.0 - AVG_CORR).rename("curv")          # your proxy: low corr -> "low curvature" -> fragile

PEAK_60  = CF.rolling(PEAK_W).max().rename("peak_60")

# Align core signals
signals_core = pd.concat(
    [CF, SYNC, EWS["acf1"], EWS["var"], EWS["skew"].abs(), CURV, AVG_CORR.rename("topology"), PEAK_60],
    axis=1
).dropna()
print("Core signals aligned:", signals_core.shape)

# ----------------------------------------------------------
# 3) STRUCTURAL METRICS (Absorption Ratio + Entropy)
# ----------------------------------------------------------
def cov_to_corr(S):
    d = np.sqrt(np.diag(S))
    d = np.where(d == 0, 1e-12, d)
    C = S / np.outer(d, d)
    return np.nan_to_num((C + C.T)/2.0)

def eig_metrics(C, k_frac=0.2):
    w = np.sort(np.linalg.eigvalsh(C))[::-1]
    w = np.maximum(w, 1e-12)
    k = max(1, int(np.ceil(k_frac * len(w))))
    ar = float(np.sum(w[:k]) / np.sum(w))
    p  = w / np.sum(w)
    ent = float(-np.sum(p * np.log(p + 1e-12)) / np.log(len(w))) if len(w) > 1 else 0.5
    return ar, ent

def rolling_struct_metrics(returns, window=252, step=5, min_assets=20, coverage_th=0.9):
    lw = LedoitWolf()
    good = returns.notna().mean() >= coverage_th
    R = returns.loc[:, good].copy()
    print(f"Using {R.shape[1]} assets with >{int(coverage_th*100)}% coverage")

    struct = pd.DataFrame(index=R.index, columns=["absorption_ratio","entropy"], dtype=float)

    total_steps = max(0, (len(R) - window)//step)
    print(f"Calculating AR + Entropy: window={window}, step={step}, steps={total_steps}")

    for j, t in enumerate(range(window, len(R), step)):
        W = R.iloc[t-window:t].copy()
        W = W.loc[:, W.notna().mean() >= coverage_th]
        if W.shape[1] < min_assets:
            continue
        W = W.apply(lambda s: s.fillna(s.mean()))
        X = W.values - np.nanmean(W.values, axis=0)

        try:
            S = lw.fit(X).covariance_
            C = cov_to_corr(S)
        except:
            C = np.corrcoef(X, rowvar=False)
            C = np.nan_to_num((C + C.T)/2.0)

        ar, ent = eig_metrics(C)
        struct.iloc[t] = [ar, ent]

        if (j+1) % 200 == 0:
            print(f"  {j+1}/{total_steps} ({(j+1)/max(total_steps,1)*100:.0f}%)")

    struct = struct.ffill().bfill()
    struct.index.name = "date"
    return struct

struct = rolling_struct_metrics(ret, window=STRUCT_WINDOW, step=STRUCT_STEP, min_assets=MIN_ASSETS, coverage_th=COVERAGE_TH)
print("Struct head/tail:\n", struct.head(), "\n", struct.tail())

# Squash-to-(-1,1) "z" for stability (matches your ~±0.99 style)
def squash_z(s):
    s = pd.Series(s).astype(float)
    z = (s - s.mean())/(s.std() + 1e-12)
    return np.tanh(z/3.0)

absorp_z = squash_z(struct["absorption_ratio"]).rename("absorp_z")
ent_z    = (-squash_z(struct["entropy"])).rename("ent_z")  # negative direction: higher entropy -> less fragile

# Merge into signals
signals = signals_core.join([absorp_z, ent_z], how="inner").dropna()
print("Signals + struct aligned:", signals.shape)

# ----------------------------------------------------------
# 4) LATENT FRAGILITY: Factor Analysis -> F_t
# ----------------------------------------------------------
FA_COLS = ["cf","sync","acf1","var","skew","curv","absorp_z","ent_z","peak_60","topology"]
X = signals[FA_COLS].copy().dropna()

sc = StandardScaler()
Xz = sc.fit_transform(X.values)

fa = FactorAnalysis(n_components=1, random_state=42)
F = fa.fit_transform(Xz).reshape(-1)
loadings = fa.components_.reshape(-1)

Ft = pd.Series((F - F.mean())/(F.std()+1e-12), index=X.index, name="F_t")
loading_tbl = pd.DataFrame({"signal": FA_COLS, "loading": loadings}).sort_values("loading")
print("\n=== Factor Loadings (FA 1-factor) ===")
print(loading_tbl.to_string(index=False))
print("\nFt stats:\n", Ft.describe())

# Add Ft to evaluation frame
df_eval = signals.join(Ft, how="inner").dropna()

# ----------------------------------------------------------
# 5) TARGETS: future 22d return + tail event (calibrated per fold)
# ----------------------------------------------------------
H = 22
spy = prices["SPY"].reindex(df_eval.index).dropna()
fut_ret_22 = (spy.shift(-H)/spy - 1.0).rename("fut_ret_22")
df_eval = df_eval.join(fut_ret_22, how="inner").dropna()
print("df_eval:", df_eval.shape, "|", df_eval.index.min(), "->", df_eval.index.max())

# daily market return for strategy PnL
mkt_ret = spy.pct_change().rename("mkt_ret")
df_eval = df_eval.join(mkt_ret, how="inner").dropna()

# ----------------------------------------------------------
# 6) WALK-FORWARD FOLDS (index-based, robust)
# ----------------------------------------------------------
def make_folds_idx(df,
                   train_n=8*252, test_n=126, purge_n=60, step_n=126,
                   min_train=1000, min_test=80):
    n = len(df)
    folds = []
    k = 1
    start_test = train_n + purge_n  # 0-based
    while (start_test + test_n) <= n:
        train_end = start_test - purge_n
        test_end  = start_test + test_n
        if train_end >= min_train and (test_end - start_test) >= min_test:
            folds.append({
                "fold": k,
                "train": df.iloc[:train_end].copy(),
                "test":  df.iloc[start_test:test_end].copy(),
                "train_end": df.index[train_end-1],
                "test_start": df.index[start_test],
                "test_end": df.index[test_end-1],
            })
            k += 1
        start_test += step_n
    return folds

folds = make_folds_idx(df_eval, TRAIN_N, TEST_N, PURGE_N, STEP_N)
print("Folds:", len(folds))
if folds:
    f0 = folds[0]
    print("Fold1:", f0["train_end"], "->", f0["test_start"], "to", f0["test_end"])

# ----------------------------------------------------------
# 7) METRICS + UTILS (EV, CER, CVaR, DD, etc.)
# ----------------------------------------------------------
def max_drawdown(eq):
    eq = np.asarray(eq, dtype=float)
    peak = np.maximum.accumulate(eq)
    dd = eq/peak - 1.0
    return float(np.min(dd))

def cvar(x, a=0.05):
    x = np.asarray(x, dtype=float)
    x = x[np.isfinite(x)]
    if len(x) == 0:
        return np.nan
    q = np.quantile(x, a)
    tail = x[x <= q]
    return float(np.mean(tail)) if len(tail) else float(q)

def cer_meanvar(x, gamma=3.0):
    x = np.asarray(x, dtype=float)
    mu = np.mean(x)*252
    sig2 = np.var(x)*252
    return float(mu - 0.5*gamma*sig2)

def eval_path(daily_ret):
    x = np.asarray(daily_ret, dtype=float)
    x = x[np.isfinite(x)]
    eq = (1.0 + x).cumprod()
    cagr = float(eq[-1]**(252/len(eq)) - 1.0) if len(eq) else 0.0
    mdd  = max_drawdown(eq) if len(eq) else 0.0
    vol  = float(np.std(x)*np.sqrt(252)) if len(x) else 0.0
    sharpe = float((np.mean(x)*252) / (vol + 1e-12)) if len(x) else 0.0
    out = {
        "CAGR": cagr,
        "MaxDD": mdd,
        "MAR": float(cagr/(abs(mdd)+1e-12)),
        "Vol": vol,
        "Sharpe": sharpe,
        "CVaR5": cvar(x, 0.05),
        "CER_g1": float(np.exp(np.mean(np.log1p(x)))**252 - 1.0) if len(x) else 0.0,
        "CER_g3": cer_meanvar(x, 3.0),
        "CER_g5": cer_meanvar(x, 5.0),
    }
    return out, eq

def apply_tc(expo, tc_bps=0.0):
    # binary expo switching cost
    e = pd.Series(expo).fillna(0.0)
    turn = e.diff().abs().fillna(0.0)
    cost = (tc_bps/1e4) * turn
    return cost

# ----------------------------------------------------------
# 8) OOS CALIBRATED PROBABILITY + STRATEGY
# ----------------------------------------------------------
def fit_predict_proba(train_df, test_df, feature_cols, q_tail=0.10):
    # Tail threshold defined on TRAIN only (avoid leakage)
    y_tr = train_df["fut_ret_22"].values
    thr = np.quantile(y_tr, q_tail)
    y_train = (train_df["fut_ret_22"] <= thr).astype(int).values
    y_test  = (test_df["fut_ret_22"]  <= thr).astype(int).values

    Xtr = train_df[feature_cols].values
    Xte = test_df[feature_cols].values

    # standardize on train only
    ss = StandardScaler()
    Xtrz = ss.fit_transform(Xtr)
    Xtez = ss.transform(Xte)

    clf = LogisticRegression(max_iter=2000, class_weight="balanced", solver="lbfgs")
    clf.fit(Xtrz, y_train)
    p_train = clf.predict_proba(Xtrz)[:,1]
    p_test  = clf.predict_proba(Xtez)[:,1]

    out = {
        "thr": thr,
        "y_test": y_test,
        "p_train": pd.Series(p_train, index=train_df.index),
        "p_test":  pd.Series(p_test,  index=test_df.index),
        "feature_cols": feature_cols
    }
    return out

def oos_run_calibrated(feature_cols, q=0.80, q_tail=0.10, tc_bps=0.0):
    # OOS: build p across folds, each fold has its own model fitted on train
    p_all = []
    expo_all = []
    ret_strat_all = []
    ret_mkt_all = []
    fold_rows = []

    for f in folds:
        tr = f["train"].dropna(subset=feature_cols + ["fut_ret_22","mkt_ret"])
        te = f["test"].dropna(subset=feature_cols + ["fut_ret_22","mkt_ret"])
        if len(tr) < 800 or len(te) < 60:
            continue

        out = fit_predict_proba(tr, te, feature_cols, q_tail=q_tail)

        # Calibrate cut on TRAIN to target exposure q
        cut = float(np.quantile(out["p_train"].values, q))

        # Trade rule: invest when crash-prob <= cut
        expo = (out["p_test"] <= cut).astype(float)

        # next-day execution (avoid same-day usage)
        expo_trade = expo.shift(1).fillna(0.0)

        # returns
        r_mkt = te["mkt_ret"]
        r_strat = expo_trade * r_mkt

        # transaction cost
        cost = apply_tc(expo_trade, tc_bps=tc_bps)
        r_strat = r_strat - cost

        p_all.append(out["p_test"])
        expo_all.append(expo_trade)
        ret_strat_all.append(r_strat)
        ret_mkt_all.append(r_mkt)

        fold_rows.append({
            "fold": f["fold"],
            "start": f["test_start"],
            "end": f["test_end"],
            "avg_expo": float(expo_trade.mean()),
            "cut": cut
        })

    if not p_all:
        raise RuntimeError("No folds produced outputs (check data length / NaNs).")

    p_all = pd.concat(p_all).sort_index()
    expo_all = pd.concat(expo_all).sort_index()
    rS = pd.concat(ret_strat_all).sort_index()
    rM = pd.concat(ret_mkt_all).sort_index()

    # align
    common = rS.index.intersection(rM.index)
    rS = rS.loc[common]
    rM = rM.loc[common]
    expo_all = expo_all.reindex(common).fillna(0.0)
    p_all = p_all.reindex(common).fillna(p_all.median())

    perfS, eqS = eval_path(rS.values)
    perfM, eqM = eval_path(rM.values)

    summary = pd.Series({
        "q": q,
        "q_tail": q_tail,
        "tc_bps": tc_bps,
        "avg_expo": float(expo_all.mean()),
        "n_days": float(len(common)),
        **{f"{k}_strat": v for k,v in perfS.items()},
        **{f"{k}_mkt": v for k,v in perfM.items()},
        "MaxDD_gap": float(perfM["MaxDD"] - perfS["MaxDD"]),   # saved drawdown
        "MAR_gap": float(perfS["MAR"] - perfM["MAR"]),
        "CER_g3_gap": float(perfS["CER_g3"] - perfM["CER_g3"]),
    })

    fold_table = pd.DataFrame(fold_rows)
    return summary, fold_table, pd.Series(eqS, index=common), pd.Series(eqM, index=common), expo_all, p_all

# Model variants (you can add more)
MODELS = {
    "BASE":      ["peak_60","rv_ann"] if "rv_ann" in df_eval.columns else ["peak_60","var"],  # fallback if no rv_ann
    "+absorp_z": ["peak_60","absorp_z"],
    "+ent_z":    ["peak_60","ent_z"],
    "+abs+ent":  ["peak_60","absorp_z","ent_z"],
    "Ft":        ["F_t"],
    "Ft+abs+ent":["F_t","absorp_z","ent_z"]
}
# Keep only existing columns
for k,v in list(MODELS.items()):
    MODELS[k] = [c for model_c in v for c in [model_c] if model_c in df_eval.columns]
    if len(MODELS[k]) == 0:
        del MODELS[k]

print("\nModels:", MODELS)

# Run all models at a few q values (q-sweep)
Q_SWEEP = [0.75, 0.80, 0.825, 0.90, 0.925]
rows = []
for name, cols in MODELS.items():
    for q in Q_SWEEP:
        summ, fold_tab, eqS, eqM, expo, p = oos_run_calibrated(cols, q=q, q_tail=Q_TAIL, tc_bps=TC_BPS)
        rows.append(pd.Series({"model": name, "features": ",".join(cols), **summ.to_dict()}))
res_all = pd.DataFrame(rows).sort_values(["MAR_strat","CAGR_strat"], ascending=False)
display(res_all[["model","q","CAGR_strat","MaxDD_strat","MAR_strat","avg_expo","CER_g3_strat","CER_g3_gap"]].head(20))

# pick best by MAR (or pick what you want)
best = res_all.iloc[0]
BEST_MODEL = best["model"]
BEST_Q = float(best["q"])
print("\nBEST:", BEST_MODEL, "q=", BEST_Q)

# Run best model again to get curves / p / expo / fold table
best_summary, best_folds, eqS, eqM, expo, p = oos_run_calibrated(MODELS[BEST_MODEL], q=BEST_Q, q_tail=Q_TAIL, tc_bps=TC_BPS)

display(best_summary.to_frame("best"))
display(best_folds)

print("\nCapital saved at worst drawdown (per $1): ", float(best_summary["MaxDD_gap"]))

# ----------------------------------------------------------
# 9) PLOTS: equity, drawdown, calibration, hysteresis
# ----------------------------------------------------------
def plot_equity(eqS, eqM, expo=None, title="Equity curve (OOS)"):
    plt.figure(figsize=(12,4))
    plt.plot(eqM.index, eqM.values, label="Market (SPY)")
    plt.plot(eqS.index, eqS.values, label="Strategy")
    plt.legend()
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.show()

    if expo is not None:
        plt.figure(figsize=(12,2.2))
        plt.plot(expo.index, expo.values)
        plt.ylim(-0.05, 1.05)
        plt.title("Exposure (0/1) over time")
        plt.grid(True, alpha=0.3)
    plt.show()

def plot_drawdown(eq, title="Drawdown"):
    x = eq.values
    peak = np.maximum.accumulate(x)
    dd = x/peak - 1.0
    plt.figure(figsize=(12,3))
    plt.plot(eq.index, dd)
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.show()

def calibration_by_decile(p, y, n_bins=10):
    tmp = pd.DataFrame({"p": p, "y": y}).dropna()
    tmp["dec"] = pd.qcut(tmp["p"], n_bins, labels=False, duplicates="drop")
    cal = tmp.groupby("dec").agg(p_mean=("p","mean"), event_rate=("y","mean"), n=("y","size"))
    return cal

def plot_calibration(p, df_eval, q_tail=0.10):
    # Use a single global y defined by full-sample threshold (just for visualization)
    thr = df_eval["fut_ret_22"].quantile(q_tail)
    y = (df_eval["fut_ret_22"] <= thr).astype(int).reindex(p.index)
    cal = calibration_by_decile(p, y, 10)
    plt.figure(figsize=(5,4))
    plt.plot(cal["p_mean"], cal["event_rate"], marker="o")
    plt.plot([0,1],[0,1], linestyle="--")
    plt.title("Calibration (deciles)")
    plt.xlabel("mean predicted p")
    plt.ylabel("event rate")
    plt.grid(True, alpha=0.3)
    plt.show()
    display(cal)

def hysteresis_panel(df_eval, p_series, n_bins=10, min_count=40, use="event"):
    # use="event": mean tail event; use="p": mean predicted probability
    d = df_eval.loc[p_series.index].copy()
    d["p"] = p_series

    # define tail event for viz (global threshold; OOS label differs per fold, but ok for shape)
    thr = d["fut_ret_22"].quantile(Q_TAIL)
    d["tail_evt"] = (d["fut_ret_22"] <= thr).astype(int)

    d["dF"] = d["F_t"].diff()
    d = d.dropna(subset=["F_t","dF"])

    d["F_bin"]  = pd.qcut(d["F_t"], n_bins, labels=False, duplicates="drop") + 1
    d["dF_bin"] = pd.qcut(d["dF"], n_bins, labels=False, duplicates="drop") + 1

    val_col = "tail_evt" if use=="event" else "p"

    # pivot
    piv = d.pivot_table(values=val_col, index="dF_bin", columns="F_bin", aggfunc="mean")
    cnt = d.pivot_table(values=val_col, index="dF_bin", columns="F_bin", aggfunc="size")

    mask = (cnt < min_count) | cnt.isna() | piv.isna()
    Z = piv.mask(mask)

    plt.figure(figsize=(7,5))
    plt.imshow(Z.values, aspect="auto", origin="lower")
    plt.colorbar(label=f"mean {val_col}")
    plt.title(f"Hysteresis surface: mean({val_col}) vs (F, dF) | min_count={min_count}")
    plt.xlabel("F-bin (low → high)")
    plt.ylabel("dF-bin (low → high)")

    # annotate counts where not masked
    for i in range(Z.shape[0]):
        for j in range(Z.shape[1]):
            if not np.isnan(Z.values[i,j]):
                plt.text(j, i, f"{int(cnt.values[i,j])}", ha="center", va="center", fontsize=8)
    plt.show()

    # return tables too
    return piv, cnt

# plots
plot_equity(eqS, eqM, expo, title=f"Equity (best={BEST_MODEL}, q={BEST_Q}, tc={TC_BPS}bps)")
plot_drawdown(eqM, "Drawdown: Market (SPY)")
plot_drawdown(eqS, "Drawdown: Strategy")
plot_calibration(p, df_eval, q_tail=Q_TAIL)

piv, cnt = hysteresis_panel(df_eval, p, n_bins=10, min_count=40, use="event")

# ----------------------------------------------------------
# 10) TABLES: Save for paper
# ----------------------------------------------------------
# Best models table
best_models = res_all.groupby("model").head(1).sort_values("MAR_strat", ascending=False)
best_models.to_csv("table_best_models.csv", index=False)

# q sweep for all
res_all.to_csv("table_q_sweep_all.csv", index=False)

# Tail sensitivity (same best model, vary q_tail)
tail_rows = []
for qt in [0.05, 0.10, 0.15]:
    summ, fold_tab, eqS2, eqM2, expo2, p2 = oos_run_calibrated(MODELS[BEST_MODEL], q=BEST_Q, q_tail=qt, tc_bps=TC_BPS)
    tail_rows.append(pd.Series({"q_tail": qt, **summ.to_dict()}))
tail_sens = pd.DataFrame(tail_rows)
tail_sens.to_csv("table_tail_sensitivity.csv", index=False)

print("\nSaved: table_best_models.csv, table_q_sweep_all.csv, table_tail_sensitivity.csv")
print("\nDONE.")

# =============================
# CARIA: OOS + Economic Value + Hysteresis (FULL)
# =============================
import numpy as np
import pandas as pd
from scipy import signal
from scipy.ndimage import gaussian_filter1d
import matplotlib.pyplot as plt

from sklearn.covariance import LedoitWolf
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import FactorAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression

# ----------------------------
# 0) Optional: Fetch prices if you don't have them
# ----------------------------
def fetch_prices_yf(tickers, start="2005-01-01"):
    import yfinance as yf
    px = yf.download(tickers, start=start, progress=False)["Close"]
    if isinstance(px, pd.Series):
        px = px.to_frame(tickers[0])
    px.index = pd.to_datetime(px.index)
    return px.sort_index()

# ----------------------------
# 1) Cleaning / Returns
# ----------------------------
def prep_prices(prices, min_coverage=0.90):
    px = prices.copy()
    px.index = pd.to_datetime(px.index)
    px = px.sort_index()
    cov = px.notna().mean()
    keep = cov[cov >= min_coverage].index.tolist()
    px = px[keep]
    # fill only for small gaps; for big holes, better remove the asset (already done via coverage)
    px = px.ffill().bfill()
    return px

def log_returns(px):
    return np.log(px).diff().dropna()

def pct_returns(px):
    return px.pct_change().dropna()

# ----------------------------
# 2) Core signals (CF, SYNC, EWS, CURV)
# ----------------------------
def compute_cf(ret, w=20):
    cf = []
    idx = ret.index
    for i in range(w, len(ret)):
        W = ret.iloc[i-w:i]
        C = W.corr().values
        n = C.shape[0]
        ac = (C.sum() - n) / (n * (n - 1))
        cf.append(ac * W.std().mean() * 100.0)
    return pd.Series(cf, index=idx[w:], name="cf")

def extract_phase(series, smooth_sigma=60):
    x = np.asarray(series, float)
    trend = gaussian_filter1d(x, sigma=smooth_sigma)
    detr = x - trend
    return np.angle(signal.hilbert(detr))

def kuramoto_order(phases_df, window=60):
    r = []
    idx = phases_df.index
    for i in range(window, len(phases_df)):
        ph = phases_df.iloc[i].values
        r.append(np.abs(np.exp(1j * ph).mean()))
    return pd.Series(r, index=idx[window:], name="sync")

def compute_ews(series, window=120):
    s = series.copy()
    out = pd.DataFrame(index=s.index)
    out["acf1"] = s.rolling(window).apply(lambda x: pd.Series(x).autocorr(lag=1), raw=False)
    out["var"]  = s.rolling(window).var()
    out["skew"] = s.rolling(window).skew()
    return out

def rolling_avg_corr(ret, window=60):
    vals = []
    idx = ret.index
    for i in range(window, len(ret)):
        W = ret.iloc[i-window:i]
        C = W.corr().values
        n = C.shape[0]
        ac = (C.sum() - n) / (n * (n - 1))
        vals.append(ac)
    return pd.Series(vals, index=idx[window:], name="avg_corr")

# -----------------------------
# 3) Structural metrics (Absorption ratio + Entropy)
# -----------------------------
def cov_to_corr(S):
    d = np.sqrt(np.diag(S))
    d = np.where(d == 0, 1e-10, d)
    C = S / np.outer(d, d)
    return np.nan_to_num((C + C.T) / 2)

def eig_metrics(C, k_frac=0.2):
    w = np.sort(np.linalg.eigvalsh(C))[::-1]
    w = np.maximum(w, 1e-10)
    k = max(1, int(np.ceil(k_frac * len(w))))
    ar = np.sum(w[:k]) / np.sum(w)
    p = w / np.sum(w)
    ent = -np.sum(p * np.log(p + 1e-10)) / np.log(len(w)) if len(w) > 1 else 0.5
    return float(ar), float(ent)

def rolling_struct_metrics(ret_log, window=252, step=5, min_assets=20, coverage_in_window=0.90):
    lw = LedoitWolf()
    out = pd.DataFrame(index=ret_log.index, columns=["absorption_ratio","entropy"], dtype=float)

    total_steps = (len(ret_log) - window) // step
    print(f"Calculating AR + Entropy: window={window}, step={step}, steps={total_steps}")

    for j, t in enumerate(range(window, len(ret_log), step), start=1):
        W = ret_log.iloc[t-window:t]
        good = W.notna().mean() >= coverage_in_window
        W = W.loc[:, good]
        if W.shape[1] < min_assets:
            continue

        W = W.apply(lambda s: s.fillna(s.mean()))
        X = W.values - np.nanmean(W.values, axis=0)

        try:
            S = lw.fit(X).covariance_
            C = cov_to_corr(S)
        except:
            C = np.corrcoef(X, rowvar=False)
            C = np.nan_to_num((C + C.T) / 2)

        ar, ent = eig_metrics(C)
        out.iloc[t] = [ar, ent]

        if j % 200 == 0:
            print(f"  {j}/{total_steps} ({j/total_steps*100:.0f}%)")

    return out.ffill().bfill()

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import FactorAnalysis

def zscore(s):
    s = pd.Series(s).astype(float).replace([np.inf, -np.inf], np.nan)
    s = s.ffill().bfill()
    return (s - s.mean()) / (s.std(ddof=0) + 1e-12)

def build_signals(prices,
                  cf_w=20, sync_w=60, ews_w=120, curv_w=60,
                  struct_window=252, struct_step=5,
                  min_assets=20):

    px = prep_prices(prices, min_coverage=0.90)
    ret = pct_returns(px)

    CF = compute_cf(ret, w=cf_w)

    phases = pd.DataFrame({c: extract_phase(ret[c].fillna(0)) for c in ret.columns}, index=ret.index)
    SYNC = kuramoto_order(phases, window=sync_w)

    EWS = compute_ews(CF, window=ews_w)
    CURV = 1.0 - rolling_avg_corr(ret, window=curv_w)

    peak_60 = CF.rolling(60).mean().rename("peak_60")

    # Align core signals
    common = CF.index
    for s in [SYNC, EWS["acf1"], EWS["var"], EWS["skew"], CURV, peak_60]:
        common = common.intersection(s.dropna().index)

    core = pd.DataFrame({
        "cf": CF.loc[common],
        "sync": SYNC.loc[common],
        "acf1": EWS["acf1"].loc[common],
        "var": EWS["var"].loc[common],
        "skew": EWS["skew"].abs().loc[common],
        "curv": CURV.loc[common],
        "peak_60": peak_60.loc[common],
    }).dropna()
    print(f"Core signals aligned: {core.shape}")

    # Structural metrics (log-returns)
    ret_log = log_returns(px)
    good_coverage = ret_log.notna().mean() >= 0.90
    ret_log = ret_log.loc[:, good_coverage]
    n_assets = ret_log.shape[1]
    print(f"Using {n_assets} assets with >90% coverage for struct")

    # 🔥 CRÍTICO: no pedir más assets de los que tienes
    min_assets_eff = min(min_assets, n_assets)
    if min_assets_eff < 3:
        raise ValueError(f"Muy pocos assets para struct: {n_assets}")

    struct = rolling_struct_metrics(
        ret_log,
        window=struct_window,
        step=struct_step,
        min_assets=min_assets_eff,
        coverage_in_window=0.90
    )

    # Alinea TODO al final (core + struct)
    idx = core.index.intersection(struct.index)
    core2 = core.loc[idx].copy()
    struct2 = struct.loc[idx].copy()

    core2["absorp_z"] = zscore(struct2["absorption_ratio"])
    core2["ent_z"]    = zscore(struct2["entropy"])

    # Factor Analysis
    fa_cols = ["cf","sync","acf1","var","skew","curv","peak_60","absorp_z","ent_z"]
    X = core2[fa_cols].dropna()
    print("FA input shape:", X.shape)
    if len(X) == 0:
        raise ValueError("FA quedó vacío: revisa NaNs en absorp_z/ent_z o en señales base.")

    Xs = StandardScaler().fit_transform(X.values)
    fa = FactorAnalysis(n_components=1, random_state=0)
    Ft = pd.Series(fa.fit_transform(Xs).ravel(), index=X.index, name="F_t")
    Ft = zscore(Ft)

    loadings = pd.DataFrame({"signal": fa_cols, "loading": fa.components_.ravel()}) \
                .sort_values("loading").reset_index(drop=True)

    df_sig = core2.join(Ft, how="inner")
    return df_sig, loadings, px


# -----------------------------
# 5) Targets: future return + tail event (tail defined per fold)
# -----------------------------
def add_targets(df_sig, spy_prices, horizon=22):
    px = spy_prices.copy().sort_index()
    px = px.loc[df_sig.index].ffill()
    fut = px.shift(-horizon) / px - 1.0

    out = df_sig.copy()
    out["fut_ret_22"] = fut
    out = out.dropna()
    return out

# -----------------------------
# 6) Walk-forward folds by index with purge
# -----------------------------
def make_folds_idx(df, train_n=8*252, test_n=126, purge_n=60, step_n=126, min_train=1000, min_test=80):
    n = len(df)
    folds = []
    start_test = train_n + purge_n + 1
    while (start_test + test_n - 1) <= n:
        train_end = start_test - purge_n - 1
        test_end  = start_test + test_n - 1
        tr = df.iloc[:train_end].copy()
        te = df.iloc[start_test:test_end].copy()
        if len(tr) >= min_train and len(te) >= min_test:
            folds.append((tr, te))
        start_test += step_n
    return folds

# -----------------------------
# 7) Metrics + utilities
# -----------------------------
def max_drawdown(eq):
    eq = np.asarray(eq, float)
    peak = np.maximum.accumulate(eq)
    dd = eq / peak - 1.0
    return float(dd.min())

def cvar(x, alpha=0.05):
    x = np.asarray(x, float)
    q = np.quantile(x, alpha)
    return float(x[x <= q].mean()) if np.any(x <= q) else float(q)

def cer_meanvar(daily_ret, gamma=3.0):
    r = np.asarray(daily_ret, float)
    mu = np.mean(r)*252
    sig = np.std(r)*np.sqrt(252)
    return float(mu - 0.5*gamma*(sig**2))

def eval_path(daily_ret):
    r = np.asarray(daily_ret, float)
    eq = np.cumprod(1.0 + r)
    cagr = float(eq[-1]**(252/len(eq)) - 1.0)
    vol  = float(np.std(r)*np.sqrt(252))
    sharpe = float((np.mean(r)*252) / (vol + 1e-12))
    mdd = max_drawdown(eq)
    return {
        "CAGR": cagr,
        "Vol": vol,
        "Sharpe": sharpe,
        "MaxDD": mdd,
        "MAR": float(cagr / (abs(mdd) + 1e-12)),
        "CVaR5": cvar(r, 0.05),
        "CER_g3": cer_meanvar(r, gamma=3.0),
        "eq": eq
    }

def turnover(expo):
    e = pd.Series(expo).astype(float)
    return float(e.diff().abs().fillna(0).mean())

# -----------------------------
# 8) OOS run: model -> prob -> exposure rule
# -----------------------------
def oos_run(df_eval, spy_daily_ret, features, q=0.80, q_tail=0.10,
            train_n=8*252, test_n=126, purge_n=60, step_n=126,
            calibrate="isotonic", tc_bps=0.0):

    df = df_eval.copy()
    df = df.dropna(subset=features + ["fut_ret_22"])

    folds = make_folds_idx(df, train_n, test_n, purge_n, step_n)
    print("Folds:", len(folds))
    if len(folds) == 0:
        raise ValueError("No folds: revisa train/test/purge/step vs tamaño de df_eval")

    all_p = pd.Series(index=df.index, dtype=float)
    all_expo = pd.Series(index=df.index, dtype=float)

    cut_by_fold = []

    for k, (tr, te) in enumerate(folds, start=1):
        thr = tr["fut_ret_22"].quantile(q_tail)
        y_tr = (tr["fut_ret_22"] <= thr).astype(int).values

        X_tr = tr[features].values
        X_te = te[features].values

        sc = StandardScaler()
        X_trs = sc.fit_transform(X_tr)
        X_tes = sc.transform(X_te)

        clf = LogisticRegression(max_iter=500, class_weight="balanced")
        clf.fit(X_trs, y_tr)

        p_tr = clf.predict_proba(X_trs)[:, 1]
        p_te = clf.predict_proba(X_tes)[:, 1]

        if calibrate == "isotonic":
            iso = IsotonicRegression(out_of_bounds="clip")
            iso.fit(p_tr, y_tr)
            p_te = iso.transform(p_te)

        cut = np.quantile(p_tr, q)         # risk-off above cut
        expo = (p_te < cut).astype(float)  # 1 = long SPY, 0 = cash

        all_p.loc[te.index] = p_te
        all_expo.loc[te.index] = expo

        cut_by_fold.append({
            "fold": k,
            "start": te.index.min(),
            "end": te.index.max(),
            "avg_expo": float(expo.mean()),
            "cut": float(cut),
        })

    # Align to daily returns (trade next day)
    expo_d = all_expo.reindex(spy_daily_ret.index).ffill().shift(1).fillna(1.0)

    # transaction costs on switches
    if tc_bps > 0:
        turn = expo_d.diff().abs().fillna(0.0)
        cost = (tc_bps / 10000.0) * turn
    else:
        cost = 0.0

    strat_ret = spy_daily_ret * expo_d - cost
    mkt_ret = spy_daily_ret.copy()

    ES = eval_path(strat_ret)
    EM = eval_path(mkt_ret)

    summary = {
        "q": q, "q_tail": q_tail, "tc_bps": tc_bps,
        "avg_expo": float(expo_d.mean()),
        "turnover": turnover(expo_d),
        "n_days": int(len(strat_ret)),

        "CAGR_strat": ES["CAGR"], "MaxDD_strat": ES["MaxDD"], "MAR_strat": ES["MAR"],
        "Vol_strat": ES["Vol"], "Sharpe_strat": ES["Sharpe"], "CVaR5_strat": ES["CVaR5"],
        "CER_g3_strat": ES["CER_g3"],

        "CAGR_mkt": EM["CAGR"], "MaxDD_mkt": EM["MaxDD"], "MAR_mkt": EM["MAR"],
        "Vol_mkt": EM["Vol"], "Sharpe_mkt": EM["Sharpe"], "CVaR5_mkt": EM["CVaR5"],
        "CER_g3_mkt": EM["CER_g3"],

        # IMPORTANT: correct sign definition
        "capital_saved_at_worstDD": abs(EM["MaxDD"]) - abs(ES["MaxDD"]),
        "CER_g3_gap": ES["CER_g3"] - EM["CER_g3"],
        "MAR_gap": ES["MAR"] - EM["MAR"],
        "MaxDD_gap": abs(EM["MaxDD"]) - abs(ES["MaxDD"]),
    }

    out = pd.DataFrame({
        "p": all_p.reindex(spy_daily_ret.index).ffill(),
        "expo": expo_d,
        "mkt_ret": mkt_ret,
        "strat_ret": strat_ret,
        "eq_mkt": np.cumprod(1 + mkt_ret),
        "eq_strat": np.cumprod(1 + strat_ret),
    })

    cut_df = pd.DataFrame(cut_by_fold)
    return summary, out, cut_df

# -----------------------------
# 9) Calibration plot (deciles)
# -----------------------------
def calibration_plot(out, df_eval, horizon=22, q_tail=0.10, n_bins=10, title="Calibration (deciles)"):
    # align scores (p) to df_eval target dates
    s = out["p"].reindex(df_eval.index).ffill()
    d = df_eval[["fut_ret_22"]].copy()
    d["p"] = s
    d = d.dropna()

    thr = d["fut_ret_22"].quantile(q_tail)
    d["evt"] = (d["fut_ret_22"] <= thr).astype(int)

    d["bin"] = pd.qcut(d["p"], n_bins, labels=False, duplicates="drop")
    g = d.groupby("bin").agg(p_mean=("p","mean"), event_rate=("evt","mean"), n=("evt","size"))

    plt.figure(figsize=(4.5,4))
    plt.plot(g["p_mean"], g["event_rate"], marker="o")
    plt.plot([0,1],[0,1], linestyle="--")
    plt.xlabel("mean predicted p")
    plt.ylabel("event rate")
    plt.title(title)
    plt.grid(True, alpha=0.2)
    plt.tight_layout()
    return g

# -----------------------------
# 10) Hysteresis surface: tail_evt vs (score, dscore)
# -----------------------------
def hysteresis_surface(df_eval, score_series, n_bins=10, min_count=40, q_tail=0.10, title=None):
    df = df_eval.copy()
    s = pd.Series(score_series).reindex(df.index)
    df = df.assign(score=s).dropna(subset=["score","fut_ret_22"])

    thr = df["fut_ret_22"].quantile(q_tail)
    df["tail_evt"] = (df["fut_ret_22"] <= thr).astype(int)

    df["dscore"] = df["score"].diff()

    df["F_bin"]  = pd.qcut(df["score"], n_bins, labels=False, duplicates="drop")
    df["dF_bin"] = pd.qcut(df["dscore"].dropna(), n_bins, labels=False, duplicates="drop")
    df = df.dropna(subset=["F_bin","dF_bin"])

    piv = df.pivot_table(index="dF_bin", columns="F_bin", values="tail_evt", aggfunc="mean")
    cnt = df.pivot_table(index="dF_bin", columns="F_bin", values="tail_evt", aggfunc="size")

    piv = piv.where(cnt >= min_count)

    plt.figure(figsize=(6.5,4.8))
    plt.imshow(piv.values, aspect="auto", origin="lower")
    plt.colorbar(label="mean tail_evt")
    plt.xlabel("F-bin (low → high)")
    plt.ylabel("dF-bin (low → high)")
    plt.title(title or f"Hysteresis surface: mean(tail_evt) vs (F, dF) | min_count={min_count}")
    for i in range(piv.shape[0]):
        for j in range(piv.shape[1]):
            if not np.isnan(piv.values[i, j]):
                plt.text(j, i, int(cnt.values[i, j]), ha="center", va="center", fontsize=8)
    plt.tight_layout()
    return piv, cnt

# -----------------------------
# 11) Plot helpers
# -----------------------------
def drawdown_series(eq):
    eq = np.asarray(eq, float)
    peak = np.maximum.accumulate(eq)
    return eq/peak - 1.0

def plot_equity(out, title="Equity"):
    plt.figure(figsize=(10,4))
    plt.plot(out.index, out["eq_mkt"], label="Market (SPY)")
    plt.plot(out.index, out["eq_strat"], label="Strategy")
    plt.legend()
    plt.title(title)
    plt.grid(True, alpha=0.2)
    plt.tight_layout()

def plot_drawdown(out, title="Drawdown"):
    plt.figure(figsize=(10,3.2))
    plt.plot(out.index, drawdown_series(out["eq_mkt"]), label="DD market")
    plt.plot(out.index, drawdown_series(out["eq_strat"]), label="DD strat")
    plt.legend()
    plt.title(title)
    plt.grid(True, alpha=0.2)
    plt.tight_layout()

def plot_signal_vs_price(px_spy, score, title="Signal vs SPY"):
    s = pd.Series(score).reindex(px_spy.index).ffill()
    plt.figure(figsize=(10,3.2))
    plt.plot(px_spy.index, zscore(px_spy), label="SPY (z)")
    plt.plot(s.index, zscore(s), label="score (z)")
    plt.legend()
    plt.title(title)
    plt.grid(True, alpha=0.2)
    plt.tight_layout()

def plot_loadings(loadings):
    plt.figure(figsize=(7,3))
    plt.bar(loadings["signal"], loadings["loading"])
    plt.xticks(rotation=45, ha="right")
    plt.title("FactorAnalysis loadings (1-factor)")
    plt.tight_layout()

# -----------------------------
# 12) Sweeps (q, tail, tc) (
# -----------------------------)
def sweep_q(df_eval, spy_ret, models, q_grid, q_tail=0.10, tc_bps=0.0):
    rows = []
    for name, feats in models.items():
        for q in q_grid:
            summ, out, _ = oos_run(df_eval, spy_ret, feats, q=q, q_tail=q_tail, tc_bps=tc_bps)
            summ["model"] = name
            rows.append(summ)
    return pd.DataFrame(rows).sort_values(["CER_g3_gap","MAR_gap"], ascending=False)

def tail_sensitivity(df_eval, spy_ret, features, q=0.80, q_tail_grid=(0.05,0.10,0.15), tc_bps=0.0):
    rows = []
    for qt in q_tail_grid:
        summ, out, _ = oos_run(df_eval, spy_ret, features, q=q, q_tail=qt, tc_bps=tc_bps)
        rows.append(summ)
    return pd.DataFrame(rows).sort_values("CER_g3_gap", ascending=False)

def tc_sensitivity(df_eval, spy_ret, features, q=0.80, q_tail=0.10, tc_grid=(0,5,10)):
    rows = []
    for tc in tc_grid:
        summ, out, _ = oos_run(df_eval, spy_ret, features, q=q, q_tail=q_tail, tc_bps=tc)
        rows.append(summ)
    return pd.DataFrame(rows).sort_values("CER_g3_gap", ascending=False)

# -----------------------------(
# 13) MAIN: run everything and export tables
# -----------------------------)
def run_all(prices, q=0.80, q_tail=0.10, tc_bps=0.0, hyster_bins=10, hyster_min=40):
    # Build signals + Ft
    df_sig, loadings, px = build_signals(prices)

    if "SPY" not in px.columns:
        raise ValueError("Necesito SPY en prices para hacer trading/targets. Agrega SPY a tu universo.")

    px_spy = px["SPY"].copy()
    spy_ret = px_spy.pct_change().dropna()

    df_eval = add_targets(df_sig, px_spy, horizon=22)
    print("df_eval:", df_eval.shape, "|", df_eval.index.min(), "->", df_eval.index.max())

    # Define candidate models (tú puedes editar esto)
    models = {
        "BASE": ["peak_60", "var"],
        "+absorp_z": ["peak_60", "absorp_z"],
        "+ent_z": ["peak_60", "ent_z"],
        "+abs+ent": ["peak_60", "absorp_z", "ent_z"],
        "Ft": ["F_t"],
        "Ft+abs+ent": ["F_t", "absorp_z", "ent_z"],
    }

    # Sweep q for all models
    q_grid = [0.75, 0.80, 0.825, 0.85, 0.90, 0.925]
    table_q = sweep_q(df_eval, spy_ret, models, q_grid=q_grid, q_tail=q_tail, tc_bps=tc_bps)
    table_q.to_csv("table_q_sweep_all.csv", index=False)

    # Pick best by CER_g3_gap then MAR_gap
    best = table_q.iloc[0].to_dict()
    best_model = best["model"]
    best_q = float(best["q"])
    best_feats = models[best_model]

    print("\nBEST:", best_model, "q=", best_q, "features=", best_feats)
    print(pd.Series(best))

    # Run best and plot everything
    summ, out, cut_df = oos_run(df_eval, spy_ret, best_feats, q=best_q, q_tail=q_tail, tc_bps=tc_bps)

    # ---- plots ----
    plot_loadings(loadings)
    plot_equity(out, title=f"Equity (best={best_model}, q={best_q}, tc={tc_bps}bps)")
    plot_drawdown(out, title="Drawdown: Market vs Strategy")
    plot_signal_vs_price(px_spy.loc[out.index], out["p"], title="Risk score p(tail) vs SPY (z)")

    cal_tbl = calibration_plot(out, df_eval, q_tail=q_tail, title="Calibration (deciles)")
    piv, cnt = hysteresis_surface(df_eval, out["p"], n_bins=hyster_bins, min_count=hyster_min,
                                  q_tail=q_tail, title=f"Hysteresis surface: mean(tail_evt) vs (p, dp) | min_count={hyster_min}")

    # ---- tables ----
    table_best = pd.DataFrame([summ])
    table_best.to_csv("table_best_models.csv", index=False)
    cut_df.to_csv("table_fold_cuts.csv", index=False)

    # Tail sensitivity around best
    tail_tbl = tail_sensitivity(df_eval, spy_ret, best_feats, q=best_q,
                                q_tail_grid=(0.05,0.10,0.15), tc_bps=tc_bps)
    tail_tbl.to_csv("table_tail_sensitivity.csv", index=False)

    # Optional tc sensitivity
    tc_tbl = tc_sensitivity(df_eval, spy_ret, best_feats, q=best_q, q_tail=q_tail, tc_grid=(0,5,10))
    tc_tbl.to_csv("table_tc_sensitivity.csv", index=False)

    print("\nSaved: table_q_sweep_all.csv, table_best_models.csv, table_fold_cuts.csv, table_tail_sensitivity.csv, table_tc_sensitivity.csv")
    return {
        "df_sig": df_sig,
        "df_eval": df_eval,
        "loadings": loadings,
        "table_q": table_q,
        "best_summary": summ,
        "out": out,
        "cut_df": cut_df,
        "calibration": cal_tbl,
        "hysteresis_piv": piv,
        "hysteresis_cnt": cnt,
        "tail_tbl": tail_tbl,
        "tc_tbl": tc_tbl
    }

# =============================
# RUN (example)
# =============================
# If you already have prices DataFrame:
# results = run_all(prices, q=0.80, q_tail=0.10, tc_bps=0.0, hyster_bins=10, hyster_min=40)

# If you DON'T have prices yet:
tickers = ["SPY","QQQ","IWM","DIA","EFA","EWJ","EWG","EWU","EWC","EWA","EEM","TLT","HYG","GLD","USO"]
prices = fetch_prices_yf(tickers, start="2005-01-01")
results = run_all(prices, q=0.80, q_tail=0.10, tc_bps=0.0)

tickers = ["SPY","QQQ","IWM","DIA","EFA","EWJ","EWG","EWU","EWC","EWA","EEM","TLT","HYG","GLD","USO"]
prices = fetch_prices_yf(tickers, start="2005-01-01")

df_sig, loadings, px = build_signals(prices, min_assets=10)  # importante por tus 14 assets
print("df_sig:", df_sig.shape, df_sig.index.min(), df_sig.index.max())
print(loadings)

def make_df_eval(df_sig, px, horizon=22, tail_q=0.10):
    spy = px["SPY"].dropna()
    ret1d = spy.pct_change()

    fut = spy.shift(-horizon) / spy - 1.0
    fut.name = "fut_ret_22"

    out = df_sig.join(fut, how="inner").dropna()

    # evento tail definido SOLO por el futuro (esto está bien), pero ojo que debe evaluarse OOS con folds + purge
    thr = out["fut_ret_22"].quantile(tail_q)
    out["tail10"] = (out["fut_ret_22"] <= thr).astype(int)

    return out

df_eval = make_df_eval(df_sig, px, horizon=22, tail_q=0.10)
print("df_eval:", df_eval.shape, df_eval.index.min(), df_eval.index.max())
print(df_eval[["F_t","absorp_z","ent_z","fut_ret_22","tail10"]].describe())
print("event_rate:", df_eval["tail10"].mean())

def make_folds_idx(df, train_n=8*252, test_n=126, purge_n=60, step_n=126, min_train=1000, min_test=80):
    n = len(df)
    folds, k = [], 1
    start_test = train_n + purge_n

    while (start_test + test_n) <= n:
        train_end = start_test - purge_n
        test_end  = start_test + test_n

        tr = df.iloc[:train_end].copy()
        te = df.iloc[start_test:test_end].copy()

        if len(tr) >= min_train and len(te) >= min_test:
            folds.append((tr, te))
            k += 1

        start_test += step_n

    return folds

folds = make_folds_idx(df_eval)
print("Folds:", len(folds))
print("Fold1:", folds[0][0].index[-1], "->", folds[0][1].index[0], "to", folds[0][1].index[-1])

def exposure_from_score(score, cut, higher_is_risk=True):
    if higher_is_risk:
        expo = (score <= cut).astype(float)
    else:
        expo = (score >= cut).astype(float)
    return expo

# prueba rápida en TODO el periodo (solo para diagnosticar)
score = df_eval["F_t"]  # o una combinación
cut = score.quantile(0.85)

expo_A = exposure_from_score(score, cut, higher_is_risk=True)
expo_B = exposure_from_score(score, cut, higher_is_risk=False)

print("A avg_expo:", expo_A.mean(), "cash_days:", (expo_A==0).mean())
print("B avg_expo:", expo_B.mean(), "cash_days:", (expo_B==0).mean())

import numpy as np
import matplotlib.pyplot as plt

def equity_from_expo(df_eval, expo, tc_bps=0.0):
    r = df_eval["SPY_ret1d"].copy() if "SPY_ret1d" in df_eval else px["SPY"].pct_change().reindex(df_eval.index).fillna(0.0)
    expo = expo.reindex(r.index).fillna(1.0)

    # costos por cambio de exposición (simple)
    turn = expo.diff().abs().fillna(0.0)
    cost = (tc_bps/1e4) * turn

    strat = (1 + expo*r - cost).cumprod()
    mkt   = (1 + r).cumprod()
    return strat, mkt, expo

# agrega ret1d para consistencia
df_eval["SPY_ret1d"] = px["SPY"].pct_change().reindex(df_eval.index).fillna(0.0)

strA, mkt, expoA = equity_from_expo(df_eval, expo_A, tc_bps=0.0)
strB, _,   expoB = equity_from_expo(df_eval, expo_B, tc_bps=0.0)

plt.figure()
plt.plot(mkt.index, mkt.values, label="Market")
plt.plot(strA.index, strA.values, label="Strat A")
plt.plot(strB.index, strB.values, label="Strat B")
plt.legend()
plt.title("Equity curves")
plt.show()

print("avg_expo A:", expoA.mean(), "avg_expo B:", expoB.mean())

import matplotlib.pyplot as plt
import numpy as np

score = df_eval["F_t"].copy()
cut = score.quantile(0.85)
expoA = (score <= cut).astype(float)
expoB = (score >= cut).astype(float)

# 1) Score + cut
plt.figure(figsize=(12,4))
plt.plot(score.index, score.values, label="F_t")
plt.axhline(cut, linestyle="--", label="cut q=0.85")
plt.title("F_t and cut"); plt.legend(); plt.show()

# 2) Exposición en el tiempo
plt.figure(figsize=(12,2.2))
plt.plot(expoA.index, expoA.values, label="expo A")
plt.plot(expoB.index, expoB.values, label="expo B")
plt.ylim(-0.05,1.05); plt.title("Exposure (A vs B)"); plt.legend(); plt.show()

# 3) ¿Dónde está el mercado cuando sales?
spy_ret1d = px["SPY"].reindex(df_eval.index).pct_change().fillna(0.0)
plt.figure(figsize=(12,3))
plt.scatter(score.values, spy_ret1d.values, s=5, alpha=0.3)
plt.axvline(cut, linestyle="--")
plt.title("SPY daily return vs F_t"); plt.show()

def hysteresis_exposure(score, q_enter=0.75, q_exit=0.85):
    enter = score.quantile(q_enter)
    exit_ = score.quantile(q_exit)
    expo = np.zeros(len(score), dtype=float)
    state = 1.0  # start invested

    for i, x in enumerate(score.values):
        if state == 1.0 and x > exit_:
            state = 0.0
        elif state == 0.0 and x < enter:
            state = 1.0
        expo[i] = state
    return pd.Series(expo, index=score.index), float(enter), float(exit_)

expoH, enter, exit_ = hysteresis_exposure(df_eval["F_t"], 0.75, 0.85)
print("enter:", enter, "exit:", exit_, "avg_expo:", float(expoH.mean()))

fut22 = px["SPY"].reindex(score.index).pct_change(22).shift(-22)   # retorno futuro 22d
tail_evt = (fut22 < fut22.quantile(0.10)).astype(int)              # evento cola (10%)

score_lag = score.shift(1) # Define score_lag as a 1-period lagged version of score

tmp = pd.DataFrame({"F": score_lag, "fut22": fut22, "tail": tail_evt}).dropna()

# bins por decil de F
tmp["dec"] = pd.qcut(tmp["F"], 10, labels=False)

tab = tmp.groupby("dec").agg(
    n=("tail","size"),
    p_tail=("tail","mean"),
    mean_fut=("fut22","mean"),
    q05=("fut22", lambda x: x.quantile(0.05)),
    q50=("fut22", "median"),
)

print(tab)

import matplotlib.pyplot as plt

plt.figure(figsize=(6,3))
plt.plot(tab.index, tab["p_tail"], marker="o")
plt.title("Tail event rate vs F decile"); plt.show()

plt.figure(figsize=(6,3))
plt.plot(tab.index, tab["mean_fut"], marker="o")
plt.title("Mean 22d return vs F decile"); plt.show()

# =========================
# CARIA / Latent Fragility Pipeline (Python, Colab-ready)
# - Core signals (CF, SYNC, EWS, CURV, peak_60)
# - Structural metrics (Absorption Ratio + Entropy)
# - FactorAnalysis -> F_t (oriented)
# - Walk-forward OOS: calibration + threshold gating + strategy metrics
# - Plots: equity, drawdown, calibration, hysteresis, diagnostics
# =========================

import warnings, math
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from scipy import signal
from scipy.ndimage import gaussian_filter1d

from sklearn.covariance import LedoitWolf
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import FactorAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, log_loss, average_precision_score

# -------------------------
# Utilities
# -------------------------
def ensure_dt_index(df):
    if not isinstance(df.index, pd.DatetimeIndex):
        df = df.copy()
        df.index = pd.to_datetime(df.index)
    df = df.sort_index()
    return df

def prep_prices(prices, min_coverage=0.90):
    px = ensure_dt_index(prices).copy()
    px = px.replace([np.inf, -np.inf], np.nan)
    # drop columns with too many NaNs
    cov = px.notna().mean()
    keep = cov[cov >= min_coverage].index
    px = px[keep]
    # forward fill small gaps (don’t invent long histories)
    px = px.ffill(limit=5)
    px = px.dropna(how="all")
    return px

def pct_returns(px):
    return px.pct_change().replace([np.inf, -np.inf], np.nan)

def log_returns(px):
    return np.log(px).diff().replace([np.inf, -np.inf], np.nan)

def zscore(s):
    s = pd.Series(s).astype(float)
    m = s.mean()
    sd = s.std()
    return (s - m) / (sd + 1e-12)

def max_drawdown(eq):
    eq = pd.Series(eq).dropna()
    peak = eq.cummax()
    dd = (eq / peak) - 1.0
    return float(dd.min()), dd

def cvar(x, alpha=0.05):
    x = np.asarray(x, dtype=float)
    x = x[np.isfinite(x)]
    if len(x) == 0:
        return np.nan
    q = np.quantile(x, alpha)
    tail = x[x <= q]
    if len(tail) == 0:
        return float(q)
    return float(tail.mean())

def cer_meanvar(x, gamma=3.0):
    # certainty equivalent rate (mean-variance approx) for daily returns
    x = np.asarray(x, dtype=float)
    x = x[np.isfinite(x)]
    if len(x) == 0:
        return np.nan
    mu = np.mean(x)
    var = np.var(x)
    # daily CER (approx): mu - 0.5*gamma*var
    cer_d = mu - 0.5 * gamma * var
    # annualize (approx)
    return float(cer_d * 252)

def cagr_from_equity(eq):
    eq = pd.Series(eq).dropna()
    if len(eq) < 2:
        return np.nan
    return float(eq.iloc[-1] ** (252/len(eq)) - 1)

def sharpe(x):
    x = np.asarray(x, dtype=float)
    x = x[np.isfinite(x)]
    if len(x) == 0:
        return np.nan
    mu = np.mean(x) * 252
    sd = np.std(x) * np.sqrt(252)
    return float(mu / (sd + 1e-12))

def exposure_turnover(expo):
    expo = pd.Series(expo).fillna(0.0).astype(float)
    turn = float(np.abs(expo.diff().fillna(0.0)).mean())
    return float(expo.mean()), turn

# -------------------------
# Core signals
# -------------------------
def compute_cf(r, w=20):
    r = r.dropna(how="all")
    cf = []
    idx = r.index
    cols = r.columns
    for i in range(w, len(r)):
        wr = r.iloc[i-w:i]
        wr = wr.dropna(axis=1, thresh=int(0.8*w))
        if wr.shape[1] < 3:
            cf.append(np.nan); continue
        c = wr.corr().values
        n = c.shape[0]
        ac = (c.sum() - n) / (n * (n - 1))
        cf.append(ac * wr.std().mean() * 100.0)
    return pd.Series(cf, index=idx[w:], name="cf")

def extract_phase(series):
    s = pd.Series(series).astype(float).fillna(0.0)
    trend = gaussian_filter1d(s.values, sigma=60)
    detr = s.values - trend
    h = signal.hilbert(detr)
    return np.angle(h)

def kuramoto_order(phases, window=60):
    phases = phases.copy()
    out = []
    idx = phases.index
    for i in range(window, len(phases)):
        ph = phases.iloc[i].values
        z = np.exp(1j * ph).mean()
        out.append(np.abs(z))
    return pd.Series(out, index=idx[window:], name="sync")

def compute_ews(series, window=120):
    s = pd.Series(series).astype(float)
    return pd.DataFrame({
        "acf1": s.rolling(window).apply(lambda x: pd.Series(x).autocorr(1), raw=False),
        "var":  s.rolling(window).var(),
        "skew": s.rolling(window).skew()
    }, index=s.index)

def rolling_avg_corr(r, window=60):
    r = r.dropna(how="all")
    out = []
    idx = r.index
    for i in range(window, len(r)):
        W = r.iloc[i-window:i].dropna(axis=1, thresh=int(0.8*window))
        if W.shape[1] < 3:
            out.append(np.nan); continue
        c = W.corr().values
        n = c.shape[0]
        avg = (c.sum() - n) / (n * (n - 1))
        out.append(avg)
    return pd.Series(out, index=idx[window:], name="avg_corr")

# -------------------------
# Structural metrics (AR + Entropy)
# -------------------------
def cov_to_corr(S):
    d = np.sqrt(np.diag(S))
    d = np.where(d == 0, 1e-10, d)
    C = S / np.outer(d, d)
    C = np.nan_to_num((C + C.T) / 2)
    return C

def eig_metrics(C, k_frac=0.2):
    w = np.sort(np.linalg.eigvalsh(C))[::-1]
    w = np.maximum(w, 1e-10)
    k = max(1, int(np.ceil(k_frac * len(w))))
    ar = float(np.sum(w[:k]) / np.sum(w))
    p = w / np.sum(w)
    ent = float(-np.sum(p * np.log(p + 1e-10)) / np.log(len(w)) if len(w) > 1 else 0.5)
    return ar, ent

def rolling_struct_metrics(ret_log, window=252, step=5, min_assets=10, coverage_in_window=0.90):
    ret_log = ret_log.copy()
    lw = LedoitWolf()
    struct = pd.DataFrame(index=ret_log.index, columns=["absorption_ratio", "entropy"], dtype=float)

    total_steps = max(1, (len(ret_log) - window) // step)
    print(f"Calculating AR + Entropy: window={window}, step={step}, steps={total_steps}")

    for j, t in enumerate(range(window, len(ret_log), step)):
        W = ret_log.iloc[t-window:t]
        W = W.loc[:, W.notna().mean() >= coverage_in_window]
        if W.shape[1] < min_assets:
            continue
        W = W.apply(lambda s: s.fillna(s.mean()))
        X = W.values - np.nanmean(W.values, axis=0)
        try:
            S = lw.fit(X).covariance_
            C = cov_to_corr(S)
        except Exception:
            C = np.corrcoef(X, rowvar=False)
            C = np.nan_to_num((C + C.T) / 2)

        ar, ent = eig_metrics(C, k_frac=0.2)
        struct.iloc[t] = [ar, ent]

        if (j + 1) % 200 == 0:
            print(f"  {j+1}/{total_steps} ({(j+1)/total_steps*100:.0f}%)")

    struct = struct.ffill().bfill()
    struct.index.name = "date"
    return struct

# -------------------------
# Build signals + FactorAnalysis Ft
# -------------------------
def drop_near_duplicates(df, cols, corr_thr=0.995):
    X = df[cols].dropna()
    if X.shape[0] < 50:
        return cols
    C = X.corr().abs()
    keep = []
    for c in cols:
        if not keep:
            keep.append(c); continue
        # if c is too correlated with any already kept, drop it
        if (C.loc[c, keep] > corr_thr).any():
            continue
        keep.append(c)
    return keep

def build_signals(prices,
                  cf_w=20, sync_w=60, ews_w=120, curv_w=60,
                  struct_window=252, struct_step=5,
                  min_coverage=0.90,
                  min_assets_floor=8):

    px = prep_prices(prices, min_coverage=min_coverage)
    ret = pct_returns(px)

    CF = compute_cf(ret, w=cf_w)

    phases = pd.DataFrame({c: extract_phase(ret[c]) for c in ret.columns}, index=ret.index)
    SYNC = kuramoto_order(phases, window=sync_w)

    EWS = compute_ews(CF, window=ews_w)
    CURV = 1.0 - rolling_avg_corr(ret, window=curv_w)  # proxy

    peak_60 = CF.rolling(60).mean().rename("peak_60")

    # Align core
    common = CF.index
    for s in [SYNC, EWS["acf1"], EWS["var"], EWS["skew"], CURV, peak_60]:
        common = common.intersection(s.dropna().index)

    core = pd.DataFrame({
        "cf": CF.loc[common],
        "sync": SYNC.loc[common],
        "acf1": EWS["acf1"].loc[common],
        "var": EWS["var"].loc[common],
        "skew": EWS["skew"].abs().loc[common],
        "curv": CURV.loc[common],
        "peak_60": peak_60.loc[common],
    }).dropna()
    print(f"Core signals aligned: {core.shape}")

    # Structural metrics
    ret_log = log_returns(px)
    good = ret_log.notna().mean() >= 0.90
    ret_log = ret_log.loc[:, good]
    n_assets = ret_log.shape[1]
    if n_assets < 3:
        raise ValueError("Muy pocos activos con cobertura suficiente para estructural.")
    # IMPORTANT: min_assets must be <= n_assets
    min_assets = min(n_assets, max(min_assets_floor, int(0.6 * n_assets)))
    print(f"Using {n_assets} assets with >90% coverage for struct | min_assets={min_assets}")

    struct = rolling_struct_metrics(ret_log, window=struct_window, step=struct_step, min_assets=min_assets)
    struct = struct.reindex(common).ffill().bfill()

    core["absorp_z"] = zscore(struct["absorption_ratio"])
    core["ent_z"]    = zscore(struct["entropy"])

    # FactorAnalysis input
    fa_cols0 = ["cf","sync","acf1","var","skew","curv","peak_60","absorp_z","ent_z"]
    fa_cols  = drop_near_duplicates(core, fa_cols0, corr_thr=0.995)

    X = core[fa_cols].dropna()
    print(f"FA input shape: {X.shape} | fa_cols={fa_cols}")

    if X.shape[0] < 200:
        raise ValueError("Muy pocas filas tras alineación. Revisa ventanas/NaNs.")
    scaler = StandardScaler()
    Xs = scaler.fit_transform(X.values)

    fa = FactorAnalysis(n_components=1, random_state=0)
    Ft = pd.Series(fa.fit_transform(Xs).ravel(), index=X.index, name="F_t")
    Ft = zscore(Ft)

    loadings = pd.DataFrame({"signal": fa_cols, "loading": fa.components_.ravel()}) \
                .sort_values("loading").reset_index(drop=True)

    df_sig = core.join(Ft, how="inner")
    return df_sig, loadings, px

# -------------------------
# Targets (SPY): future 22d return + tail event
# -------------------------
def build_eval(df_sig, px, market_col="SPY", horizon=22):
    px = px.copy()
    if market_col not in px.columns:
        # if not in universe, take first column
        market_col = px.columns[0]

    mret = px[market_col].pct_change()
    fut = (1.0 + mret).rolling(horizon).apply(np.prod, raw=True).shift(-horizon) - 1.0
    fut.name = f"fut_ret_{horizon}"
    rv_ann = mret.rolling(22).std() * np.sqrt(252)
    rv_ann.name = "rv_ann"

    df = df_sig.join(rv_ann, how="inner").join(fut, how="inner")
    df = df.dropna()
    return df, market_col

# -------------------------
# Walk-forward folds (by rows)
# -------------------------
def make_folds_idx(df, train_n=8*252, test_n=126, purge_n=60, step_n=126, min_train=1000, min_test=80):
    n = len(df)
    folds = []
    start_test = train_n + purge_n
    while (start_test + test_n) <= n:
        train_end = start_test - purge_n
        test_end = start_test + test_n

        train_idx = np.arange(0, train_end)
        test_idx  = np.arange(start_test, test_end)

        if len(train_idx) >= min_train and len(test_idx) >= min_test:
            folds.append((train_idx, test_idx))
        start_test += step_n
    return folds

# -------------------------
# Exposure rule (FIXED)
# -------------------------
def exposure_from_score(score, cut, higher_is_risk=True):
    score = pd.Series(score)
    if higher_is_risk:
        return (score <= cut).astype(float)   # invest when NOT in top-risk tail
    else:
        return (score >= cut).astype(float)   # invest when high is good

# -------------------------
# OOS run: fit on train, predict test, choose cut from train-preds
# -------------------------
def oos_run_calibrated(df_eval, features, q=0.80, q_tail=0.10, tc_bps=0.0, horizon=22, market_col="SPY"):
    df = df_eval.copy()

    # daily returns for equity curve
    mret = df["mret"].copy()

    folds = make_folds_idx(df, train_n=8*252, test_n=126, purge_n=60, step_n=126)
    print("Folds:", len(folds))
    if len(folds) > 0:
        tr, te = folds[0]
        print("Fold1:", df.index[tr[-1]], "->", df.index[te[0]], "to", df.index[te[-1]])

    p_all = pd.Series(index=df.index, dtype=float)
    expo_all = pd.Series(index=df.index, dtype=float)

    fold_rows = []

    for k, (tr, te) in enumerate(folds, 1):
        train = df.iloc[tr].copy()
        test  = df.iloc[te].copy()

        # define tail threshold ONLY on train (avoid leakage)
        thr = train[f"fut_ret_{horizon}"].quantile(q_tail)
        y_tr = (train[f"fut_ret_{horizon}"] <= thr).astype(int)
        y_te = (test[f"fut_ret_{horizon}"] <= thr).astype(int)

        X_tr = train[features].replace([np.inf, -np.inf], np.nan).dropna()
        y_tr = y_tr.loc[X_tr.index]

        X_te = test[features].replace([np.inf, -np.inf], np.nan).dropna()
        y_te = y_te.loc[X_te.index]

        if len(X_tr) < 200 or len(X_te) < 40:
            continue

        scaler = StandardScaler()
        Xtr_s = scaler.fit_transform(X_tr.values)
        Xte_s = scaler.transform(X_te.values)

        clf = LogisticRegression(max_iter=2000, class_weight="balanced", solver="lbfgs")
        clf.fit(Xtr_s, y_tr.values)

        p_tr = pd.Series(clf.predict_proba(Xtr_s)[:, 1], index=X_tr.index)
        p_te = pd.Series(clf.predict_proba(Xte_s)[:, 1], index=X_te.index)

        # risk budget cut from TRAIN predicted probabilities
        cut = float(p_tr.quantile(q))
        expo_te = exposure_from_score(p_te, cut, higher_is_risk=True)

        p_all.loc[p_te.index] = p_te
        expo_all.loc[p_te.index] = expo_te

        # fold diagnostics
        try:
            auc = roc_auc_score(y_te.values, p_te.values) if y_te.nunique() > 1 else np.nan
            pr  = average_precision_score(y_te.values, p_te.values) if y_te.sum() > 0 else np.nan
            ll  = log_loss(y_te.values, np.clip(p_te.values, 1e-6, 1-1e-6)) if y_te.nunique() > 1 else np.nan
        except Exception:
            auc, pr, ll = np.nan, np.nan, np.nan

        fold_rows.append({
            "fold": k,
            "start": test.index.min(),
            "end": test.index.max(),
            "event_rate": float(y_te.mean()),
            "auc": auc,
            "prauc": pr,
            "logloss": ll,
            "avg_expo": float(expo_te.mean()),
            "cut": cut
        })

    folds_df = pd.DataFrame(fold_rows)

    # build equity curves (OOS only where we have expo)
    expo = expo_all.fillna(method="ffill").fillna(1.0)
    # shift exposure by 1 day to avoid same-day lookahead
    expo_shift = expo.shift(1).fillna(1.0)

    # transaction cost (bps) applied on switches
    tc = tc_bps / 10000.0
    turn = expo_shift.diff().abs().fillna(0.0)
    strat_ret = expo_shift * mret - tc * turn

    eq_m = (1.0 + mret.fillna(0.0)).cumprod()
    eq_s = (1.0 + strat_ret.fillna(0.0)).cumprod()

    return eq_s, eq_m, expo_shift, p_all, folds_df, strat_ret, mret

# -------------------------
# Metrics table (economic value)
# -------------------------
def summarize_strategy(eq_s, eq_m, strat_ret, mret, expo):
    cagr_s = cagr_from_equity(eq_s)
    cagr_m = cagr_from_equity(eq_m)

    mdd_s, dd_s = max_drawdown(eq_s)
    mdd_m, dd_m = max_drawdown(eq_m)

    vol_s = float(np.std(strat_ret.dropna()) * np.sqrt(252))
    vol_m = float(np.std(mret.dropna()) * np.sqrt(252))

    sh_s = sharpe(strat_ret.dropna())
    sh_m = sharpe(mret.dropna())

    cvar_s = cvar(strat_ret.dropna(), 0.05)
    cvar_m = cvar(mret.dropna(), 0.05)

    cer3_s = cer_meanvar(strat_ret.dropna(), gamma=3.0)
    cer3_m = cer_meanvar(mret.dropna(), gamma=3.0)

    avg_expo, turn = exposure_turnover(expo)

    out = pd.Series({
        "CAGR_strat": cagr_s,
        "MaxDD_strat": mdd_s,
        "MAR_strat": (cagr_s / abs(mdd_s)) if (mdd_s < 0) else np.nan,
        "Vol_strat": vol_s,
        "Sharpe_strat": sh_s,
        "CVaR5_strat": cvar_s,
        "CER_g3_strat": cer3_s,

        "CAGR_mkt": cagr_m,
        "MaxDD_mkt": mdd_m,
        "MAR_mkt": (cagr_m / abs(mdd_m)) if (mdd_m < 0) else np.nan,
        "Vol_mkt": vol_m,
        "Sharpe_mkt": sh_m,
        "CVaR5_mkt": cvar_m,
        "CER_g3_mkt": cer3_m,

        # "economic value" style deltas
        "capital_saved_at_worstDD": (mdd_m - mdd_s),   # positive = saved
        "CER_g3_gap": (cer3_s - cer3_m),
        "avg_expo": avg_expo,
        "turnover": turn,
        "n_days": int(len(eq_s.dropna()))
    })
    return out, dd_s, dd_m

# -------------------------
# Plots
# -------------------------
def plot_equity(eq_m, eq_s, title="Equity"):
    plt.figure(figsize=(12,4))
    plt.plot(eq_m.index, eq_m.values, label="Market")
    plt.plot(eq_s.index, eq_s.values, label="Strategy")
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

def plot_drawdowns(dd_m, dd_s):
    plt.figure(figsize=(12,3))
    plt.plot(dd_m.index, dd_m.values)
    plt.title("Drawdown: Market")
    plt.grid(True, alpha=0.3)
    plt.show()

    plt.figure(figsize=(12,3))
    plt.plot(dd_s.index, dd_s.values)
    plt.title("Drawdown: Strategy")
    plt.grid(True, alpha=0.3)
    plt.show()

def plot_calibration(p, y, n_bins=10):
    df = pd.DataFrame({"p": p, "y": y}).dropna()
    if df.empty:
        print("No calibration data.")
        return
    df["bin"] = pd.qcut(df["p"], n_bins, duplicates="drop")
    g = df.groupby("bin").agg(p_mean=("p","mean"), event_rate=("y","mean"), n=("y","size"))
    plt.figure(figsize=(5,4))
    plt.plot(g["p_mean"], g["event_rate"], marker="o")
    plt.plot([0,1],[0,1], linestyle="--")
    plt.title("Calibration (deciles)")
    plt.xlabel("mean predicted p")
    plt.ylabel("event rate")
    plt.grid(True, alpha=0.3)
    plt.show()
    return g

def hysteresis_panel(df, score_col="F_t", tail_col="tail10", n_bins=10, min_count=40):
    d = df[[score_col, tail_col]].dropna().copy()
    d["dF"] = d[score_col].diff()
    d = d.dropna()

    d["F_bin"]  = pd.qcut(d[score_col], n_bins, labels=False, duplicates="drop")
    d["dF_bin"] = pd.qcut(d["dF"], n_bins, labels=False, duplicates="drop")

    tab = d.groupby(["dF_bin","F_bin"])[tail_col].agg(["mean","count"]).reset_index()
    grid = tab.pivot(index="dF_bin", columns="F_bin", values="mean")
    cnt  = tab.pivot(index="dF_bin", columns="F_bin", values="count")

    # mask sparse cells
    grid = grid.where(cnt >= min_count)

    plt.figure(figsize=(7,5))
    im = plt.imshow(grid.values, aspect="auto", origin="lower")
    plt.colorbar(im, label=f"mean {tail_col}")
    plt.title(f"Hysteresis surface: mean({tail_col}) vs (F, dF) | min_count={min_count}")
    plt.xlabel("F-bin (low → high)")
    plt.ylabel("dF-bin (low → high)")

    # annotate counts
    for i in range(grid.shape[0]):
        for j in range(grid.shape[1]):
            if np.isfinite(grid.values[i,j]):
                plt.text(j, i, int(cnt.values[i,j]), ha="center", va="center", fontsize=8)

    plt.show()
    return grid, cnt

# -------------------------
# RUN ALL (clean, staged)
# -------------------------
def run_all(prices, q=0.80, q_tail=0.10, tc_bps=0.0, horizon=22, market_col="SPY"):
    # 1) Signals + Ft
    df_sig, loadings, px = build_signals(prices)

    # 2) Eval frame
    df_eval, mkt = build_eval(df_sig, px, market_col=market_col, horizon=horizon)
    # attach daily market returns (aligned)
    df_eval["mret"] = px[mkt].pct_change().reindex(df_eval.index)
    df_eval = df_eval.dropna(subset=["mret", f"fut_ret_{horizon}"])

    # 3) Orient Ft so HIGH = more tail risk (otherwise flip)
    thr_global = df_eval[f"fut_ret_{horizon}"].quantile(q_tail)
    df_eval["tail10"] = (df_eval[f"fut_ret_{horizon}"] <= thr_global).astype(int)
    if df_eval["F_t"].corr(df_eval["tail10"]) < 0:
        df_eval["F_t"] *= -1
        loadings["loading"] *= -1

    print("\n=== Factor loadings (sorted) ===")
    print(loadings)

    print(f"\ndf_eval: {df_eval.shape} | {df_eval.index.min()} -> {df_eval.index.max()} | mkt={mkt}")

    # 4) Models (you can extend)
    models = {
        "BASE": ["peak_60", "var"],
        "+absorp_z": ["peak_60", "absorp_z"],
        "+ent_z": ["peak_60", "ent_z"],
        "+abs+ent": ["peak_60", "absorp_z", "ent_z"],
        "Ft": ["F_t"],
        "Ft+abs+ent": ["F_t", "absorp_z", "ent_z"],
    }
    print("\nModels:", models)

    rows = []
    fold_tables = {}

    for name, feats in models.items():
        eq_s, eq_m, expo, p, folds_df, strat_ret, mret = oos_run_calibrated(
            df_eval, feats, q=q, q_tail=q_tail, tc_bps=tc_bps, horizon=horizon, market_col=mkt
        )
        summ, dd_s, dd_m = summarize_strategy(eq_s, eq_m, strat_ret, mret, expo)
        summ["model"] = name
        summ["q"] = q
        summ["q_tail"] = q_tail
        summ["tc_bps"] = tc_bps
        rows.append(summ)

        fold_tables[name] = folds_df

    table = pd.DataFrame(rows).sort_values(["CER_g3_gap","MAR_strat"], ascending=False)
    print("\n=== Summary (sorted by CER_g3_gap then MAR_strat) ===")
    print(table[["model","q","CAGR_strat","MaxDD_strat","MAR_strat","avg_expo","CER_g3_strat","CER_g3_gap"]].head(15))

    best = table.iloc[0].copy()
    best_model = best["model"]
    best_feats = models[best_model]
    print("\nBEST:", best_model, "| features=", best_feats)
    print(best)

    # Re-run best for plots (OOS)
    eq_s, eq_m, expo, p, folds_df, strat_ret, mret = oos_run_calibrated(
        df_eval, best_feats, q=q, q_tail=q_tail, tc_bps=tc_bps, horizon=horizon, market_col=mkt
    )
    summ, dd_s, dd_m = summarize_strategy(eq_s, eq_m, strat_ret, mret, expo)

    # Save tables
    table.to_csv("table_models_summary.csv", index=False)
    folds_df.to_csv("table_best_folds.csv", index=False)

    # Plots
    plot_equity(eq_m, eq_s, title=f"Equity (best={best_model}, q={q}, tc={tc_bps}bps)")
    plot_drawdowns(dd_m, dd_s)

    # Calibration on best p (OOS)
    # define tail labels globally for visualization (not used for training cut)
    y = df_eval["tail10"]
    calib = plot_calibration(p, y, n_bins=10)
    if calib is not None:
        calib.to_csv("table_calibration_deciles.csv")

    # Hysteresis (on Ft, global tail10)
    _ = hysteresis_panel(df_eval, score_col="F_t", tail_col="tail10", n_bins=10, min_count=40)

    return {
        "df_sig": df_sig, "df_eval": df_eval, "loadings": loadings,
        "table_models": table, "best_summary": summ,
        "best_model": best_model, "best_features": best_feats,
        "best_folds": folds_df
    }

# -------------------------
# RUN (this is what you need to execute to "see output")
# -------------------------
def fetch_prices_yf(tickers, start="2005-01-01"):
    import yfinance as yf
    data = yf.download(tickers, start=start, progress=False, auto_adjust=True)
    if isinstance(data.columns, pd.MultiIndex):
        px = data["Close"].copy()
    else:
        px = data.copy()
    px = ensure_dt_index(px)
    return px

# ======= CHANGE YOUR UNIVERSE HERE =======
tickers = ["SPY","QQQ","IWM","DIA","EFA","EWJ","EWG","EWU","EWC","EWA","EEM","TLT","HYG","GLD","USO"]
prices = fetch_prices_yf(tickers, start="2005-01-01")
print("Prices:", prices.shape, "|", prices.index.min(), "->", prices.index.max())

out = run_all(prices, q=0.80, q_tail=0.10, tc_bps=0.0, horizon=22, market_col="SPY")
print("\nDONE. Saved: table_models_summary.csv, table_best_folds.csv, table_calibration_deciles.csv")

from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import TimeSeriesSplit

def fit_model_calibrated(tr, features, q_tail=0.10):
    thr = tr[ycol].quantile(q_tail)
    ytr = (tr[ycol] <= thr).astype(int).values
    Xtr = tr[features].values

    base = Pipeline([
        ("scaler", StandardScaler()),
        ("lr", LogisticRegression(max_iter=1200, class_weight="balanced"))
    ])

    # calibrate only on TRAIN using time-series splits
    tscv = TimeSeriesSplit(n_splits=3)
    cal = CalibratedClassifierCV(base, method="isotonic", cv=tscv)
    cal.fit(Xtr, ytr)
    return cal

def oos_run_calibrated(features, q=0.80, q_tail=0.10, train_n=8*252, test_n=252, purge_n=H):
    dff = df_eval.copy()
    dff["dF"]   = dff["F_t"].diff()
    dff["FxdF"] = dff["F_t"] * dff["dF"]
    dff = dff.dropna(subset=features + ["dF","FxdF", ycol, "rv_ann", "F_t"])

    daily_mkt = prices[mkt].pct_change().reindex(dff.index).fillna(0.0)
    folds = make_folds_nonoverlap(len(dff), train_n=train_n, test_n=test_n, purge_n=purge_n)

    expo = pd.Series(np.nan, index=dff.index)
    p_hat = pd.Series(np.nan, index=dff.index)

    for (a,b,c,d) in folds:
        tr = dff.iloc[a:b].copy()
        te = dff.iloc[c:d].copy()

        clf = fit_model_calibrated(tr, features, q_tail=q_tail)
        p_tr = clf.predict_proba(tr[features].values)[:,1]
        cut  = np.quantile(p_tr, q)

        p_te = clf.predict_proba(te[features].values)[:,1]
        expo.iloc[c:d] = (p_te <= cut).astype(float)
        p_hat.iloc[c:d] = p_te

    mask = expo.notna()
    expo_oos = expo[mask].astype(float)
    p_oos    = p_hat[mask].astype(float)
    ret_mkt  = daily_mkt.loc[expo_oos.index]
    ret_strat = ret_mkt * expo_oos

    perf_strat, eq_strat = eval_path(ret_strat.values)
    perf_mkt,   eq_mkt   = eval_path(ret_mkt.values)

    return eq_strat, eq_mkt, expo_oos, p_oos

# Run calibrated version for the hysteresis plot
eqS_c, eqM_c, expo_c, p_c = oos_run_calibrated(ABS, q=0.80, q_tail=0.10)
hysteresis_panel(df_eval, p_c, n_bins=8, min_count=30)

def run_table(models, qs, q_tail=0.10, tc_bps=0.0):
    rows=[]
    for name, feats in models.items():
        for q in qs:
            s, _, _, _, _, _ = oos_run(feats, q=q, q_tail=q_tail, tc_bps=tc_bps)
            rows.append({"model": name, **s})
    return pd.DataFrame(rows)

models = {
    "BASE": BASE,
    "+absorp_z": ABS,
    "+ent_z": ENT,
    "+abs+ent": BOTH
}
qs = [0.75, 0.775, 0.80, 0.825, 0.85, 0.875, 0.90, 0.925, 0.95]

res = run_table(models, qs, q_tail=0.10, tc_bps=0.0)

# 1) Best per model (by MAR)
best = res.sort_values("MAR_strat", ascending=False).groupby("model").head(1)
print(best[["model","q","CAGR_strat","MaxDD_strat","MAR_strat","avg_expo","CER_g3_strat","CER_g3_mkt","CER_g3_gap"]])

# 2) Sensitivity tail definition (your robustness)
tails = []
for qt in [0.05, 0.10, 0.15]:
    s, *_ = oos_run(ABS, q=0.80, q_tail=qt, tc_bps=0.0)
    tails.append(s)
tail_df = pd.DataFrame(tails)[["q_tail","CAGR_strat","MaxDD_strat","MAR_strat","CER_g3_strat","CER_g3_gap","avg_expo"]]
print("\nTail sensitivity:")
print(tail_df)

# 3) Economic value: capital saved at max drawdown (for $1 notional)
# "saved" = DD_mkt - DD_strat  (positive means strat loses less at worst point)
ev = winner_summary.copy()
capital_saved = abs(ev["MaxDD_mkt"]) - abs(ev["MaxDD_strat"])
print("\nCapital saved at worst drawdown (per $1):", float(capital_saved))

# 4) Optional: include transaction costs stress-test
res_tc = run_table({"+absorp_z": ABS}, qs, q_tail=0.10, tc_bps=5.0)  # 5 bps per switch
print("\nWith 5 bps switching cost (sample):")
print(res_tc.sort_values("MAR_strat", ascending=False).head(5)[["model","q","CAGR_strat","MaxDD_strat","MAR_strat","avg_expo"]])

# 5) Export tables (CSV + LaTeX)
best.to_csv("table_best_models.csv", index=False)
tail_df.to_csv("table_tail_sensitivity.csv", index=False)
res.to_csv("table_q_sweep_all.csv", index=False)

print("\nSaved: table_best_models.csv, table_tail_sensitivity.csv, table_q_sweep_all.csv")

import numpy as np

def perm_test_diff(df, dec, metric="tail10", n_perm=5000, seed=0):
    rng = np.random.default_rng(seed)
    sub = df[df["F_decile"]==dec].dropna(subset=[metric,"path"]).copy()
    a = sub[sub["path"]=="falling"][metric].values
    b = sub[sub["path"]=="rising"][metric].values
    if len(a)<30 or len(b)<30:
        return np.nan
    obs = a.mean() - b.mean()
    pooled = np.r_[a,b]
    na = len(a)
    cnt = 0
    for _ in range(n_perm):
        rng.shuffle(pooled)
        diff = pooled[:na].mean() - pooled[na:].mean()
        cnt += (abs(diff) >= abs(obs))
    return obs, (cnt+1)/(n_perm+1)

# arma dfh como antes (con tail10, F_decile, path)
# dfh = ...

rows=[]
for dec in range(1,11):
    out = perm_test_diff(dfh, dec, metric="tail10", n_perm=3000, seed=dec)
    if isinstance(out, tuple):
        obs, p = out
        rows.append({"decile": dec, "Δp_tail10": obs, "p_value": p})
pd.DataFrame(rows)

import pandas as pd

dfp = dfh.copy()
dfp["dF"] = dfp["F_t"].diff()
dfp = dfp.dropna(subset=["dF"])

dfp["F_bin"]  = pd.qcut(dfp["F_t"], 10, labels=False) + 1
dfp["dF_bin"] = pd.qcut(dfp["dF"],  10, labels=False) + 1

heat = dfp.pivot_table(index="dF_bin", columns="F_bin", values="tail10", aggfunc="mean")
heat

master = pd.concat([signals_df, Ft], axis=1)
if "xhat" in locals():
    master = pd.concat([master, xhat.rename("xhat_cusp"), a_t, b_t], axis=1)

master = master.dropna()
master.to_csv("caria_latent_fragility_master.csv", index=True)

print("Saved caria_latent_fragility_master.csv", master.shape)
master.head()

# --- AUTO-DETECT index columns ---
idx_cols = [c for c in df.columns if c.endswith("_index")]

if len(idx_cols) == 0:
    raise ValueError("No encontré columnas que terminen en '_index'. Revisa df.columns[:50]")

prices_idx = df[idx_cols].copy()

# Asegura DateTimeIndex
if not isinstance(prices_idx.index, pd.DatetimeIndex):
    prices_idx.index = pd.to_datetime(prices_idx.index)

# Returns
ret = prices_idx.pct_change().dropna(how="all")
ret.columns = [c.replace("_index", "") for c in ret.columns]

print("Using indices:", ret.shape[1], "from", ret.index.min(), "to", ret.index.max())

# === BASE SIGNALS ===
idx_cols = [f'{c}_index' for c in COUNTRIES if f'{c}_index' in df.columns]
prices_idx = df[idx_cols].copy()

if not isinstance(prices_idx.index, pd.DatetimeIndex):
    prices_idx.index = pd.to_datetime(prices_idx.index)

ret = prices_idx.pct_change().dropna(how="all")
ret.columns = [c.replace('_index', '') for c in ret.columns]

def rolling_avg_corr(r, window=60):
    out = []
    idx = r.index
    for i in range(window, len(r)):
        c = r.iloc[i-window:i].corr().values
        n = c.shape[0]
        avg = (c.sum() - n) / (n * (n - 1))
        out.append(avg)
    return pd.Series(out, index=idx[window:])

# 1) Crisis Factor (CF) = (avg corr) * (avg std) * 100
def compute_cf(r, w=20):
    avg_corr = rolling_avg_corr(r, window=w)
    avg_std  = r.rolling(w).std().mean(axis=1).loc[avg_corr.index]
    return (avg_corr * avg_std * 100).rename("cf")

CF = compute_cf(ret, w=20)

# 4) "Curvature proxy": decide convención
# Si quieres "más correlación = más fragilidad", usa directamente avg_corr:
CURV = rolling_avg_corr(ret, 60).rename("curv")

# 2) Synchronization (Kuramoto)
def extract_phase_safe(series):
    s = series.dropna()
    baseline = pd.Series(gaussian_filter1d(s.values, sigma=60), index=s.index)
    detr = s - baseline
    analytic = signal.hilbert(detr.values)
    return pd.Series(np.angle(analytic), index=s.index)

phases = {}
for c in ret.columns:
    phases[c] = extract_phase_safe(ret[c])

common_idx = None
for s in phases.values():
    common_idx = s.index if common_idx is None else common_idx.intersection(s.index)

phases_df = pd.DataFrame({k: v.loc[common_idx] for k, v in phases.items()}).dropna()

def kuramoto_order(phases_df, window=60):
    r = []
    idx = phases_df.index
    for i in range(window, len(phases_df)):
        z = np.exp(1j * phases_df.iloc[i].values).mean()
        r.append(np.abs(z))
    return pd.Series(r, index=idx[window:], name="sync")

SYNC = kuramoto_order(phases_df, window=60)

# 3) EWS (ACF, Var, Skew) sobre CF
def compute_ews(series, window=120):
    return pd.DataFrame({
        "acf1": series.rolling(window).apply(lambda x: pd.Series(x).autocorr(1), raw=False),
        "var":  series.rolling(window).var(),
        "skew": series.rolling(window).skew(),
    }, index=series.index)

EWS = compute_ews(CF, window=120)

print("Signals ready:",
      "CF", CF.dropna().shape,
      "SYNC", SYNC.dropna().shape,
      "EWS", EWS.dropna().shape,
      "CURV", CURV.dropna().shape)