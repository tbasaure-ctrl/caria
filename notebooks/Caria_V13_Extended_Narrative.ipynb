{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CARIA V13: MANUAL DATA LOADING + GGSI\n",
                "\n",
                "**Changes:**\n",
                "1.  **Manual Data Loading**: Reads CSV files from Google Drive instead of using APIs (to avoid limits/errors).\n",
                "2.  **Extended Universe**: 27 Countries (Hubs, Finance, Mfg, Energy).\n",
                "3.  **Synthetic Global Stress Index (GGSI)**: Computed internally from the loaded data.\n",
                "\n",
                "**Setup:**\n",
                "1.  Mount Google Drive.\n",
                "2.  Ensure data is in `/content/drive/MyDrive/Caria_Data/`.\n",
                "    -   `market/`: Contains `USA.csv`, `CHN.csv`, `Gold.csv`, etc.\n",
                "    -   `macro/`: Contains `USA_gdp_growth.csv`, etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch matplotlib seaborn scikit-learn networkx\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import networkx as nx\n",
                "import warnings\n",
                "import os\n",
                "from google.colab import drive\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"üîß Device: {DEVICE}\")\n",
                "\n",
                "# Mount Drive\n",
                "drive.mount('/content/drive')\n",
                "DATA_PATH = '/content/drive/MyDrive/Caria_Data'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. MANUAL DATA FETCHER"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ManualDataFetcher:\n",
                "    def __init__(self, data_path):\n",
                "        self.data_path = data_path\n",
                "        self.countries = [\n",
                "            'USA', 'CHN', 'JPN', 'DEU', 'GBR', 'FRA', 'IND', 'BRA',\n",
                "            'CAN', 'KOR', 'AUS', 'MEX', 'IDN', 'ZAF', 'CHL',\n",
                "            'SGP', 'NLD', 'HKG', 'CHE', 'ARE', 'TWN', 'VNM',\n",
                "            'MYS', 'THA', 'POL', 'NOR', 'QAT'\n",
                "        ]\n",
                "\n",
                "    def fetch_all(self):\n",
                "        print(f\"\\nüìÇ Loading Data from {self.data_path}...\")\n",
                "        market_data = {}\n",
                "        \n",
                "        # 1. Load Market Data (Indices + Commodities)\n",
                "        try:\n",
                "            print(\"  Loading Market Indices...\")\n",
                "            for iso in self.countries:\n",
                "                path = f'{self.data_path}/market/{iso}.csv'\n",
                "                if os.path.exists(path):\n",
                "                    try:\n",
                "                        df = pd.read_csv(path, parse_dates=['Date'])\n",
                "                        df.set_index('Date', inplace=True)\n",
                "                        # Handle different column names\n",
                "                        col = df['Close'] if 'Close' in df.columns else df.iloc[:, 0]\n",
                "                        market_data[f'{iso}_index'] = col\n",
                "                        print(f\"    ‚úì {iso} ({len(col)} pts)\")\n",
                "                    except Exception as e:\n",
                "                        print(f\"    ‚ö†Ô∏è Error reading {iso}: {e}\")\n",
                "                else:\n",
                "                    print(f\"    ‚úó {iso} (file not found)\")\n",
                "            \n",
                "            # Load Commodities/Global\n",
                "            for glob in ['Gold', 'Oil', 'Copper', 'VIX', 'DXY']:\n",
                "                path = f'{self.data_path}/market/{glob}.csv'\n",
                "                if os.path.exists(path):\n",
                "                    try:\n",
                "                        df = pd.read_csv(path, parse_dates=['Date'])\n",
                "                        df.set_index('Date', inplace=True)\n",
                "                        col = df['Close'] if 'Close' in df.columns else df.iloc[:, 0]\n",
                "                        market_data[glob] = col\n",
                "                        print(f\"    ‚úì {glob}\")\n",
                "                    except:\n",
                "                        pass\n",
                "                else:\n",
                "                    print(f\"    ‚úó {glob} (missing)\")\n",
                "                    \n",
                "        except Exception as e:\n",
                "            print(f\"  ‚ö†Ô∏è Error loading market data: {e}\")\n",
                "\n",
                "        return market_data\n",
                "\n",
                "    def get_macro_data(self):\n",
                "        print(\"\\n  Loading Macro Data...\")\n",
                "        te_data = {}\n",
                "        indicators = ['gdp_growth', 'inflation', 'interest_rate', 'unemployment']\n",
                "        \n",
                "        for iso in self.countries:\n",
                "            iso_data = {}\n",
                "            for ind in indicators:\n",
                "                path = f'{self.data_path}/macro/{iso}_{ind}.csv'\n",
                "                if os.path.exists(path):\n",
                "                    try:\n",
                "                        df = pd.read_csv(path, parse_dates=['Date'])\n",
                "                        df.set_index('Date', inplace=True)\n",
                "                        col = df['Value'] if 'Value' in df.columns else df.iloc[:, 0]\n",
                "                        iso_data[ind] = col\n",
                "                    except:\n",
                "                        pass\n",
                "            if iso_data:\n",
                "                te_data[iso] = iso_data\n",
                "                print(f\"    ‚úì {iso}: {len(iso_data)} indicators\")\n",
                "            \n",
                "        return te_data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. DATA ASSEMBLER & GGSI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GlobalDataAssembler:\n",
                "    def __init__(self, data_path):\n",
                "        self.fetcher = ManualDataFetcher(data_path)\n",
                "        self.countries = self.fetcher.countries\n",
                "\n",
                "    def _to_zscore(self, series, window=60):\n",
                "        if series is None or len(series) < window:\n",
                "            return None\n",
                "        roll_mean = series.rolling(window, min_periods=5).mean()\n",
                "        roll_std = series.rolling(window, min_periods=5).std()\n",
                "        z = (series - roll_mean) / (roll_std + 1e-8)\n",
                "        return z.replace([np.inf, -np.inf], 0).fillna(0).clip(-5, 5)\n",
                "\n",
                "    def _interpolate(self, series, target_index):\n",
                "        if series is None:\n",
                "            return pd.Series(0.0, index=target_index)\n",
                "        # Handle duplicate indices if any\n",
                "        series = series[~series.index.duplicated(keep='first')]\n",
                "        return series.reindex(target_index).ffill().bfill().fillna(0)\n",
                "\n",
                "    def _compute_ggsi(self, market_data, all_dates):\n",
                "        print(\"\\nüåç Constructing GGSI (Global Geopolitical Stress Index)...\")\n",
                "        \n",
                "        # 1. Global Volatility\n",
                "        majors = ['USA', 'CHN', 'DEU', 'JPN', 'GBR']\n",
                "        vol_series = []\n",
                "        returns_df = pd.DataFrame(index=all_dates)\n",
                "\n",
                "        for iso in majors:\n",
                "            key = f'{iso}_index'\n",
                "            if key in market_data:\n",
                "                s = self._interpolate(market_data[key], all_dates)\n",
                "                ret = np.log(s / s.shift(1)).fillna(0)\n",
                "                vol = ret.rolling(20).std() * np.sqrt(252)\n",
                "                vol_series.append(self._to_zscore(vol, 252))\n",
                "                returns_df[iso] = ret\n",
                "        \n",
                "        if not vol_series:\n",
                "             return pd.Series(0, index=all_dates)\n",
                "\n",
                "        global_vol_z = pd.concat(vol_series, axis=1).mean(axis=1).fillna(0)\n",
                "\n",
                "        # 2. Global Correlation\n",
                "        if len(returns_df.columns) > 1:\n",
                "            corr_series = returns_df.rolling(60).corr().groupby(level=0).mean().mean(axis=1)\n",
                "            global_corr_z = self._to_zscore(corr_series, 252)\n",
                "        else:\n",
                "            global_corr_z = pd.Series(0, index=all_dates)\n",
                "\n",
                "        # 3. Safe Haven\n",
                "        gold = self._interpolate(market_data.get('Gold'), all_dates)\n",
                "        dxy = self._interpolate(market_data.get('DXY'), all_dates)\n",
                "        safe_haven_z = (self._to_zscore(gold, 60) + self._to_zscore(dxy, 60)) / 2\n",
                "\n",
                "        # 4. Oil Shock\n",
                "        oil = self._interpolate(market_data.get('Oil'), all_dates)\n",
                "        oil_ret = np.log(oil / oil.shift(1)).fillna(0)\n",
                "        oil_vol_z = self._to_zscore(oil_ret.rolling(20).std(), 252)\n",
                "\n",
                "        ggsi = (global_vol_z + global_corr_z + safe_haven_z + oil_vol_z) / 4\n",
                "        return ggsi.fillna(0)\n",
                "\n",
                "    def fetch(self):\n",
                "        market_data = self.fetcher.fetch_all()\n",
                "        te_data = self.fetcher.get_macro_data()\n",
                "\n",
                "        # Master Index\n",
                "        all_dates = pd.DatetimeIndex([])\n",
                "        for s in market_data.values():\n",
                "            if s is not None:\n",
                "                all_dates = all_dates.union(s.index)\n",
                "        \n",
                "        if len(all_dates) == 0:\n",
                "            print(\"‚ùå No data found! Please check your CSV files.\")\n",
                "            return None, None, None, [], [], []\n",
                "\n",
                "        all_dates = all_dates.sort_values()\n",
                "        \n",
                "        # Compute GGSI\n",
                "        ggsi = self._compute_ggsi(market_data, all_dates)\n",
                "\n",
                "        # Build Node Features\n",
                "        node_features = {}\n",
                "        has_market = {}\n",
                "\n",
                "        for iso in self.countries:\n",
                "            feat = pd.DataFrame(index=all_dates)\n",
                "            te_iso = te_data.get(iso, {})\n",
                "            has_market[iso] = False\n",
                "\n",
                "            # Market\n",
                "            idx_key = f'{iso}_index'\n",
                "            if idx_key in market_data:\n",
                "                prices = self._interpolate(market_data[idx_key], all_dates)\n",
                "                if (prices > 0).sum() > len(prices) * 0.3:\n",
                "                    has_market[iso] = True\n",
                "                    feat['market_z'] = self._to_zscore(prices, 20)\n",
                "                    feat['mom_20'] = (prices / prices.shift(20) - 1).clip(-0.5, 0.5)\n",
                "                    feat['vol_20'] = np.log(prices/prices.shift(1)).rolling(20).std() * np.sqrt(252)\n",
                "\n",
                "            # Macro\n",
                "            for ind_name, series in te_iso.items():\n",
                "                s = self._interpolate(series, all_dates)\n",
                "                feat[f'{ind_name}_z'] = self._to_zscore(s, 60)\n",
                "\n",
                "            node_features[iso] = feat.fillna(0)\n",
                "\n",
                "        # Global Features\n",
                "        global_feats = pd.DataFrame(index=all_dates)\n",
                "        global_feats['GGSI'] = ggsi\n",
                "        for c in ['Oil', 'Gold', 'Copper']:\n",
                "            if c in market_data:\n",
                "                s = self._interpolate(market_data[c], all_dates)\n",
                "                global_feats[f'{c}_z'] = self._to_zscore(s, 20)\n",
                "        \n",
                "        global_feats = global_feats.fillna(0)\n",
                "\n",
                "        # Tensor Construction\n",
                "        all_cols = sorted(list(set().union(*[df.columns for df in node_features.values()])))\n",
                "        \n",
                "        n_time = len(all_dates)\n",
                "        n_nodes = len(self.countries)\n",
                "        n_node_feats = len(all_cols)\n",
                "        n_global_feats = len(global_feats.columns)\n",
                "\n",
                "        tensor = np.zeros((n_time, n_nodes, n_node_feats + n_global_feats))\n",
                "        \n",
                "        for i, iso in enumerate(self.countries):\n",
                "            df = node_features[iso]\n",
                "            for col in all_cols:\n",
                "                if col not in df.columns:\n",
                "                    df[col] = 0\n",
                "            tensor[:, i, :n_node_feats] = df[all_cols].values\n",
                "            tensor[:, i, n_node_feats:] = global_feats.values\n",
                "\n",
                "        # Labels\n",
                "        labels = np.zeros((n_time, n_nodes))\n",
                "        valid_mask = np.zeros((n_time, n_nodes))\n",
                "        mkt_idx = all_cols.index('market_z') if 'market_z' in all_cols else 0\n",
                "\n",
                "        for i, iso in enumerate(self.countries):\n",
                "            if has_market[iso]:\n",
                "                mkt_z = tensor[:, i, mkt_idx]\n",
                "                labels[:-5, i] = (mkt_z[5:] > mkt_z[:-5]).astype(float)\n",
                "                valid_mask[:, i] = 1.0\n",
                "\n",
                "        return tensor, labels, valid_mask, self.countries, all_dates, all_cols"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. MODEL & TRAINING"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EconomicGraphDiscoverer(nn.Module):\n",
                "    def __init__(self, num_nodes, in_feats, d_model=64, n_layers=2, dropout=0.3):\n",
                "        super().__init__()\n",
                "        self.num_nodes = num_nodes\n",
                "        self.input_proj = nn.Sequential(\n",
                "            nn.Linear(in_feats, d_model),\n",
                "            nn.GELU(),\n",
                "            nn.Dropout(dropout)\n",
                "        )\n",
                "        self.node_embed = nn.Parameter(torch.randn(num_nodes, 32) * 0.1)\n",
                "        self.adj_temperature = 0.3\n",
                "        self.graph_layers = nn.ModuleList([\n",
                "            nn.Sequential(\n",
                "                nn.Linear(d_model, d_model),\n",
                "                nn.GELU(),\n",
                "                nn.Dropout(dropout),\n",
                "                nn.LayerNorm(d_model)\n",
                "            ) for _ in range(n_layers)\n",
                "        ])\n",
                "        self.temporal = nn.GRU(d_model * num_nodes, d_model * 2, batch_first=True, num_layers=2, dropout=dropout)\n",
                "        self.output = nn.Sequential(\n",
                "            nn.Linear(d_model * 2, d_model),\n",
                "            nn.GELU(),\n",
                "            nn.Linear(d_model, num_nodes)\n",
                "        )\n",
                "\n",
                "    def get_adjacency(self):\n",
                "        sim = torch.mm(self.node_embed, self.node_embed.t())\n",
                "        return F.softmax(sim / self.adj_temperature, dim=-1)\n",
                "\n",
                "    def forward(self, x):\n",
                "        b, s, n, f = x.shape\n",
                "        x = self.input_proj(x)\n",
                "        adj = self.get_adjacency()\n",
                "        x_flat = x.view(b * s, n, -1)\n",
                "        for layer in self.graph_layers:\n",
                "            x_agg = torch.bmm(adj.unsqueeze(0).expand(b*s, n, n), x_flat)\n",
                "            x_flat = layer(x_flat + x_agg)\n",
                "        x = x_flat.view(b, s, n, -1)\n",
                "        x_temporal = x.reshape(b, s, -1)\n",
                "        _, h = self.temporal(x_temporal)\n",
                "        return self.output(h[-1])\n",
                "\n",
                "class EconomicDataset(Dataset):\n",
                "    def __init__(self, data, labels, mask, seq_len=45):\n",
                "        self.data = data\n",
                "        self.labels = labels\n",
                "        self.mask = mask\n",
                "        self.seq_len = seq_len\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.data) - self.seq_len - 5\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        x = self.data[idx:idx+self.seq_len]\n",
                "        y = self.labels[idx+self.seq_len]\n",
                "        m = self.mask[idx+self.seq_len]\n",
                "        return torch.FloatTensor(x), torch.FloatTensor(y), torch.FloatTensor(m)\n",
                "\n",
                "def main():\n",
                "    assembler = GlobalDataAssembler(DATA_PATH)\n",
                "    tensor, labels, mask, nodes, dates, cols = assembler.fetch()\n",
                "    \n",
                "    if tensor is None:\n",
                "        return\n",
                "\n",
                "    print(f\"\\nüìä Tensor Shape: {tensor.shape} (Time, Nodes, Feats)\")\n",
                "    print(f\"üåç Nodes: {nodes}\")\n",
                "\n",
                "    split = int(len(tensor) * 0.8)\n",
                "    train_ds = EconomicDataset(tensor[:split], labels[:split], mask[:split])\n",
                "    val_ds = EconomicDataset(tensor[split:], labels[split:], mask[split:])\n",
                "    \n",
                "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
                "    val_loader = DataLoader(val_ds, batch_size=64)\n",
                "\n",
                "    model = EconomicGraphDiscoverer(len(nodes), tensor.shape[2]).to(DEVICE)\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
                "    \n",
                "    print(\"\\nüöÄ Training...\")\n",
                "    for epoch in range(50):\n",
                "        model.train()\n",
                "        total_loss = 0\n",
                "        for x, y, m in train_loader:\n",
                "            x, y, m = x.to(DEVICE), y.to(DEVICE), m.to(DEVICE)\n",
                "            optimizer.zero_grad()\n",
                "            out = model(x)\n",
                "            loss = F.binary_cross_entropy_with_logits(out, y, reduction='none')\n",
                "            loss = (loss * m).sum() / (m.sum() + 1e-8)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            total_loss += loss.item()\n",
                "        \n",
                "        if (epoch+1) % 10 == 0:\n",
                "            print(f\"Epoch {epoch+1}: Loss {total_loss/len(train_loader):.4f}\")\n",
                "\n",
                "    adj = model.get_adjacency().cpu().detach().numpy()\n",
                "    plt.figure(figsize=(12, 10))\n",
                "    sns.heatmap(adj, xticklabels=nodes, yticklabels=nodes, cmap='RdYlBu_r')\n",
                "    plt.title(\"Learned Global Economic Dependencies (V13)\")\n",
                "    plt.show()\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}