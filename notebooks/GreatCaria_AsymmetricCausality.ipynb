{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Great Caria: Asymmetric Causality Engine\n",
                "\n",
                "**Goal**: Detect asymmetric causal relationships between global economies.\n",
                "\n",
                "Core insight: Tightening propagates faster than easing. A shock from USA→China \n",
                "is not the same as China→USA. We learn these directed, asymmetric edges."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup\n",
                "!pip install torch pandas numpy scipy statsmodels tqdm matplotlib seaborn -q\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from scipy import stats\n",
                "from statsmodels.tsa.stattools import grangercausalitytests\n",
                "from tqdm.auto import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Mount Drive & Find Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# === FIND YOUR DATA ===\n",
                "print('Searching for te_*.parquet files...')\n",
                "found_path = None\n",
                "for root, dirs, files in os.walk('/content/drive'):\n",
                "    parquets = [f for f in files if f.startswith('te_') and f.endswith('.parquet')]\n",
                "    if parquets:\n",
                "        print(f'Found {len(parquets)} country files in: {root}/')\n",
                "        for p in parquets[:5]:\n",
                "            print(f'  - {p}')\n",
                "        if len(parquets) > 5:\n",
                "            print(f'  ... and {len(parquets)-5} more')\n",
                "        found_path = root + '/'\n",
                "        break\n",
                "\n",
                "if found_path:\n",
                "    print(f'\\n==> Set DATA_PATH = \"{found_path}\"')\n",
                "else:\n",
                "    print('No te_*.parquet files found!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SET YOUR DATA PATH HERE ===\n",
                "# Copy the path from above, or set manually\n",
                "DATA_PATH = '/content/drive/MyDrive/Caria/models/data/raw/'  # <-- UPDATE THIS!\n",
                "\n",
                "# Verify\n",
                "if os.path.exists(DATA_PATH):\n",
                "    files = [f for f in os.listdir(DATA_PATH) if f.startswith('te_') and f.endswith('.parquet')]\n",
                "    print(f'✓ Found {len(files)} country files')\n",
                "else:\n",
                "    print(f'✗ Path not found: {DATA_PATH}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Country Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "COUNTRIES = ['USA', 'CHN', 'JPN', 'DEU', 'GBR', 'FRA', 'IND', 'BRA', 'CAN', 'KOR',\n",
                "             'AUS', 'MEX', 'IDN', 'ZAF', 'CHL', 'SGP', 'NLD', 'HKG', 'CHE', 'TWN', 'VNM', 'NOR']\n",
                "\n",
                "def load_country_data(data_path, countries):\n",
                "    all_data = {}\n",
                "    loaded = []\n",
                "    \n",
                "    for country in tqdm(countries, desc=\"Loading\"):\n",
                "        filepath = f\"{data_path}te_{country}.parquet\"\n",
                "        if not os.path.exists(filepath):\n",
                "            continue\n",
                "        try:\n",
                "            df = pd.read_parquet(filepath)\n",
                "            if 'date' in df.columns:\n",
                "                df['date'] = pd.to_datetime(df['date'])\n",
                "                df = df.set_index('date')\n",
                "            df.columns = [f\"{country}_{col}\" for col in df.columns]\n",
                "            all_data[country] = df\n",
                "            loaded.append(country)\n",
                "        except Exception as e:\n",
                "            print(f\"Error {country}: {e}\")\n",
                "    \n",
                "    if not all_data:\n",
                "        raise ValueError(\"No data loaded!\")\n",
                "    \n",
                "    merged = pd.concat(all_data.values(), axis=1, join='outer').sort_index().ffill().dropna()\n",
                "    print(f\"✓ Loaded {len(loaded)} countries, {merged.shape[0]} rows, {merged.shape[1]} features\")\n",
                "    print(f\"  Countries: {loaded}\")\n",
                "    return merged, loaded\n",
                "\n",
                "df_countries, COUNTRIES = load_country_data(DATA_PATH, COUNTRIES)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Granger Causality Baseline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find a common feature to test (e.g., 'Index', 'GDP', etc.)\n",
                "sample_country = COUNTRIES[0]\n",
                "features = [c.replace(f\"{sample_country}_\", \"\") for c in df_countries.columns if c.startswith(f\"{sample_country}_\")]\n",
                "print(f\"Available features: {features[:10]}...\")\n",
                "\n",
                "# Pick first available feature for Granger test\n",
                "TEST_FEATURE = features[0] if features else 'value'\n",
                "print(f\"Using feature: {TEST_FEATURE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_granger_matrix(df, countries, feature, max_lag=4):\n",
                "    n = len(countries)\n",
                "    gc = np.ones((n, n))\n",
                "    \n",
                "    for i, c1 in enumerate(tqdm(countries, desc=\"Granger\")):\n",
                "        for j, c2 in enumerate(countries):\n",
                "            if i == j: continue\n",
                "            col1, col2 = f\"{c1}_{feature}\", f\"{c2}_{feature}\"\n",
                "            if col1 not in df.columns or col2 not in df.columns: continue\n",
                "            try:\n",
                "                data = df[[col2, col1]].dropna()\n",
                "                if len(data) < 100: continue\n",
                "                result = grangercausalitytests(data, maxlag=max_lag, verbose=False)\n",
                "                gc[i, j] = min([result[l][0]['ssr_ftest'][1] for l in range(1, max_lag+1)])\n",
                "            except: pass\n",
                "    return gc\n",
                "\n",
                "gc_matrix = compute_granger_matrix(df_countries, COUNTRIES, TEST_FEATURE)\n",
                "gc_sig = 1 - gc_matrix\n",
                "np.fill_diagonal(gc_sig, 0)\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(gc_sig, xticklabels=COUNTRIES, yticklabels=COUNTRIES, cmap='Reds', annot=True, fmt='.2f')\n",
                "plt.title('Granger Causality: i → j')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Asymmetric Causal Transformer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AsymmetricCausalAttention(nn.Module):\n",
                "    def __init__(self, d_model, n_heads=4, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.n_heads = n_heads\n",
                "        self.d_k = d_model // n_heads\n",
                "        self.W_cause = nn.Linear(d_model, d_model)\n",
                "        self.W_effect = nn.Linear(d_model, d_model)\n",
                "        self.W_value = nn.Linear(d_model, d_model)\n",
                "        self.W_out = nn.Linear(d_model, d_model)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.scale = np.sqrt(self.d_k)\n",
                "    \n",
                "    def forward(self, x, return_attention=False):\n",
                "        B, N, D = x.shape\n",
                "        Q = self.W_cause(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)\n",
                "        K = self.W_effect(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)\n",
                "        V = self.W_value(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)\n",
                "        \n",
                "        attn = F.softmax(torch.matmul(Q, K.transpose(-2, -1)) / self.scale, dim=-1)\n",
                "        attn = self.dropout(attn)\n",
                "        out = torch.matmul(attn, V).transpose(1, 2).contiguous().view(B, N, D)\n",
                "        out = self.W_out(out)\n",
                "        \n",
                "        if return_attention:\n",
                "            return out, attn.mean(dim=1)\n",
                "        return out\n",
                "\n",
                "class TemporalEncoder(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim):\n",
                "        super().__init__()\n",
                "        self.lstm = nn.LSTM(input_dim, hidden_dim, 2, batch_first=True, bidirectional=True)\n",
                "        self.proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        B, T, N, F = x.shape\n",
                "        x = x.permute(0, 2, 1, 3).reshape(B * N, T, F)\n",
                "        out, _ = self.lstm(x)\n",
                "        out = self.proj(out[:, -1, :])\n",
                "        return out.view(B, N, -1)\n",
                "\n",
                "class AsymmetricCausalFormer(nn.Module):\n",
                "    def __init__(self, n_countries, n_features, d_model=64, n_heads=4, n_layers=3):\n",
                "        super().__init__()\n",
                "        self.temporal = TemporalEncoder(n_features, d_model)\n",
                "        self.attns = nn.ModuleList([AsymmetricCausalAttention(d_model, n_heads) for _ in range(n_layers)])\n",
                "        self.norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])\n",
                "        self.pred = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Dropout(0.1), nn.Linear(d_model, 1))\n",
                "    \n",
                "    def forward(self, x, return_attention=False):\n",
                "        h = self.temporal(x)\n",
                "        attns = []\n",
                "        for attn, norm in zip(self.attns, self.norms):\n",
                "            if return_attention:\n",
                "                h_new, a = attn(h, True)\n",
                "                attns.append(a)\n",
                "            else:\n",
                "                h_new = attn(h)\n",
                "            h = norm(h + h_new)\n",
                "        preds = self.pred(h).squeeze(-1)\n",
                "        if return_attention:\n",
                "            return preds, torch.stack(attns).mean(dim=0)\n",
                "        return preds\n",
                "\n",
                "print(\"Model defined ✓\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Create Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CountryDataset(Dataset):\n",
                "    def __init__(self, df, countries, seq_len=60):\n",
                "        self.seq_len = seq_len\n",
                "        self.countries = countries\n",
                "        \n",
                "        # Find common features\n",
                "        all_suffixes = None\n",
                "        for c in countries:\n",
                "            suffixes = set(col.replace(f\"{c}_\", \"\") for col in df.columns if col.startswith(f\"{c}_\"))\n",
                "            all_suffixes = suffixes if all_suffixes is None else all_suffixes & suffixes\n",
                "        self.features = sorted(list(all_suffixes))\n",
                "        self.n_features = len(self.features)\n",
                "        print(f\"Using {self.n_features} common features\")\n",
                "        \n",
                "        # Build tensor\n",
                "        T = len(df)\n",
                "        tensor = np.zeros((T, len(countries), self.n_features))\n",
                "        for i, c in enumerate(countries):\n",
                "            for j, f in enumerate(self.features):\n",
                "                col = f\"{c}_{f}\"\n",
                "                if col in df.columns:\n",
                "                    tensor[:, i, j] = df[col].values\n",
                "        \n",
                "        # Normalize\n",
                "        self.mean = tensor.mean(axis=0, keepdims=True)\n",
                "        self.std = tensor.std(axis=0, keepdims=True) + 1e-8\n",
                "        tensor = (tensor - self.mean) / self.std\n",
                "        \n",
                "        # Targets: next-step change in first feature\n",
                "        targets = np.diff(tensor[:, :, 0], axis=0)\n",
                "        \n",
                "        # Create sequences\n",
                "        self.X, self.y = [], []\n",
                "        for t in range(seq_len, T - 1):\n",
                "            self.X.append(tensor[t-seq_len:t])\n",
                "            self.y.append(targets[t])\n",
                "        self.X, self.y = np.array(self.X), np.array(self.y)\n",
                "        print(f\"Dataset: {len(self.X)} samples\")\n",
                "    \n",
                "    def __len__(self): return len(self.X)\n",
                "    def __getitem__(self, i): return torch.FloatTensor(self.X[i]), torch.FloatTensor(self.y[i])\n",
                "\n",
                "dataset = CountryDataset(df_countries, COUNTRIES, seq_len=60)\n",
                "train_size = int(0.8 * len(dataset))\n",
                "train_data, test_data = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
                "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
                "test_loader = DataLoader(test_data, batch_size=32)\n",
                "print(f\"Train: {len(train_data)}, Test: {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = AsymmetricCausalFormer(len(COUNTRIES), dataset.n_features, d_model=64, n_heads=4, n_layers=3).to(device)\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 50)\n",
                "criterion = nn.MSELoss()\n",
                "\n",
                "for epoch in range(50):\n",
                "    model.train()\n",
                "    train_loss = sum(criterion(model(X.to(device)), y.to(device)).backward() or criterion(model(X.to(device)), y.to(device)).item() \n",
                "                     for X, y in train_loader) / len(train_loader) if False else 0\n",
                "    \n",
                "    # Proper training loop\n",
                "    train_loss = 0\n",
                "    for X, y in train_loader:\n",
                "        X, y = X.to(device), y.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        loss = criterion(model(X), y)\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizer.step()\n",
                "        train_loss += loss.item()\n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    model.eval()\n",
                "    test_loss = sum(criterion(model(X.to(device)), y.to(device)).item() for X, y in test_loader) / len(test_loader)\n",
                "    scheduler.step()\n",
                "    \n",
                "    if epoch % 10 == 0:\n",
                "        print(f\"Epoch {epoch}: Train={train_loss:.6f}, Test={test_loss:.6f}\")\n",
                "\n",
                "torch.save(model.state_dict(), 'great_caria_asymmetric.pth')\n",
                "print(\"Model saved ✓\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Extract & Visualize Asymmetric Causality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "all_attn = []\n",
                "with torch.no_grad():\n",
                "    for X, _ in test_loader:\n",
                "        _, attn = model(X.to(device), return_attention=True)\n",
                "        all_attn.append(attn.cpu().numpy())\n",
                "\n",
                "causality = np.concatenate(all_attn, axis=0).mean(axis=0)\n",
                "asymmetry = np.abs(causality - causality.T)\n",
                "\n",
                "print(f\"Asymmetry Score: {asymmetry.mean():.4f} (0=symmetric, higher=more asymmetric)\")\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
                "sns.heatmap(causality, xticklabels=COUNTRIES, yticklabels=COUNTRIES, cmap='Blues', ax=axes[0], annot=True, fmt='.2f')\n",
                "axes[0].set_title('Learned Causality: i → j')\n",
                "sns.heatmap(asymmetry, xticklabels=COUNTRIES, yticklabels=COUNTRIES, cmap='Reds', ax=axes[1], annot=True, fmt='.2f')\n",
                "axes[1].set_title('Asymmetry: |A[i,j] - A[j,i]|')\n",
                "plt.tight_layout()\n",
                "plt.savefig('great_caria_causality.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Top Asymmetric Pairs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pairs = []\n",
                "for i in range(len(COUNTRIES)):\n",
                "    for j in range(i+1, len(COUNTRIES)):\n",
                "        i_to_j, j_to_i = causality[i,j], causality[j,i]\n",
                "        dominant = f\"{COUNTRIES[i]} → {COUNTRIES[j]}\" if i_to_j > j_to_i else f\"{COUNTRIES[j]} → {COUNTRIES[i]}\"\n",
                "        ratio = max(i_to_j, j_to_i) / (min(i_to_j, j_to_i) + 1e-6)\n",
                "        pairs.append({'pair': dominant, 'asymmetry': asymmetry[i,j], 'ratio': ratio})\n",
                "\n",
                "pairs = sorted(pairs, key=lambda x: x['asymmetry'], reverse=True)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"TOP 10 ASYMMETRIC CAUSAL RELATIONSHIPS\")\n",
                "print(\"=\"*50)\n",
                "for i, p in enumerate(pairs[:10]):\n",
                "    print(f\"{i+1}. {p['pair']} (asymmetry: {p['asymmetry']:.4f}, ratio: {p['ratio']:.1f}x)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Export for Frontend"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "output = {\n",
                "    'modelVersion': 'Great Caria v1.0 - Asymmetric Causality',\n",
                "    'countries': COUNTRIES,\n",
                "    'causality_matrix': causality.tolist(),\n",
                "    'asymmetry_matrix': asymmetry.tolist(),\n",
                "    'top_pairs': pairs[:10],\n",
                "    'stats': {'avg_asymmetry': float(asymmetry.mean()), 'max_asymmetry': float(asymmetry.max())}\n",
                "}\n",
                "\n",
                "with open('great_caria_signals.json', 'w') as f:\n",
                "    json.dump(output, f, indent=2)\n",
                "\n",
                "print(\"Exported to great_caria_signals.json ✓\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}