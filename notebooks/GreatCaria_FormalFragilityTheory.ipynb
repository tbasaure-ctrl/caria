{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GREAT CARIA - Formal Fragility Theory\n\n## Framework:\n1. **F_t Formal Definition**: Latent fragility as interaction of signals\n2. **Temporal Decomposition**: Slow/Medium/Fast bands\n3. **Extended Crisis Catalog**: 10+ events (not just 2)\n4. **Bifurcation Dynamics**: Cusp catastrophe, saddle-node, dynamic coupling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install PyWavelets networkx scikit-learn -q\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from scipy import stats, signal, optimize\n",
                "from scipy.ndimage import gaussian_filter1d\n",
                "from scipy.linalg import eigh\n",
                "import pywt\n",
                "import networkx as nx\n",
                "from sklearn.decomposition import PCA, FactorAnalysis\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, roc_auc_score\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm.auto import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "MARKET_PATH = '/content/drive/MyDrive/CARIA/data/raw/yahoo_market.parquet'\n",
                "df = pd.read_parquet(MARKET_PATH)\n",
                "COUNTRIES = ['USA', 'CHN', 'JPN', 'DEU', 'GBR', 'FRA', 'BRA', 'MEX', 'KOR', 'AUS', 'IND', 'ZAF']\n",
                "print(f'Data: {df.shape}, Range: {df.index.min().date()} to {df.index.max().date()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# CRISIS CATALOG (Extended)\n\n| Event | Date | Type |\n|-------|------|------|\n| DotCom Peak | 2000-03-10 | Bubble |\n| 9/11 | 2001-09-11 | Shock |\n| Lehman | 2008-09-15 | Systemic |\n| Flash Crash | 2010-05-06 | Liquidity |\n| Euro Crisis | 2011-08-05 | Sovereign |\n| Taper Tantrum | 2013-05-22 | Policy |\n| China Crash | 2015-08-24 | Emerging |\n| Brexit | 2016-06-24 | Political |\n| Volmageddon | 2018-02-05 | Vol |\n| Repo Crisis | 2019-09-17 | Liquidity |\n| COVID | 2020-03-11 | Pandemic |\n| Gilt Crisis | 2022-09-23 | Sovereign |\n| SVB | 2023-03-10 | Banking |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === EXTENDED CRISIS CATALOG ===\n",
                "CRISES = {\n",
                "    'DotCom': pd.Timestamp('2000-03-10'),\n",
                "    '9/11': pd.Timestamp('2001-09-11'),\n",
                "    'Lehman': pd.Timestamp('2008-09-15'),\n",
                "    'Flash_Crash': pd.Timestamp('2010-05-06'),\n",
                "    'Euro_Crisis': pd.Timestamp('2011-08-05'),\n",
                "    'Taper_Tantrum': pd.Timestamp('2013-05-22'),\n",
                "    'China_Crash': pd.Timestamp('2015-08-24'),\n",
                "    'Brexit': pd.Timestamp('2016-06-24'),\n",
                "    'Volmageddon': pd.Timestamp('2018-02-05'),\n",
                "    'Repo_Crisis': pd.Timestamp('2019-09-17'),\n",
                "    'COVID': pd.Timestamp('2020-03-11'),\n",
                "    'Gilt_Crisis': pd.Timestamp('2022-09-23'),\n",
                "    'SVB': pd.Timestamp('2023-03-10')\n",
                "}\n",
                "\n",
                "# Filter to available data\n",
                "data_start = df.index.min()\n",
                "CRISES = {k: v for k, v in CRISES.items() if v >= data_start}\n",
                "print(f'Crises in data range: {len(CRISES)}')\n",
                "for name, date in CRISES.items():\n",
                "    print(f'  {name}: {date.date()}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === BASE SIGNALS ===\n",
                "idx_cols = [f'{c}_index' for c in COUNTRIES if f'{c}_index' in df.columns]\n",
                "ret = df[idx_cols].pct_change().dropna()\n",
                "ret.columns = [c.replace('_index', '') for c in ret.columns]\n",
                "\n",
                "# 1. Crisis Factor (CF)\n",
                "def compute_cf(r, w=20):\n",
                "    cf = []\n",
                "    for i in range(w, len(r)):\n",
                "        wr = r.iloc[i-w:i]\n",
                "        c = wr.corr().values\n",
                "        ac = (c.sum() - len(c)) / (len(c) * (len(c) - 1))\n",
                "        cf.append(ac * wr.std().mean() * 100)\n",
                "    return pd.Series(cf, index=r.index[w:])\n",
                "\n",
                "CF = compute_cf(ret)\n",
                "\n",
                "# 2. Synchronization (Kuramoto)\n",
                "def extract_phase(series):\n",
                "    detrended = series - gaussian_filter1d(series.values, sigma=60)\n",
                "    return np.angle(signal.hilbert(detrended))\n",
                "\n",
                "phases = pd.DataFrame({c: extract_phase(ret[c].fillna(0)) for c in ret.columns}, index=ret.index)\n",
                "\n",
                "def kuramoto_order(phases, window=60):\n",
                "    r = [np.abs(np.exp(1j * phases.iloc[i].values).mean()) for i in range(window, len(phases))]\n",
                "    return pd.Series(r, index=phases.index[window:])\n",
                "\n",
                "SYNC = kuramoto_order(phases)\n",
                "\n",
                "# 3. EWS (ACF, Var, Skew)\n",
                "def compute_ews(series, window=120):\n",
                "    return pd.DataFrame({\n",
                "        'acf1': series.rolling(window).apply(lambda x: x.autocorr(1), raw=False),\n",
                "        'var': series.rolling(window).var(),\n",
                "        'skew': series.rolling(window).skew()\n",
                "    })\n",
                "\n",
                "EWS = compute_ews(CF)\n",
                "\n",
                "# 4. Network curvature proxy (avg correlation)\n",
                "def rolling_avg_corr(r, window=60):\n",
                "    corrs = []\n",
                "    for i in range(window, len(r)):\n",
                "        c = r.iloc[i-window:i].corr().values\n",
                "        avg = (c.sum() - len(c)) / (len(c) * (len(c) - 1))\n",
                "        corrs.append(avg)\n",
                "    return pd.Series(corrs, index=r.index[window:])\n",
                "\n",
                "CURV = 1 - rolling_avg_corr(ret, 60)  # Low correlation = low curvature = fragile\n",
                "\n",
                "print(f'Signals computed: CF={len(CF)}, SYNC={len(SYNC)}, EWS={len(EWS.dropna())}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 1: FORMAL FRAGILITY DEFINITION\n\n$$F_t = f(CF_t, S_t, ACF_t, Var_t, Skew_t, Topology_t)$$\n\nUsing **Factor Analysis** to extract a latent fragility variable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1A: Align all signals ===\n",
                "common_idx = CF.index\n",
                "for sig in [SYNC, EWS['acf1'], EWS['var'], EWS['skew'], CURV]:\n",
                "    common_idx = common_idx.intersection(sig.dropna().index)\n",
                "\n",
                "signals_df = pd.DataFrame({\n",
                "    'cf': CF.loc[common_idx],\n",
                "    'sync': SYNC.loc[common_idx],\n",
                "    'acf1': EWS['acf1'].loc[common_idx],\n",
                "    'var': EWS['var'].loc[common_idx],\n",
                "    'skew': EWS['skew'].abs().loc[common_idx],\n",
                "    'curv': CURV.loc[common_idx]\n",
                "}).dropna()\n",
                "\n",
                "print(f'Aligned signals: {signals_df.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1B: Factor Analysis for latent F_t ===\n",
                "print('=== Factor Analysis for Latent Fragility ===')\n",
                "\n",
                "# Standardize\n",
                "scaler = StandardScaler()\n",
                "X = scaler.fit_transform(signals_df)\n",
                "\n",
                "# Factor Analysis with 1 latent factor\n",
                "fa = FactorAnalysis(n_components=1, random_state=42)\n",
                "F_latent = fa.fit_transform(X).flatten()\n",
                "F_t = pd.Series(F_latent, index=signals_df.index, name='F_t')\n",
                "\n",
                "# Loadings show how each signal contributes\n",
                "loadings = pd.DataFrame({\n",
                "    'signal': signals_df.columns,\n",
                "    'loading': fa.components_[0]\n",
                "}).sort_values('loading', ascending=False)\n",
                "\n",
                "print('\\nFactor Loadings (contribution to latent fragility):')\n",
                "print(loadings.to_string(index=False))\n",
                "\n",
                "# Explained variance\n",
                "pca = PCA(n_components=1)\n",
                "pca.fit(X)\n",
                "print(f'\\nVariance explained by F_t: {pca.explained_variance_ratio_[0]:.1%}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1C: Validate F_t against crises ===\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(14, 5))\n",
                "ax.fill_between(F_t.index, F_t.values, alpha=0.3, color='red')\n",
                "ax.plot(F_t.index, F_t.values, 'r-', linewidth=0.5)\n",
                "ax.axhline(F_t.quantile(0.9), color='orange', linestyle='--', label='90th pct')\n",
                "\n",
                "for name, date in CRISES.items():\n",
                "    if date in F_t.index or (date > F_t.index.min() and date < F_t.index.max()):\n",
                "        ax.axvline(date, color='blue', alpha=0.5, linestyle=':')\n",
                "        ax.text(date, ax.get_ylim()[1], name, rotation=90, fontsize=8, va='top')\n",
                "\n",
                "ax.set_title('Latent Fragility F_t (Factor Analysis)')\n",
                "ax.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 2: TEMPORAL DECOMPOSITION\n\nDecompose into:\n- **Slow Band** (>60 days): Structural/Cycle\n- **Medium Band** (10-60 days): Resonance/Systemic\n- **Fast Band** (<10 days): Critical noise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 2A: Wavelet decomposition into bands ===\n",
                "print('=== Temporal Band Decomposition ===')\n",
                "\n",
                "def decompose_bands(series, wavelet='db4'):\n",
                "    \"\"\"Decompose into slow/medium/fast bands\"\"\"\n",
                "    coeffs = pywt.wavedec(series.values, wavelet, level=6)\n",
                "    \n",
                "    # Fast (levels 0-2): ~1-8 days\n",
                "    fast_coeffs = [np.zeros_like(c) for c in coeffs]\n",
                "    for i in range(min(3, len(coeffs))):\n",
                "        fast_coeffs[i] = coeffs[i]\n",
                "    fast = pywt.waverec(fast_coeffs, wavelet)[:len(series)]\n",
                "    \n",
                "    # Medium (levels 3-4): ~8-64 days\n",
                "    med_coeffs = [np.zeros_like(c) for c in coeffs]\n",
                "    for i in range(3, min(5, len(coeffs))):\n",
                "        med_coeffs[i] = coeffs[i]\n",
                "    medium = pywt.waverec(med_coeffs, wavelet)[:len(series)]\n",
                "    \n",
                "    # Slow (levels 5+): >64 days\n",
                "    slow_coeffs = [np.zeros_like(c) for c in coeffs]\n",
                "    for i in range(5, len(coeffs)):\n",
                "        slow_coeffs[i] = coeffs[i]\n",
                "    slow = pywt.waverec(slow_coeffs, wavelet)[:len(series)]\n",
                "    \n",
                "    return {\n",
                "        'fast': pd.Series(fast, index=series.index),\n",
                "        'medium': pd.Series(medium, index=series.index),\n",
                "        'slow': pd.Series(slow, index=series.index)\n",
                "    }\n",
                "\n",
                "# Decompose F_t\n",
                "F_bands = decompose_bands(F_t)\n",
                "print(f'Decomposition complete')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 2B: Visualize bands ===\n",
                "\n",
                "fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\n",
                "\n",
                "axes[0].plot(F_t.index, F_t.values, 'k-', alpha=0.7)\n",
                "axes[0].set_ylabel('F_t (full)')\n",
                "axes[0].set_title('Temporal Decomposition of Latent Fragility')\n",
                "\n",
                "axes[1].plot(F_bands['slow'].index, F_bands['slow'].values, 'b-', linewidth=2)\n",
                "axes[1].set_ylabel('Slow (>60d)')\n",
                "axes[1].set_title('Structural/Cycle Component')\n",
                "\n",
                "axes[2].plot(F_bands['medium'].index, F_bands['medium'].values, 'orange', linewidth=1)\n",
                "axes[2].set_ylabel('Medium (10-60d)')\n",
                "axes[2].set_title('Systemic Resonance Component')\n",
                "\n",
                "axes[3].plot(F_bands['fast'].index, F_bands['fast'].values, 'r-', alpha=0.7, linewidth=0.5)\n",
                "axes[3].set_ylabel('Fast (<10d)')\n",
                "axes[3].set_title('Critical Noise Component')\n",
                "\n",
                "for ax in axes:\n",
                "    for name, date in CRISES.items():\n",
                "        if date > F_t.index.min() and date < F_t.index.max():\n",
                "            ax.axvline(date, color='red', alpha=0.3, linestyle=':')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 2C: Which band leads? ===\n",
                "print('\\n=== Band Lead-Lag Analysis ===')\n",
                "\n",
                "def compute_lead(band, crisis_date, threshold_pct=0.8):\n",
                "    \"\"\"How many days before crisis did band cross threshold?\"\"\"\n",
                "    threshold = band.quantile(threshold_pct)\n",
                "    pre = band[(band.index < crisis_date) & (band.index > crisis_date - pd.Timedelta(days=180))]\n",
                "    crossings = pre[pre > threshold]\n",
                "    if len(crossings) > 0:\n",
                "        return (crisis_date - crossings.index[0]).days\n",
                "    return 0\n",
                "\n",
                "band_leads = {band: [] for band in ['slow', 'medium', 'fast']}\n",
                "\n",
                "for crisis_name, crisis_date in CRISES.items():\n",
                "    if crisis_date < F_t.index.min() or crisis_date > F_t.index.max():\n",
                "        continue\n",
                "    for band in ['slow', 'medium', 'fast']:\n",
                "        lead = compute_lead(F_bands[band], crisis_date)\n",
                "        band_leads[band].append(lead)\n",
                "\n",
                "print('Average lead time by band:')\n",
                "for band, leads in band_leads.items():\n",
                "    avg = np.mean(leads) if leads else 0\n",
                "    print(f'  {band:8s}: {avg:.0f} days')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 3: BIFURCATION DYNAMICS\n\n## Cusp Catastrophe Model:\n$$V(x) = \\frac{x^4}{4} + \\frac{a \\cdot x^2}{2} + b \\cdot x$$\n\nWhere:\n- $x$ = system state (F_t)\n- $a$ = asymmetry parameter\n- $b$ = bifurcation parameter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 3A: Cusp catastrophe fitting ===\n",
                "print('=== Cusp Catastrophe Analysis ===')\n",
                "\n",
                "def cusp_potential(x, a, b):\n",
                "    \"\"\"Cusp catastrophe potential V(x) = x^4/4 + a*x^2/2 + b*x\"\"\"\n",
                "    return x**4/4 + a*x**2/2 + b*x\n",
                "\n",
                "def cusp_gradient(x, a, b):\n",
                "    \"\"\"dV/dx = x^3 + a*x + b\"\"\"\n",
                "    return x**3 + a*x + b\n",
                "\n",
                "def fit_cusp_params(series, window=120):\n",
                "    \"\"\"Estimate cusp parameters from time series\"\"\"\n",
                "    a_params = []\n",
                "    b_params = []\n",
                "    bimodality = []\n",
                "    dates = []\n",
                "    \n",
                "    for i in range(window, len(series), 10):\n",
                "        window_data = series.iloc[i-window:i].values\n",
                "        \n",
                "        # Estimate a (related to skewness)\n",
                "        skew = stats.skew(window_data)\n",
                "        \n",
                "        # Estimate b (related to kurtosis)\n",
                "        kurt = stats.kurtosis(window_data)\n",
                "        \n",
                "        # Bimodality coefficient\n",
                "        bimod = (skew**2 + 1) / (kurt + 3 + 1e-8)\n",
                "        \n",
                "        # Map to cusp parameters (heuristic)\n",
                "        a = -np.sign(kurt) * np.abs(kurt) / 10  # Negative a = two minima\n",
                "        b = skew / 5\n",
                "        \n",
                "        a_params.append(a)\n",
                "        b_params.append(b)\n",
                "        bimodality.append(bimod)\n",
                "        dates.append(series.index[i])\n",
                "    \n",
                "    return pd.DataFrame({\n",
                "        'a': a_params,\n",
                "        'b': b_params,\n",
                "        'bimodality': bimodality\n",
                "    }, index=dates)\n",
                "\n",
                "cusp_params = fit_cusp_params(F_t)\n",
                "print(f'Cusp parameters computed: {len(cusp_params)} windows')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 3B: Bifurcation diagram ===\n",
                "\n",
                "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
                "\n",
                "axes[0].plot(cusp_params.index, cusp_params['a'], 'b-')\n",
                "axes[0].axhline(0, color='gray', linestyle='--')\n",
                "axes[0].set_ylabel('a (bifurcation)')\n",
                "axes[0].set_title('Cusp Catastrophe Parameters')\n",
                "# Negative a = unstable (two attractors possible)\n",
                "axes[0].fill_between(cusp_params.index, cusp_params['a'], 0, \n",
                "                     where=cusp_params['a'] < 0, alpha=0.3, color='red')\n",
                "\n",
                "axes[1].plot(cusp_params.index, cusp_params['b'], 'orange')\n",
                "axes[1].axhline(0, color='gray', linestyle='--')\n",
                "axes[1].set_ylabel('b (asymmetry)')\n",
                "\n",
                "axes[2].plot(cusp_params.index, cusp_params['bimodality'], 'purple')\n",
                "axes[2].axhline(0.555, color='red', linestyle='--', label='Bimodal threshold')\n",
                "axes[2].set_ylabel('Bimodality')\n",
                "axes[2].legend()\n",
                "\n",
                "for ax in axes:\n",
                "    for name, date in CRISES.items():\n",
                "        if date > cusp_params.index.min() and date < cusp_params.index.max():\n",
                "            ax.axvline(date, color='red', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 3C: Saddle-node dynamics (proximity to bifurcation) ===\n",
                "print('\\n=== Saddle-Node Dynamics ===')\n",
                "\n",
                "def saddle_node_distance(series, window=60):\n",
                "    \"\"\"Distance to saddle-node bifurcation\n",
                "    \n",
                "    Near bifurcation:\n",
                "    - Variance increases\n",
                "    - ACF(1) â†’ 1 (critical slowing down)\n",
                "    - Skewness changes sign\n",
                "    \"\"\"\n",
                "    distances = []\n",
                "    for i in range(window, len(series)):\n",
                "        w = series.iloc[i-window:i]\n",
                "        \n",
                "        # Normalized indicators\n",
                "        acf1 = w.autocorr(1)\n",
                "        var_norm = w.var() / series.var()  # Relative variance\n",
                "        \n",
                "        # Distance to bifurcation (lower = closer)\n",
                "        dist = (1 - acf1) * (1 / (var_norm + 0.01))\n",
                "        distances.append(dist)\n",
                "    \n",
                "    return pd.Series(distances, index=series.index[window:])\n",
                "\n",
                "saddle_dist = saddle_node_distance(F_t)\n",
                "\n",
                "# Lower distance = closer to bifurcation\n",
                "fig, ax = plt.subplots(figsize=(14, 5))\n",
                "ax.plot(saddle_dist.index, saddle_dist.values, 'g-', alpha=0.7)\n",
                "ax.set_ylabel('Distance to Bifurcation')\n",
                "ax.set_title('Saddle-Node Bifurcation Proximity (lower = closer to crisis)')\n",
                "ax.invert_yaxis()  # Invert so \"closer to crisis\" is up\n",
                "\n",
                "for name, date in CRISES.items():\n",
                "    if date > saddle_dist.index.min() and date < saddle_dist.index.max():\n",
                "        ax.axvline(date, color='red', alpha=0.5)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 4: UNIFIED FRAGILITY TENSOR\n\nCombine all components into a single framework"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 4A: Construct Fragility Tensor ===\n",
                "print('=== Constructing Fragility Tensor ===')\n",
                "\n",
                "# Align all components\n",
                "tensor_idx = F_t.index.intersection(cusp_params.index).intersection(saddle_dist.index)\n",
                "\n",
                "fragility_tensor = pd.DataFrame({\n",
                "    # Latent state\n",
                "    'F_t': F_t.loc[tensor_idx],\n",
                "    \n",
                "    # Temporal components\n",
                "    'F_slow': F_bands['slow'].loc[tensor_idx],\n",
                "    'F_medium': F_bands['medium'].loc[tensor_idx],\n",
                "    'F_fast': F_bands['fast'].loc[tensor_idx],\n",
                "    \n",
                "    # Bifurcation parameters\n",
                "    'cusp_a': cusp_params['a'].loc[tensor_idx],\n",
                "    'cusp_b': cusp_params['b'].loc[tensor_idx],\n",
                "    'bimodality': cusp_params['bimodality'].loc[tensor_idx],\n",
                "    \n",
                "    # Proximity\n",
                "    'saddle_dist': saddle_dist.loc[tensor_idx]\n",
                "})\n",
                "\n",
                "# Normalize each component\n",
                "for col in fragility_tensor.columns:\n",
                "    fragility_tensor[f'{col}_norm'] = (fragility_tensor[col] - fragility_tensor[col].min()) / \\\n",
                "                                       (fragility_tensor[col].max() - fragility_tensor[col].min() + 1e-8)\n",
                "\n",
                "print(f'Fragility Tensor: {fragility_tensor.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 4B: Integrated Fragility Index (IFI) from tensor ===\n",
                "\n",
                "# Weights based on theory:\n",
                "# - Slow band: structural buildup (early)\n",
                "# - Cusp a < 0: bistability (danger)\n",
                "# - Saddle dist low: near bifurcation\n",
                "\n",
                "IFI = (\n",
                "    0.20 * fragility_tensor['F_t_norm'] +\n",
                "    0.25 * fragility_tensor['F_slow_norm'] +\n",
                "    0.15 * fragility_tensor['F_medium_norm'] +\n",
                "    0.10 * fragility_tensor['F_fast_norm'] +\n",
                "    0.15 * (1 - fragility_tensor['cusp_a_norm']) +  # Inverted: low a = dangerous\n",
                "    0.15 * (1 - fragility_tensor['saddle_dist_norm'])  # Inverted: low dist = dangerous\n",
                ")\n",
                "\n",
                "print(f'IFI range: {IFI.min():.3f} - {IFI.max():.3f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 4C: Final visualization ===\n",
                "\n",
                "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
                "\n",
                "# IFI\n",
                "axes[0].fill_between(IFI.index, IFI.values, alpha=0.3, color='red')\n",
                "axes[0].plot(IFI.index, IFI.values, 'r-', linewidth=0.8)\n",
                "axes[0].axhline(IFI.quantile(0.8), color='orange', linestyle='--', label='Warning (80%)')\n",
                "axes[0].axhline(IFI.quantile(0.95), color='darkred', linestyle='--', label='Critical (95%)')\n",
                "axes[0].set_ylabel('IFI')\n",
                "axes[0].set_title('Integrated Fragility Index (Formal Model)')\n",
                "axes[0].legend()\n",
                "\n",
                "# Temporal bands\n",
                "axes[1].plot(fragility_tensor.index, fragility_tensor['F_slow'], 'b-', label='Slow', alpha=0.8)\n",
                "axes[1].plot(fragility_tensor.index, fragility_tensor['F_medium'], 'orange', label='Medium', alpha=0.6)\n",
                "axes[1].plot(fragility_tensor.index, fragility_tensor['F_fast'], 'r-', label='Fast', alpha=0.4)\n",
                "axes[1].set_ylabel('Band Value')\n",
                "axes[1].set_title('Temporal Decomposition')\n",
                "axes[1].legend()\n",
                "\n",
                "# Bifurcation status\n",
                "axes[2].fill_between(cusp_params.index, cusp_params['a'], 0, \n",
                "                     where=cusp_params['a'] < 0, alpha=0.5, color='red', label='Bistable (a<0)')\n",
                "axes[2].fill_between(cusp_params.index, cusp_params['a'], 0,\n",
                "                     where=cusp_params['a'] >= 0, alpha=0.3, color='green', label='Stable (aâ‰¥0)')\n",
                "axes[2].set_ylabel('Cusp a')\n",
                "axes[2].set_title('Bifurcation Status')\n",
                "axes[2].legend()\n",
                "\n",
                "for ax in axes:\n",
                "    for name, date in CRISES.items():\n",
                "        if date > IFI.index.min() and date < IFI.index.max():\n",
                "            ax.axvline(date, color='blue', alpha=0.3, linestyle=':')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/drive/MyDrive/CARIA/models/formal_fragility.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === VALIDATION: Lead time per crisis ===\n",
                "print('\\n=== Lead Time Analysis (All Crises) ===')\n",
                "\n",
                "def compute_lead_all(indicator, crisis_date, pre_days=180, threshold_pct=0.8):\n",
                "    threshold = indicator.quantile(threshold_pct)\n",
                "    pre = indicator[(indicator.index < crisis_date) & \n",
                "                    (indicator.index > crisis_date - pd.Timedelta(days=pre_days))]\n",
                "    crossings = pre[pre > threshold]\n",
                "    return (crisis_date - crossings.index[0]).days if len(crossings) > 0 else 0\n",
                "\n",
                "lead_results = []\n",
                "for crisis_name, crisis_date in CRISES.items():\n",
                "    if crisis_date < IFI.index.min() + pd.Timedelta(days=180):\n",
                "        continue\n",
                "    if crisis_date > IFI.index.max():\n",
                "        continue\n",
                "    \n",
                "    lead_ifi = compute_lead_all(IFI, crisis_date)\n",
                "    lead_slow = compute_lead_all(fragility_tensor['F_slow_norm'], crisis_date)\n",
                "    \n",
                "    lead_results.append({\n",
                "        'crisis': crisis_name,\n",
                "        'date': crisis_date.date(),\n",
                "        'IFI_lead': lead_ifi,\n",
                "        'Slow_lead': lead_slow\n",
                "    })\n",
                "\n",
                "lead_df = pd.DataFrame(lead_results)\n",
                "print(lead_df.to_string(index=False))\n",
                "print(f'\\nAverage lead: IFI={lead_df[\"IFI_lead\"].mean():.0f}d, Slow={lead_df[\"Slow_lead\"].mean():.0f}d')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === EXPORT ===\n",
                "import json\n",
                "\n",
                "export = {\n",
                "    'version': 'Great Caria Formal Fragility v1.0',\n",
                "    'generated': pd.Timestamp.now().isoformat(),\n",
                "    'methodology': 'Factor Analysis + Wavelet Decomposition + Cusp Catastrophe',\n",
                "    'current': {\n",
                "        'ifi': float(IFI.iloc[-1]),\n",
                "        'F_t': float(F_t.iloc[-1]),\n",
                "        'F_slow': float(F_bands['slow'].iloc[-1]),\n",
                "        'F_medium': float(F_bands['medium'].iloc[-1]),\n",
                "        'F_fast': float(F_bands['fast'].iloc[-1]),\n",
                "        'cusp_a': float(cusp_params['a'].iloc[-1]),\n",
                "        'bifurcation_stable': bool(cusp_params['a'].iloc[-1] >= 0)\n",
                "    },\n",
                "    'factor_loadings': loadings.set_index('signal')['loading'].to_dict(),\n",
                "    'thresholds': {\n",
                "        'warning': float(IFI.quantile(0.8)),\n",
                "        'critical': float(IFI.quantile(0.95))\n",
                "    },\n",
                "    'crises_validated': len(lead_results),\n",
                "    'avg_lead_days': float(lead_df['IFI_lead'].mean()) if len(lead_df) > 0 else 0,\n",
                "    'history': [\n",
                "        {'date': d.isoformat(), 'ifi': float(IFI.loc[d])}\n",
                "        for d in IFI.index[-252:]\n",
                "    ]\n",
                "}\n",
                "\n",
                "with open('/content/drive/MyDrive/CARIA/models/formal_fragility_export.json', 'w') as f:\n",
                "    json.dump(export, f, indent=2)\n",
                "\n",
                "print('\\nâœ“ Exported: formal_fragility_export.json')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === FINAL SUMMARY ===\n",
                "print('\\n' + '='*70)\n",
                "print('GREAT CARIA - FORMAL FRAGILITY THEORY')\n",
                "print('='*70)\n",
                "\n",
                "print('\\nðŸ“ FORMAL MODEL:')\n",
                "print('  F_t = FactorAnalysis(CF, Sync, ACF1, Var, Skew, Curvature)')\n",
                "print(f'  Variance explained: {pca.explained_variance_ratio_[0]:.1%}')\n",
                "\n",
                "print('\\nâ±ï¸ TEMPORAL BANDS:')\n",
                "for band, leads in band_leads.items():\n",
                "    print(f'  {band:8s}: avg lead = {np.mean(leads):.0f} days')\n",
                "\n",
                "print('\\nðŸŒ€ BIFURCATION DYNAMICS:')\n",
                "print(f'  Cusp catastrophe model fitted')\n",
                "print(f'  Current stability: {\"STABLE\" if cusp_params[\"a\"].iloc[-1] >= 0 else \"BISTABLE\"}')\n",
                "\n",
                "print('\\nðŸ“Š CRISIS VALIDATION:')\n",
                "print(f'  Crises tested: {len(lead_results)}')\n",
                "print(f'  Average IFI lead: {lead_df[\"IFI_lead\"].mean():.0f} days')\n",
                "\n",
                "print('\\n' + '='*70)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}