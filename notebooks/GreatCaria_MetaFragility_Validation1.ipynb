{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GREAT CARIA - Meta-Fragility Composite + Complexity Validation\n",
    "\n",
    "## Mejoras finales:\n",
    "1. **Meta-Fragility Composite**: Se√±ales m√°s tempranas de cada m√≥dulo\n",
    "2. **Surrogate Testing**: Null hypothesis testing\n",
    "3. **Bifurcation Markers**: Detecci√≥n de puntos cr√≠ticos\n",
    "4. **Bootstrap Confidence**: Intervalos de confianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyWavelets networkx -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats, signal\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import pywt\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MARKET_PATH = '/content/drive/MyDrive/CARIA/data/raw/yahoo_market.parquet'\n",
    "df = pd.read_parquet(MARKET_PATH)\n",
    "COUNTRIES = ['USA', 'CHN', 'JPN', 'DEU', 'GBR', 'FRA', 'BRA', 'MEX', 'KOR', 'AUS', 'IND', 'ZAF']\n",
    "print(f'Data: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BASE COMPUTATIONS (from moonshot) ===\n",
    "idx_cols = [f'{c}_index' for c in COUNTRIES if f'{c}_index' in df.columns]\n",
    "ret = df[idx_cols].pct_change().dropna()\n",
    "ret.columns = [c.replace('_index', '') for c in ret.columns]\n",
    "\n",
    "# Crisis Factor\n",
    "def compute_cf(r, w=20):\n",
    "    cf = []\n",
    "    for i in range(w, len(r)):\n",
    "        wr = r.iloc[i-w:i]\n",
    "        c = wr.corr().values\n",
    "        ac = (c.sum() - len(c)) / (len(c) * (len(c) - 1))\n",
    "        cf.append(ac * wr.std().mean() * 100)\n",
    "    return pd.Series(cf, index=r.index[w:])\n",
    "\n",
    "CF = compute_cf(ret)\n",
    "\n",
    "# Kuramoto sync\n",
    "def extract_phase(series):\n",
    "    detrended = series - gaussian_filter1d(series.values, sigma=60)\n",
    "    analytic = signal.hilbert(detrended)\n",
    "    return np.angle(analytic)\n",
    "\n",
    "phases = pd.DataFrame({c: extract_phase(ret[c].fillna(0)) for c in ret.columns}, index=ret.index)\n",
    "\n",
    "def kuramoto_order(phases, window=60):\n",
    "    r = []\n",
    "    for i in range(window, len(phases)):\n",
    "        ph = phases.iloc[i].values\n",
    "        r.append(np.abs(np.exp(1j * ph).mean()))\n",
    "    return pd.Series(r, index=phases.index[window:])\n",
    "\n",
    "sync_order = kuramoto_order(phases)\n",
    "\n",
    "# EWS\n",
    "def compute_ews(series, window=120):\n",
    "    ac1 = series.rolling(window).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "    var = series.rolling(window).var()\n",
    "    skew = series.rolling(window).skew()\n",
    "    return pd.DataFrame({'ac1': ac1, 'variance': var, 'skewness': skew})\n",
    "\n",
    "ews = compute_ews(CF)\n",
    "print(f'Signals computed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Additional Complexity Metrics (Curvature & TDA) ===\n",
    "print('=== Additional Complexity Metrics ===')\n",
    "import networkx as nx\n",
    "\n",
    "# Rolling window parameters\n",
    "window = 60\n",
    "threshold = 0.7\n",
    "curvature_series = pd.Series(index=ret.index, dtype=float)\n",
    "tda_h1 = pd.Series(index=ret.index, dtype=float)\n",
    "tda_loops = pd.Series(index=ret.index, dtype=float)\n",
    "\n",
    "for date in ret.index:\n",
    "    window_data = ret.loc[:date].tail(window)\n",
    "    if len(window_data) < window:\n",
    "        continue\n",
    "    # correlation matrix\n",
    "    corr = window_data.corr().fillna(0)\n",
    "    # adjacency matrix for positive correlations above threshold\n",
    "    adjacency = (corr.values > threshold).astype(int)\n",
    "    G = nx.from_numpy_array(adjacency)\n",
    "    # compute curvature proxy as average clustering coefficient\n",
    "    try:\n",
    "        curvature_series.loc[date] = nx.average_clustering(G)\n",
    "    except:\n",
    "        curvature_series.loc[date] = np.nan\n",
    "    # compute loops via Euler characteristic (edges - nodes + components)\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    num_components = nx.number_connected_components(G)\n",
    "    loops = num_edges - num_nodes + num_components\n",
    "    tda_loops.loc[date] = loops\n",
    "    tda_h1.loc[date] = loops / max(num_nodes, 1)\n",
    "\n",
    "# Combine into DataFrame\n",
    "tda_df = pd.DataFrame({'h1_lifetime': tda_h1, 'num_loops': tda_loops})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: META-FRAGILITY COMPOSITE\n",
    "\n",
    "Combina las se√±ales M√ÅS TEMPRANAS de cada m√≥dulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1A: Identify earliest signals per module ===\n",
    "print('=== Identifying Earliest Warning Signals ===')\n",
    "\n",
    "# Define crisis events\n",
    "CRISES = {\n",
    "    'Lehman': pd.Timestamp('2008-09-15'),\n",
    "    'COVID': pd.Timestamp('2020-03-11')\n",
    "}\n",
    "\n",
    "def find_lead_time(signal, crisis_date, threshold_pct=0.9):\n",
    "    \"\"\"Find how many days before crisis the signal crossed threshold\"\"\"\n",
    "    threshold = signal.quantile(threshold_pct)\n",
    "    pre_crisis = signal[signal.index < crisis_date].iloc[-120:]  # 6 months before\n",
    "    \n",
    "    # Find first crossing\n",
    "    crossings = pre_crisis[pre_crisis > threshold]\n",
    "    if len(crossings) > 0:\n",
    "        first_cross = crossings.index[0]\n",
    "        return (crisis_date - first_cross).days\n",
    "    return 0\n",
    "\n",
    "# Compute lead times for each signal\n",
    "signals_to_test = {\n",
    "    'CF': CF,\n",
    "    'Sync': sync_order,\n",
    "    'ACF1': ews['ac1'],\n",
    "    'Variance': ews['variance'],\n",
    "    'Skewness': ews['skewness'].abs(),\n",
    "    'Curvature': 1 - curvature_series,\n",
    "    'H1': tda_df['h1_lifetime'],\n",
    "    'Loops': tda_df['num_loops']\n",
    "}\n",
    "lead_times = {}\n",
    "for crisis_name, crisis_date in CRISES.items():\n",
    "    lead_times[crisis_name] = {}\n",
    "    for sig_name, sig in signals_to_test.items():\n",
    "        try:\n",
    "            lead = find_lead_time(sig.dropna(), crisis_date)\n",
    "            lead_times[crisis_name][sig_name] = lead\n",
    "        except:\n",
    "            lead_times[crisis_name][sig_name] = 0\n",
    "\n",
    "print('\\nLead times (days before crisis):')\n",
    "lead_df = pd.DataFrame(lead_times)\n",
    "print(lead_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1B: Create Meta-Fragility with optimal weights ===\n",
    "print('\n",
    "=== Meta-Fragility Composite ===')\n",
    "\n",
    "# Weight signals by their average lead time\n",
    "avg_leads = lead_df.mean(axis=1)\n",
    "weights = avg_leads / avg_leads.sum()\n",
    "print('Signal weights based on lead time:')\n",
    "print(weights)\n",
    "\n",
    "# Align all signals\n",
    "common_idx = CF.dropna().index\n",
    "for s in [sync_order, ews['ac1'], ews['variance'], ews['skewness'].abs(), 1 - curvature_series, tda_df['h1_lifetime'], tda_df['num_loops']]:\n",
    "    common_idx = common_idx.intersection(s.dropna().index)\n",
    "\n",
    "# Normalization helper\n",
    "def normalize(s):\n",
    "    return (s - s.min()) / (s.max() - s.min() + 1e-8)\n",
    "\n",
    "# Compute normalized components (invert curvature to reflect fragility)\n",
    "cf_norm      = normalize(CF.loc[common_idx])\n",
    "sync_norm    = normalize(sync_order.loc[common_idx])\n",
    "acf_norm     = normalize(ews['ac1'].loc[common_idx])\n",
    "var_norm     = normalize(ews['variance'].loc[common_idx])\n",
    "skew_norm    = normalize(ews['skewness'].abs().loc[common_idx])\n",
    "curv_norm    = 1 - normalize(curvature_series.loc[common_idx])\n",
    "h1_norm      = normalize(tda_df['h1_lifetime'].loc[common_idx])\n",
    "loops_norm   = normalize(tda_df['num_loops'].loc[common_idx])\n",
    "\n",
    "# Weighted meta-fragility composite\n",
    "meta_fragility = (\n",
    "    weights['CF']       * cf_norm +\n",
    "    weights['Sync']     * sync_norm +\n",
    "    weights['ACF1']     * acf_norm +\n",
    "    weights['Variance'] * var_norm +\n",
    "    weights['Skewness'] * skew_norm +\n",
    "    weights['Curvature']* curv_norm +\n",
    "    weights['H1']       * h1_norm +\n",
    "    weights['Loops']    * loops_norm\n",
    ")\n",
    "\n",
    "print(f'\n",
    "Meta-Fragility range: {meta_fragility.min():.3f} - {meta_fragility.max():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1C: Compare lead times ===\n",
    "\n",
    "# Old IFI (equal weights)\n",
    "ifi_equal = (\n",
    "    0.2 * normalize(CF.loc[common_idx]) +\n",
    "    0.2 * normalize(sync_order.loc[common_idx]) +\n",
    "    0.2 * normalize(ews['ac1'].loc[common_idx]) +\n",
    "    0.2 * normalize(ews['variance'].loc[common_idx]) +\n",
    "    0.2 * normalize(ews['skewness'].abs().loc[common_idx])\n",
    ")\n",
    "\n",
    "# Compare lead times\n",
    "print('Lead time comparison:')\n",
    "for crisis_name, crisis_date in CRISES.items():\n",
    "    lead_old = find_lead_time(ifi_equal.dropna(), crisis_date)\n",
    "    lead_new = find_lead_time(meta_fragility.dropna(), crisis_date)\n",
    "    print(f'{crisis_name}: IFI={lead_old}d, Meta={lead_new}d, improvement={lead_new-lead_old}d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: SURROGATE TESTING (Null Hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2A: Phase randomization surrogate ===\n",
    "print('=== Surrogate Testing ===')\n",
    "\n",
    "def phase_surrogate(series):\n",
    "    \"\"\"Create phase-randomized surrogate preserving spectrum\"\"\"\n",
    "    n = len(series)\n",
    "    fft = np.fft.rfft(series)\n",
    "    random_phases = np.exp(1j * np.random.uniform(0, 2*np.pi, len(fft)))\n",
    "    # Keep DC and Nyquist real\n",
    "    random_phases[0] = 1\n",
    "    if n % 2 == 0:\n",
    "        random_phases[-1] = 1\n",
    "    surrogate = np.fft.irfft(fft * random_phases, n)\n",
    "    return surrogate\n",
    "\n",
    "# Generate surrogates for each country\n",
    "N_SURROGATES = 100\n",
    "surrogate_syncs = []\n",
    "\n",
    "print(f'Generating {N_SURROGATES} surrogates...')\n",
    "for i in tqdm(range(N_SURROGATES)):\n",
    "    # Randomize phases for each country\n",
    "    surrogate_ret = pd.DataFrame({\n",
    "        c: phase_surrogate(ret[c].fillna(0).values)\n",
    "        for c in ret.columns\n",
    "    }, index=ret.index)\n",
    "    \n",
    "    # Compute sync for surrogate\n",
    "    surr_phases = pd.DataFrame({\n",
    "        c: extract_phase(surrogate_ret[c])\n",
    "        for c in surrogate_ret.columns\n",
    "    })\n",
    "    surr_sync = kuramoto_order(surr_phases)\n",
    "    surrogate_syncs.append(surr_sync.max())  # Max sync under null\n",
    "\n",
    "# Compare\n",
    "real_max_sync = sync_order.max()\n",
    "surr_mean = np.mean(surrogate_syncs)\n",
    "surr_std = np.std(surrogate_syncs)\n",
    "z_score = (real_max_sync - surr_mean) / (surr_std + 1e-8)\n",
    "p_value = 1 - stats.norm.cdf(z_score)\n",
    "\n",
    "print(f'\\nSynchronization null test:')\n",
    "print(f'  Real max sync: {real_max_sync:.3f}')\n",
    "print(f'  Surrogate mean: {surr_mean:.3f} ¬± {surr_std:.3f}')\n",
    "print(f'  Z-score: {z_score:.2f}')\n",
    "print(f'  p-value: {p_value:.4f}')\n",
    "print(f'  ‚úì SIGNIFICANT (p<0.05)' if p_value < 0.05 else '  ‚úó NOT SIGNIFICANT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2B: Shuffle surrogate for ACF1 ===\n",
    "print('\\n=== ACF1 Null Test ===')\n",
    "\n",
    "# Under null: CF is random walk, ACF1 should be ~1 anyway\n",
    "# Better test: does ACF1 increase BEFORE crises more than random?\n",
    "\n",
    "def pre_crisis_increase(signal, crisis_date, pre_days=60):\n",
    "    \"\"\"Measure increase in signal before crisis\"\"\"\n",
    "    pre_60 = signal.loc[(signal.index < crisis_date) & \n",
    "                        (signal.index > crisis_date - pd.Timedelta(days=pre_days))].mean()\n",
    "    pre_180 = signal.loc[(signal.index < crisis_date - pd.Timedelta(days=pre_days)) & \n",
    "                         (signal.index > crisis_date - pd.Timedelta(days=180))].mean()\n",
    "    return pre_60 - pre_180\n",
    "\n",
    "# Real increases\n",
    "real_increases = []\n",
    "for crisis_date in CRISES.values():\n",
    "    try:\n",
    "        inc = pre_crisis_increase(ews['ac1'].dropna(), crisis_date)\n",
    "        real_increases.append(inc)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "real_avg_increase = np.mean(real_increases)\n",
    "\n",
    "# Surrogate: random dates\n",
    "surr_increases = []\n",
    "valid_dates = ews['ac1'].dropna().index[180:-60]\n",
    "for _ in range(500):\n",
    "    random_date = np.random.choice(valid_dates)\n",
    "    inc = pre_crisis_increase(ews['ac1'].dropna(), random_date)\n",
    "    surr_increases.append(inc)\n",
    "\n",
    "surr_mean = np.mean(surr_increases)\n",
    "surr_std = np.std(surr_increases)\n",
    "z_score = (real_avg_increase - surr_mean) / (surr_std + 1e-8)\n",
    "p_value = 1 - stats.norm.cdf(z_score)\n",
    "\n",
    "print(f'Pre-crisis ACF1 increase test:')\n",
    "print(f'  Real increase: {real_avg_increase:.4f}')\n",
    "print(f'  Random dates: {surr_mean:.4f} ¬± {surr_std:.4f}')\n",
    "print(f'  Z-score: {z_score:.2f}')\n",
    "print(f'  p-value: {p_value:.4f}')\n",
    "print(f'  ‚úì SIGNIFICANT' if p_value < 0.05 else '  ‚úó NOT SIGNIFICANT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: BIFURCATION MARKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3A: Potential analysis (bifurcation detection) ===\n",
    "print('=== Bifurcation Detection ===')\n",
    "\n",
    "def estimate_potential(series, window=120, n_bins=50):\n",
    "    \"\"\"Estimate potential function from time series\"\"\"\n",
    "    potentials = []\n",
    "    dates = []\n",
    "    \n",
    "    for i in range(window, len(series), window//2):\n",
    "        window_data = series.iloc[i-window:i].values\n",
    "        \n",
    "        # Histogram -> probability\n",
    "        hist, bin_edges = np.histogram(window_data, bins=n_bins, density=True)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        \n",
    "        # Potential = -log(probability)\n",
    "        potential = -np.log(hist + 1e-10)\n",
    "        \n",
    "        # Number of local minima (stable states)\n",
    "        from scipy.signal import find_peaks\n",
    "        minima, _ = find_peaks(-potential)\n",
    "        n_states = len(minima)\n",
    "        \n",
    "        # Bimodality coefficient\n",
    "        skew = stats.skew(window_data)\n",
    "        kurt = stats.kurtosis(window_data)\n",
    "        bimodality = (skew**2 + 1) / (kurt + 3)\n",
    "        \n",
    "        potentials.append({'n_states': n_states, 'bimodality': bimodality})\n",
    "        dates.append(series.index[i])\n",
    "    \n",
    "    return pd.DataFrame(potentials, index=dates)\n",
    "\n",
    "potential_analysis = estimate_potential(CF)\n",
    "print(f'Potential analysis: {len(potential_analysis)} windows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3B: Flickering detection ===\n",
    "print('\\n=== Flickering Detection ===')\n",
    "\n",
    "def detect_flickering(series, window=60, threshold_std=1.5):\n",
    "    \"\"\"Detect rapid oscillations (flickering) before bifurcation\"\"\"\n",
    "    # Number of sign changes in derivative\n",
    "    diff = series.diff()\n",
    "    sign_changes = (diff * diff.shift(1) < 0).rolling(window).sum()\n",
    "    \n",
    "    # High flickering = more sign changes than normal\n",
    "    mean_changes = sign_changes.mean()\n",
    "    std_changes = sign_changes.std()\n",
    "    flickering = (sign_changes - mean_changes) / (std_changes + 1e-8)\n",
    "    \n",
    "    return flickering\n",
    "\n",
    "flickering = detect_flickering(CF)\n",
    "\n",
    "# Check pre-crisis flickering\n",
    "for crisis_name, crisis_date in CRISES.items():\n",
    "    try:\n",
    "        pre_flicker = flickering.loc[(flickering.index < crisis_date) & \n",
    "                                     (flickering.index > crisis_date - pd.Timedelta(days=60))].mean()\n",
    "        print(f'{crisis_name}: pre-crisis flickering z-score = {pre_flicker:.2f}')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3C: Combined bifurcation indicator ===\n",
    "\n",
    "# Combine signals that indicate approaching bifurcation\n",
    "bifurcation_indicator = pd.DataFrame(index=flickering.dropna().index)\n",
    "common = bifurcation_indicator.index.intersection(ews['ac1'].dropna().index)\n",
    "common = common.intersection(potential_analysis.index)\n",
    "\n",
    "bifurcation_indicator = pd.DataFrame({\n",
    "    'flickering': flickering.loc[common],\n",
    "    'ac1': ews['ac1'].loc[common],\n",
    "    'variance': ews['variance'].loc[common],\n",
    "    'bimodality': potential_analysis['bimodality'].reindex(common, method='ffill')\n",
    "}).dropna()\n",
    "\n",
    "# Normalize and combine\n",
    "for col in bifurcation_indicator.columns:\n",
    "    bifurcation_indicator[col] = normalize(bifurcation_indicator[col])\n",
    "\n",
    "bifurcation_score = bifurcation_indicator.mean(axis=1)\n",
    "\n",
    "print(f'Bifurcation score range: {bifurcation_score.min():.3f} - {bifurcation_score.max():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 4: BOOTSTRAP CONFIDENCE INTERVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4A: Bootstrap prediction accuracy ===\n",
    "print('=== Bootstrap Validation ===')\n",
    "\n",
    "# Target: market drawdown\n",
    "sp500_ret = df['USA_index'].pct_change()\n",
    "sp500_20d = sp500_ret.rolling(20).sum().shift(-20)\n",
    "crisis_target = (sp500_20d < -0.05).astype(int)\n",
    "\n",
    "# Align\n",
    "common = meta_fragility.index.intersection(crisis_target.dropna().index)\n",
    "X = meta_fragility.loc[common].values.reshape(-1, 1)\n",
    "y = crisis_target.loc[common].values\n",
    "\n",
    "# Bootstrap\n",
    "N_BOOTSTRAP = 100\n",
    "PURGE = 30\n",
    "bootstrap_accs = []\n",
    "bootstrap_aucs = []\n",
    "\n",
    "n = len(X)\n",
    "train_size = int(n * 0.7)\n",
    "\n",
    "for _ in tqdm(range(N_BOOTSTRAP)):\n",
    "    # Bootstrap sample from training data\n",
    "    boot_idx = np.random.choice(train_size, train_size, replace=True)\n",
    "    X_train = X[boot_idx]\n",
    "    y_train = y[boot_idx]\n",
    "    \n",
    "    # Fixed test set (after purge)\n",
    "    X_test = X[train_size + PURGE:]\n",
    "    y_test = y[train_size + PURGE:]\n",
    "    \n",
    "    if len(np.unique(y_train)) < 2 or len(X_test) == 0:\n",
    "        continue\n",
    "    \n",
    "    lr = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    y_prob = lr.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    bootstrap_accs.append(accuracy_score(y_test, y_pred))\n",
    "    try:\n",
    "        bootstrap_aucs.append(roc_auc_score(y_test, y_prob))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Confidence intervals\n",
    "acc_ci = np.percentile(bootstrap_accs, [2.5, 97.5])\n",
    "auc_ci = np.percentile(bootstrap_aucs, [2.5, 97.5]) if bootstrap_aucs else [0, 0]\n",
    "\n",
    "print(f'\\nAccuracy: {np.mean(bootstrap_accs):.1%} (95% CI: {acc_ci[0]:.1%} - {acc_ci[1]:.1%})')\n",
    "print(f'AUC: {np.mean(bootstrap_aucs):.3f} (95% CI: {auc_ci[0]:.3f} - {auc_ci[1]:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 5: FINAL VALIDATION SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot everything ===\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 16), sharex=True)\n",
    "\n",
    "# Meta-Fragility\n",
    "axes[0].fill_between(meta_fragility.index, meta_fragility.values, alpha=0.3, color='red')\n",
    "axes[0].plot(meta_fragility.index, meta_fragility.values, 'r-', linewidth=1)\n",
    "axes[0].set_ylabel('Meta-Fragility')\n",
    "axes[0].set_title('Meta-Fragility Composite (weighted by lead time)')\n",
    "\n",
    "# Bifurcation score\n",
    "axes[1].fill_between(bifurcation_score.index, bifurcation_score.values, alpha=0.3, color='purple')\n",
    "axes[1].plot(bifurcation_score.index, bifurcation_score.values, 'purple', linewidth=1)\n",
    "axes[1].set_ylabel('Bifurcation Score')\n",
    "axes[1].set_title('Bifurcation Indicator (flickering + AC1 + variance + bimodality)')\n",
    "\n",
    "# Flickering\n",
    "axes[2].plot(flickering.index, flickering.values, 'orange', alpha=0.7)\n",
    "axes[2].axhline(2, color='red', linestyle='--', label='High flickering')\n",
    "axes[2].set_ylabel('Flickering (z-score)')\n",
    "axes[2].set_title('Flickering Detector')\n",
    "axes[2].legend()\n",
    "\n",
    "# S&P 500\n",
    "sp500 = df['USA_index'].loc[meta_fragility.index].dropna()\n",
    "axes[3].plot(sp500.index, sp500.values, 'b-', alpha=0.7)\n",
    "axes[3].set_ylabel('S&P 500')\n",
    "axes[3].set_yscale('log')\n",
    "\n",
    "# Mark crises\n",
    "for ax in axes:\n",
    "    for name, date in CRISES.items():\n",
    "        ax.axvline(date, color='red', alpha=0.5, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/CARIA/models/meta_fragility_validation.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL SUMMARY ===\n",
    "print('\\n' + '='*70)\n",
    "print('GREAT CARIA - META-FRAGILITY COMPOSITE + COMPLEXITY VALIDATION')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nüìä SIGNAL WEIGHTS (by lead time):')\n",
    "for sig, w in weights.items():\n",
    "    lead = avg_leads[sig]\n",
    "    print(f'  {sig:12s}: weight={w:.2f} (avg lead={lead:.0f}d)')\n",
    "\n",
    "print('\\nüî¨ SURROGATE TESTS:')\n",
    "print(f'  Synchronization: z={z_score:.2f}, p<0.05 = {p_value < 0.05}')\n",
    "\n",
    "print('\\nüìà BOOTSTRAP CONFIDENCE:')\n",
    "print(f'  Accuracy: {np.mean(bootstrap_accs):.1%} [{acc_ci[0]:.1%} - {acc_ci[1]:.1%}]')\n",
    "print(f'  AUC: {np.mean(bootstrap_aucs):.3f} [{auc_ci[0]:.3f} - {auc_ci[1]:.3f}]')\n",
    "\n",
    "print('\\nüåã BIFURCATION DETECTION:')\n",
    "print(f'  Bifurcation score pre-Lehman: {bifurcation_score.loc[(bifurcation_score.index < CRISES[\"Lehman\"]) & (bifurcation_score.index > CRISES[\"Lehman\"] - pd.Timedelta(days=60))].mean():.3f}')\n",
    "print(f'  Bifurcation score pre-COVID: {bifurcation_score.loc[(bifurcation_score.index < CRISES[\"COVID\"]) & (bifurcation_score.index > CRISES[\"COVID\"] - pd.Timedelta(days=60))].mean():.3f}')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('STATUS: Ready for frontend integration')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPORT FOR FRONTEND ===\n",
    "import json\n",
    "\n",
    "export = {\n",
    "    'version': 'Great Caria Meta-Fragility v1.0',\n",
    "    'generated': pd.Timestamp.now().isoformat(),\n",
    "    'current': {\n",
    "        'meta_fragility': float(meta_fragility.iloc[-1]),\n",
    "        'bifurcation_score': float(bifurcation_score.iloc[-1]),\n",
    "        'flickering': float(flickering.iloc[-1]),\n",
    "        'acf1': float(ews['ac1'].iloc[-1]),\n",
    "        'sync_order': float(sync_order.iloc[-1])\n",
    "    },\n",
    "    'thresholds': {\n",
    "        'warning': float(meta_fragility.quantile(0.8)),\n",
    "        'critical': float(meta_fragility.quantile(0.95))\n",
    "    },\n",
    "    'weights': weights.to_dict(),\n",
    "    'validation': {\n",
    "        'accuracy_mean': float(np.mean(bootstrap_accs)),\n",
    "        'accuracy_ci_low': float(acc_ci[0]),\n",
    "        'accuracy_ci_high': float(acc_ci[1]),\n",
    "        'auc_mean': float(np.mean(bootstrap_aucs)),\n",
    "        'surrogate_p_value': float(p_value)\n",
    "    },\n",
    "    'history': [\n",
    "        {'date': d.isoformat(), 'mf': float(meta_fragility.loc[d]), 'bf': float(bifurcation_score.loc[d]) if d in bifurcation_score.index else 0}\n",
    "        for d in meta_fragility.index[-252:]\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('/content/drive/MyDrive/CARIA/models/meta_fragility_export.json', 'w') as f:\n",
    "    json.dump(export, f, indent=2)\n",
    "\n",
    "print('\\n‚úì Exported: meta_fragility_export.json')\n",
    "print('\\nDownload this file and place in: Caria_repo/caria/models/')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
