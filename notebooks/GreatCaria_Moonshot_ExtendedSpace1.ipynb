{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GREAT CARIA MOONSHOT - Extended Economic Space\n",
    "\n",
    "## Components:\n",
    "1. **Alternative Data**: EPU, GPR, GDELT sentiment, Google Trends\n",
    "2. **Multi-scale Analysis**: Wavelets, cross-correlation by frequency\n",
    "3. **Kuramoto Synchronization**: Phase coherence as crisis precursor\n",
    "4. **Ricci Curvature**: Network geometry for fragility\n",
    "5. **Topological Persistence**: Homology for early warning\n",
    "6. **Critical Transitions**: Slowing down, variance signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pywt networkx GraphRicciCurvature gudhi -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats, signal\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import pywt\n",
    "import networkx as nx\n",
    "import gudhi\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MARKET_PATH = '/content/drive/MyDrive/CARIA/data/raw/yahoo_market.parquet'\n",
    "df = pd.read_parquet(MARKET_PATH)\n",
    "COUNTRIES = ['USA', 'CHN', 'JPN', 'DEU', 'GBR', 'FRA', 'BRA', 'MEX', 'KOR', 'AUS', 'IND', 'ZAF']\n",
    "print(f'Data: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BASE DATA ===\n",
    "idx_cols = [f'{c}_index' for c in COUNTRIES if f'{c}_index' in df.columns]\n",
    "ret = df[idx_cols].pct_change().dropna()\n",
    "ret.columns = [c.replace('_index', '') for c in ret.columns]\n",
    "\n",
    "# Crisis Factor\n",
    "def compute_cf(r, w=20):\n",
    "    cf = []\n",
    "    for i in range(w, len(r)):\n",
    "        wr = r.iloc[i-w:i]\n",
    "        c = wr.corr().values\n",
    "        ac = (c.sum() - len(c)) / (len(c) * (len(c) - 1))\n",
    "        cf.append(ac * wr.std().mean() * 100)\n",
    "    return pd.Series(cf, index=r.index[w:])\n",
    "\n",
    "CF = compute_cf(ret)\n",
    "print(f'CF: {len(CF)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: ALTERNATIVE DATA SOURCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1A: Create proxy indicators from available data ===\n",
    "print('=== Alternative Data Proxies ===')\n",
    "\n",
    "# VIX as fear/uncertainty proxy (already have)\n",
    "vix = df['VIX'].loc[CF.index]\n",
    "\n",
    "# DXY as dollar stress proxy\n",
    "dxy = df['DXY'].loc[CF.index]\n",
    "\n",
    "# Oil volatility as geopolitical proxy\n",
    "oil_vol = df['Oil'].pct_change().rolling(20).std().loc[CF.index] * 100\n",
    "\n",
    "# Gold/Oil ratio as risk-off proxy\n",
    "gold_oil = (df['Gold'] / df['Oil']).loc[CF.index]\n",
    "\n",
    "# EM stress: avg EM FX volatility\n",
    "em_fx_cols = [c for c in df.columns if c.endswith('_fx') and c.split('_')[0] in ['BRA', 'MEX', 'ZAF', 'IND']]\n",
    "if em_fx_cols:\n",
    "    em_stress = df[em_fx_cols].pct_change().rolling(20).std().mean(axis=1).loc[CF.index] * 100\n",
    "else:\n",
    "    em_stress = pd.Series(0, index=CF.index)\n",
    "\n",
    "# Dispersion index: std of country returns (low = herding)\n",
    "dispersion = ret.std(axis=1).loc[CF.index]\n",
    "\n",
    "# Combine into alternative data frame\n",
    "alt_data = pd.DataFrame({\n",
    "    'vix': vix,\n",
    "    'dxy': dxy,\n",
    "    'oil_vol': oil_vol,\n",
    "    'gold_oil': gold_oil,\n",
    "    'em_stress': em_stress,\n",
    "    'dispersion': dispersion,\n",
    "    'cf': CF\n",
    "}).dropna()\n",
    "\n",
    "print(f'Alternative data: {alt_data.shape}')\n",
    "print(alt_data.corr()['cf'].sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: MULTI-SCALE WAVELET ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2A: Wavelet decomposition ===\n",
    "print('=== Wavelet Multi-scale Analysis ===')\n",
    "\n",
    "def wavelet_decompose(series, wavelet='db4', level=5):\n",
    "    \"\"\"Decompose into frequency bands\"\"\"\n",
    "    coeffs = pywt.wavedec(series.values, wavelet, level=level)\n",
    "    # Reconstruct each level\n",
    "    reconstructed = {}\n",
    "    for i in range(level + 1):\n",
    "        coeffs_copy = [np.zeros_like(c) for c in coeffs]\n",
    "        coeffs_copy[i] = coeffs[i]\n",
    "        rec = pywt.waverec(coeffs_copy, wavelet)[:len(series)]\n",
    "        reconstructed[f'level_{i}'] = rec\n",
    "    return reconstructed\n",
    "\n",
    "# Decompose CF\n",
    "cf_waves = wavelet_decompose(CF)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
    "for i, (name, wave) in enumerate(cf_waves.items()):\n",
    "    if i < 6:\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        ax.plot(CF.index[:len(wave)], wave[:len(CF)], alpha=0.7)\n",
    "        ax.set_title(f'{name} (frequency band {i})')\n",
    "plt.suptitle('Crisis Factor Wavelet Decomposition')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2B: Cross-correlation by frequency band ===\n",
    "print('\\n=== Cross-correlation by Frequency ===')\n",
    "\n",
    "# Decompose each country\n",
    "country_waves = {}\n",
    "for c in ret.columns:\n",
    "    country_waves[c] = wavelet_decompose(ret[c].fillna(0))\n",
    "\n",
    "# Compute correlation networks at each scale\n",
    "scale_networks = {}\n",
    "for level in range(6):\n",
    "    corr_matrix = np.zeros((len(ret.columns), len(ret.columns)))\n",
    "    for i, c1 in enumerate(ret.columns):\n",
    "        for j, c2 in enumerate(ret.columns):\n",
    "            w1 = country_waves[c1][f'level_{level}']\n",
    "            w2 = country_waves[c2][f'level_{level}']\n",
    "            min_len = min(len(w1), len(w2))\n",
    "            corr_matrix[i, j] = np.corrcoef(w1[:min_len], w2[:min_len])[0, 1]\n",
    "    scale_networks[level] = corr_matrix\n",
    "    avg_corr = (corr_matrix.sum() - len(corr_matrix)) / (len(corr_matrix) * (len(corr_matrix) - 1))\n",
    "    print(f'Level {level}: avg correlation = {avg_corr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: KURAMOTO SYNCHRONIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3A: Phase extraction via Hilbert transform ===\n",
    "print('=== Kuramoto Synchronization Analysis ===')\n",
    "\n",
    "def extract_phase(series):\n",
    "    \"\"\"Extract instantaneous phase using Hilbert transform\"\"\"\n",
    "    # Detrend\n",
    "    detrended = series - gaussian_filter1d(series.values, sigma=60)\n",
    "    # Hilbert transform\n",
    "    analytic = signal.hilbert(detrended)\n",
    "    phase = np.angle(analytic)\n",
    "    return phase\n",
    "\n",
    "# Extract phases for each country\n",
    "phases = pd.DataFrame({c: extract_phase(ret[c].fillna(0)) for c in ret.columns}, index=ret.index)\n",
    "\n",
    "# Kuramoto order parameter r(t)\n",
    "def kuramoto_order_param(phases, window=60):\n",
    "    \"\"\"Compute synchronization order parameter\"\"\"\n",
    "    r = []\n",
    "    for i in range(window, len(phases)):\n",
    "        ph = phases.iloc[i].values\n",
    "        # r = |<e^{i*theta}>|\n",
    "        complex_phases = np.exp(1j * ph)\n",
    "        r_t = np.abs(complex_phases.mean())\n",
    "        r.append(r_t)\n",
    "    return pd.Series(r, index=phases.index[window:])\n",
    "\n",
    "sync_order = kuramoto_order_param(phases, window=60)\n",
    "print(f'Sync order range: {sync_order.min():.3f} - {sync_order.max():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3B: Plot synchronization vs crises ===\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Sync order\n",
    "axes[0].plot(sync_order.index, sync_order.values, 'b-', alpha=0.7)\n",
    "axes[0].axhline(sync_order.quantile(0.9), color='red', linestyle='--', label='90th percentile')\n",
    "axes[0].set_ylabel('Kuramoto r(t)')\n",
    "axes[0].set_title('Global Market Synchronization')\n",
    "axes[0].legend()\n",
    "\n",
    "# Mark crises\n",
    "for name, date in [('Lehman', '2008-09-15'), ('COVID', '2020-03-11')]:\n",
    "    axes[0].axvline(pd.Timestamp(date), color='red', alpha=0.5)\n",
    "    axes[1].axvline(pd.Timestamp(date), color='red', alpha=0.5)\n",
    "\n",
    "# VIX for comparison\n",
    "axes[1].plot(vix.index, vix.values, 'orange', alpha=0.7)\n",
    "axes[1].set_ylabel('VIX')\n",
    "axes[1].set_title('VIX (for comparison)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation sync vs VIX\n",
    "common = sync_order.index.intersection(vix.index)\n",
    "corr_sync_vix = sync_order.loc[common].corr(vix.loc[common])\n",
    "print(f'\\nSync-VIX correlation: {corr_sync_vix:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 4: RICCI CURVATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4A: Ollivier-Ricci curvature on correlation network ===\n",
    "print('=== Ricci Curvature Analysis ===')\n",
    "\n",
    "try:\n",
    "    from GraphRicciCurvature.OllivierRicci import OllivierRicci\n",
    "    HAS_RICCI = True\n",
    "except:\n",
    "    print('GraphRicciCurvature not available, using simplified version')\n",
    "    HAS_RICCI = False\n",
    "\n",
    "def compute_network_curvature(returns, window=60):\n",
    "    \"\"\"Compute average Ricci curvature of correlation network\"\"\"\n",
    "    corr = returns.corr().values\n",
    "    corr = np.clip(corr, -1, 1)\n",
    "    dist = np.sqrt(2 * (1 - corr))\n",
    "    np.fill_diagonal(dist, 0)\n",
    "    \n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    n = len(returns.columns)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if dist[i,j] < 1.5:  # Only close connections\n",
    "                G.add_edge(i, j, weight=1/(dist[i,j]+0.01))\n",
    "    \n",
    "    if HAS_RICCI and G.number_of_edges() > 0:\n",
    "        try:\n",
    "            orc = OllivierRicci(G, alpha=0.5)\n",
    "            orc.compute_ricci_curvature()\n",
    "            curvatures = [d['ricciCurvature'] for _, _, d in orc.G.edges(data=True)]\n",
    "            return np.mean(curvatures) if curvatures else 0\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Simplified: use clustering coefficient as proxy\n",
    "    if G.number_of_nodes() > 2:\n",
    "        return nx.average_clustering(G)\n",
    "    return 0\n",
    "\n",
    "# Compute rolling curvature\n",
    "curvatures = []\n",
    "dates = []\n",
    "window = 60\n",
    "step = 20\n",
    "\n",
    "for i in tqdm(range(window, len(ret), step)):\n",
    "    window_ret = ret.iloc[i-window:i]\n",
    "    curv = compute_network_curvature(window_ret)\n",
    "    curvatures.append(curv)\n",
    "    dates.append(ret.index[i])\n",
    "\n",
    "curvature_series = pd.Series(curvatures, index=dates)\n",
    "print(f'Curvature range: {curvature_series.min():.3f} - {curvature_series.max():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4B: Plot curvature vs crises ===\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(curvature_series.index, curvature_series.values, 'g-', linewidth=1.5)\n",
    "ax.axhline(curvature_series.mean(), color='gray', linestyle='--', label='Mean')\n",
    "\n",
    "# Mark crises\n",
    "for name, date in [('Lehman', '2008-09-15'), ('COVID', '2020-03-11')]:\n",
    "    ax.axvline(pd.Timestamp(date), color='red', alpha=0.7, label=name if name=='Lehman' else '')\n",
    "\n",
    "ax.set_ylabel('Network Curvature (clustering proxy)')\n",
    "ax.set_title('Ricci Curvature of Market Network')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Before vs after crisis\n",
    "lehman = pd.Timestamp('2008-09-15')\n",
    "pre_lehman = curvature_series[(curvature_series.index < lehman) & (curvature_series.index > lehman - pd.Timedelta(days=180))].mean()\n",
    "post_lehman = curvature_series[(curvature_series.index > lehman) & (curvature_series.index < lehman + pd.Timedelta(days=180))].mean()\n",
    "print(f'\\nLehman: pre={pre_lehman:.3f}, post={post_lehman:.3f}, change={post_lehman-pre_lehman:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 5: TOPOLOGICAL PERSISTENCE (TDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5A: Persistent homology of correlation network ===\n",
    "print('=== Topological Data Analysis ===')\n",
    "\n",
    "def compute_persistence_features(returns):\n",
    "    \"\"\"Compute TDA features from returns\"\"\"\n",
    "    corr = returns.corr().values\n",
    "    corr = np.clip(corr, -1, 1)\n",
    "    dist = np.sqrt(2 * (1 - corr))\n",
    "    np.fill_diagonal(dist, 0)\n",
    "    \n",
    "    try:\n",
    "        rips = gudhi.RipsComplex(distance_matrix=dist, max_edge_length=2.5)\n",
    "        st = rips.create_simplex_tree(max_dimension=2)\n",
    "        st.compute_persistence()\n",
    "        \n",
    "        # H0 (connected components)\n",
    "        h0 = st.persistence_intervals_in_dimension(0)\n",
    "        h0_lifetime = np.sum(h0[:, 1] - h0[:, 0]) if len(h0) > 0 else 0\n",
    "        \n",
    "        # H1 (loops)\n",
    "        h1 = st.persistence_intervals_in_dimension(1)\n",
    "        if len(h1) > 0:\n",
    "            lifetimes = h1[:, 1] - h1[:, 0]\n",
    "            lifetimes = lifetimes[np.isfinite(lifetimes)]\n",
    "            h1_lifetime = np.sum(lifetimes) if len(lifetimes) > 0 else 0\n",
    "            n_loops = len(lifetimes)\n",
    "        else:\n",
    "            h1_lifetime = 0\n",
    "            n_loops = 0\n",
    "        \n",
    "        return {'h0_lifetime': h0_lifetime, 'h1_lifetime': h1_lifetime, 'n_loops': n_loops}\n",
    "    except:\n",
    "        return {'h0_lifetime': 0, 'h1_lifetime': 0, 'n_loops': 0}\n",
    "\n",
    "# Compute rolling TDA\n",
    "tda_features = []\n",
    "tda_dates = []\n",
    "window = 60\n",
    "step = 10\n",
    "\n",
    "for i in tqdm(range(window, len(ret), step)):\n",
    "    window_ret = ret.iloc[i-window:i]\n",
    "    feat = compute_persistence_features(window_ret)\n",
    "    tda_features.append(feat)\n",
    "    tda_dates.append(ret.index[i])\n",
    "\n",
    "tda_df = pd.DataFrame(tda_features, index=tda_dates)\n",
    "print(f'TDA features computed: {tda_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5B: TDA as early warning ===\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "axes[0].plot(tda_df.index, tda_df['h1_lifetime'], 'purple', alpha=0.7)\n",
    "axes[0].set_ylabel('H1 Lifetime')\n",
    "axes[0].set_title('Topological Complexity (Loops in Correlation Network)')\n",
    "\n",
    "axes[1].plot(tda_df.index, tda_df['n_loops'], 'orange', alpha=0.7)\n",
    "axes[1].set_ylabel('Number of Loops')\n",
    "\n",
    "for name, date in [('Lehman', '2008-09-15'), ('COVID', '2020-03-11')]:\n",
    "    axes[0].axvline(pd.Timestamp(date), color='red', alpha=0.5)\n",
    "    axes[1].axvline(pd.Timestamp(date), color='red', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 6: CRITICAL TRANSITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6A: Early warning signals ===\n",
    "print('=== Critical Transition Signals ===')\n",
    "\n",
    "def compute_ews(series, window=120):\n",
    "    \"\"\"Compute early warning signals\"\"\"\n",
    "    # Autocorrelation at lag 1\n",
    "    ac1 = series.rolling(window).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "    \n",
    "    # Variance\n",
    "    var = series.rolling(window).var()\n",
    "    \n",
    "    # Skewness\n",
    "    skew = series.rolling(window).skew()\n",
    "    \n",
    "    return pd.DataFrame({'ac1': ac1, 'variance': var, 'skewness': skew})\n",
    "\n",
    "ews = compute_ews(CF)\n",
    "print(f'EWS computed: {ews.dropna().shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6B: Plot EWS vs crises ===\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "axes[0].plot(ews.index, ews['ac1'], 'b-', alpha=0.7)\n",
    "axes[0].set_ylabel('Autocorrelation(1)')\n",
    "axes[0].set_title('Early Warning Signals for Crisis Factor')\n",
    "\n",
    "axes[1].plot(ews.index, ews['variance'], 'g-', alpha=0.7)\n",
    "axes[1].set_ylabel('Variance')\n",
    "\n",
    "axes[2].plot(ews.index, ews['skewness'], 'r-', alpha=0.7)\n",
    "axes[2].set_ylabel('Skewness')\n",
    "\n",
    "# Mark crises\n",
    "for ax in axes:\n",
    "    for name, date in [('Lehman', '2008-09-15'), ('COVID', '2020-03-11')]:\n",
    "        ax.axvline(pd.Timestamp(date), color='red', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check pre-crisis behavior\n",
    "for name, date in [('Lehman', '2008-09-15'), ('COVID', '2020-03-11')]:\n",
    "    try:\n",
    "        d = pd.Timestamp(date)\n",
    "        pre_180 = ews['ac1'].loc[(ews.index < d) & (ews.index > d - pd.Timedelta(days=180))].mean()\n",
    "        pre_60 = ews['ac1'].loc[(ews.index < d) & (ews.index > d - pd.Timedelta(days=60))].mean()\n",
    "        print(f'{name}: AC1 pre-180d={pre_180:.3f}, pre-60d={pre_60:.3f}, trend={pre_60-pre_180:+.3f}')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 7: INTEGRATED FRAGILITY INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7A: Combine all signals into unified index ===\n",
    "print('=== Integrated Fragility Index ===')\n",
    "\n",
    "# Align all signals\n",
    "common_idx = CF.index\n",
    "for series in [sync_order, curvature_series, tda_df['h1_lifetime'], ews['ac1']]:\n",
    "    common_idx = common_idx.intersection(series.dropna().index)\n",
    "\n",
    "# Normalize each signal to [0,1]\n",
    "def normalize(s):\n",
    "    return (s - s.min()) / (s.max() - s.min() + 1e-8)\n",
    "\n",
    "signals = pd.DataFrame({\n",
    "    'cf': normalize(CF.loc[common_idx]),\n",
    "    'sync': normalize(sync_order.loc[common_idx]),\n",
    "    'curv_inv': 1 - normalize(curvature_series.loc[common_idx]),  # Invert: low curv = high fragility\n",
    "    'tda': normalize(tda_df['h1_lifetime'].loc[common_idx]),\n",
    "    'ac1': normalize(ews['ac1'].loc[common_idx]),\n",
    "    'var': normalize(ews['variance'].loc[common_idx])\n",
    "})\n",
    "\n",
    "# Integrated index = weighted average\n",
    "weights = {'cf': 0.25, 'sync': 0.20, 'curv_inv': 0.15, 'tda': 0.15, 'ac1': 0.15, 'var': 0.10}\n",
    "integrated_fragility = sum(signals[k] * v for k, v in weights.items())\n",
    "\n",
    "print(f'Integrated Fragility computed: {len(integrated_fragility)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7B: Plot integrated index ===\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Integrated fragility\n",
    "axes[0].fill_between(integrated_fragility.index, integrated_fragility.values, alpha=0.3, color='red')\n",
    "axes[0].plot(integrated_fragility.index, integrated_fragility.values, 'r-', linewidth=1)\n",
    "axes[0].axhline(integrated_fragility.quantile(0.8), color='orange', linestyle='--', label='Warning (80%)')\n",
    "axes[0].axhline(integrated_fragility.quantile(0.95), color='darkred', linestyle='--', label='Critical (95%)')\n",
    "axes[0].set_ylabel('Integrated Fragility')\n",
    "axes[0].set_title('Great Caria Moonshot: Integrated Systemic Fragility Index')\n",
    "axes[0].legend()\n",
    "\n",
    "# S&P 500 for reference\n",
    "sp500 = df['USA_index'].loc[integrated_fragility.index].dropna()\n",
    "axes[1].plot(sp500.index, sp500.values, 'b-', alpha=0.7)\n",
    "axes[1].set_ylabel('S&P 500')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "# Mark crises\n",
    "for ax in axes:\n",
    "    for name, date in [('Lehman', '2008-09-15'), ('COVID', '2020-03-11')]:\n",
    "        ax.axvline(pd.Timestamp(date), color='red', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7C: Predictive power of integrated index ===\n",
    "print('\\n=== Integrated Index Prediction Test ===')\n",
    "\n",
    "# Target: will market drawdown > 5% in next 20 days?\n",
    "sp500_ret = df['USA_index'].pct_change()\n",
    "sp500_20d = sp500_ret.rolling(20).sum().shift(-20)  # Forward 20d return\n",
    "crisis_target = (sp500_20d < -0.05).astype(int).loc[integrated_fragility.index].dropna()\n",
    "\n",
    "# Features\n",
    "X = signals.loc[crisis_target.index]\n",
    "y = crisis_target\n",
    "\n",
    "# Purged split\n",
    "PURGE = 30\n",
    "n = len(X)\n",
    "train_end = int(n * 0.7)\n",
    "test_start = train_end + PURGE\n",
    "\n",
    "X_train, y_train = X.iloc[:train_end].values, y.iloc[:train_end].values\n",
    "X_test, y_test = X.iloc[test_start:].values, y.iloc[test_start:].values\n",
    "\n",
    "# Model\n",
    "lr = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Baseline (random)\n",
    "y_shuf = np.random.permutation(y_train)\n",
    "lr_shuf = LogisticRegression(max_iter=1000).fit(X_train, y_shuf)\n",
    "acc_shuf = accuracy_score(y_test, lr_shuf.predict(X_test))\n",
    "\n",
    "print(f'Crisis prediction accuracy: {acc:.1%}')\n",
    "print(f'Shuffle baseline: {acc_shuf:.1%}')\n",
    "print(f'Lift: {(acc - acc_shuf)*100:.1f}pp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 8A: Meta-Fragility Composite (New) ===\n",
    "print('=== Meta-Fragility Composite (New) ===')\n",
    "\n",
    "# Define crisis dates for lead calculations (Lehman and COVID events). Adjust or extend as needed.\n",
    "crisis_dates = [pd.Timestamp('2008-09-15'), pd.Timestamp('2020-03-09')]\n",
    "\n",
    "# Collect early-warning signals into a dictionary.\n",
    "# Invert curvature (low curvature implies high fragility) and take absolute skewness.\n",
    "# Include the Crisis Factor itself as a signal to capture its behaviour.\n",
    "signals = {\n",
    "    'CF': CF,\n",
    "    'Sync': sync_order,\n",
    "    'ACF1': ews['ac1'],\n",
    "    'Variance': ews['variance'],\n",
    "    'Skewness': ews['skewness'].abs(),\n",
    "    'Curvature': 1 - curvature_series,\n",
    "    'H1': tda_df['h1_lifetime'],\n",
    "    'Loops': tda_df['num_loops']\n",
    "}\n",
    "\n",
    "# Align signals and drop any dates with missing values\n",
    "signal_df = pd.concat(signals, axis=1).dropna()\n",
    "\n",
    "# Normalize each series to zero mean and unit variance (add epsilon to avoid divide-by-zero)\n",
    "signal_norm = (signal_df - signal_df.mean()) / (signal_df.std() + 1e-8)\n",
    "\n",
    "# Compute average lead times: for each signal and each crisis, find the index of maximum absolute deviation\n",
    "# within a 60 day window preceding the crisis. The lead is the number of days between the crisis and that date.\n",
    "lead_times = {}\n",
    "for name, series in signal_norm.items():\n",
    "    leads = []\n",
    "    for cdate in crisis_dates:\n",
    "        window = series.loc[:cdate].tail(60)\n",
    "        if not window.empty:\n",
    "            idx_max = window.abs().idxmax()\n",
    "            lead = max((cdate - idx_max).days, 1)  # ensure positive\n",
    "            leads.append(lead)\n",
    "    lead_times[name] = np.mean(leads) if leads else 1\n",
    "\n",
    "# Convert lead times to weights by taking their inverse and normalizing\n",
    "weights = {k: 1.0/v for k, v in lead_times.items()}\n",
    "total_w = sum(weights.values())\n",
    "weights = {k: w/total_w for k, w in weights.items()}\n",
    "print('Signal weights (inverse mean lead time):')\n",
    "print(weights)\n",
    "\n",
    "# Construct meta-fragility composite as weighted sum of normalized signals\n",
    "meta_fragility_new = pd.Series(0.0, index=signal_norm.index)\n",
    "for k, w in weights.items():\n",
    "    meta_fragility_new += w * signal_norm[k]\n",
    "\n",
    "# Smooth the series with a 5-day centered rolling average to reduce noise\n",
    "meta_fragility_new = meta_fragility_new.rolling(5, center=True, min_periods=1).mean()\n",
    "\n",
    "# Plot new meta fragility vs original integrated fragility\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(meta_fragility_new.index, meta_fragility_new, label='Meta‑Fragility (New)', linewidth=2)\n",
    "ax.plot(integrated.index, integrated, label='Integrated Fragility (Original)', alpha=0.5)\n",
    "for cdate in crisis_dates:\n",
    "    ax.axvline(cdate, color='red', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('Composite Score')\n",
    "ax.set_title('Meta‑Fragility Composite vs Integrated Fragility Index')\n",
    "ax.legend()\n",
    "\n",
    "# Evaluate predictive power: logistic regression on 20-day forward drawdowns (>5%) in S&P 500\n",
    "print('\n",
    "=== Meta‑Fragility Prediction Test ===')\n",
    "sp500_ret = df['USA_index'].pct_change()\n",
    "target = (sp500_ret.rolling(20).sum().shift(-20) < -0.05).astype(int)\n",
    "common_idx = meta_fragility_new.dropna().index.intersection(target.dropna().index)\n",
    "X = meta_fragility_new.loc[common_idx].values.reshape(-1, 1)\n",
    "y = target.loc[common_idx].values\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X)\n",
    "acc = accuracy_score(y, preds)\n",
    "print(f'Accuracy: {acc * 100:.1f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL SUMMARY ===\n",
    "print('\\n' + '='*60)\n",
    "print('GREAT CARIA MOONSHOT - COMPLETE')\n",
    "print('='*60)\n",
    "print('\\nComponents implemented:')\n",
    "print('  ✓ Alternative data proxies (VIX, DXY, Oil vol, EM stress, dispersion)')\n",
    "print('  ✓ Wavelet multi-scale decomposition')\n",
    "print('  ✓ Kuramoto synchronization (phase coherence)')\n",
    "print('  ✓ Network curvature (Ricci proxy)')\n",
    "print('  ✓ Topological persistence (H0, H1 homology)')\n",
    "print('  ✓ Critical transition signals (AC1, variance, skewness)')\n",
    "print('  ✓ Integrated Fragility Index')\n",
    "print('\\nCrisis prediction with integrated signals:')\n",
    "print(f'  Accuracy: {acc:.1%}, Lift: {(acc-acc_shuf)*100:.1f}pp')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
