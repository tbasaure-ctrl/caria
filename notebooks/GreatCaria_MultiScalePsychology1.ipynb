{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GREAT CARIA - Multi-Scale Fragility with Temporal Relativity\n\n## Core Framework:\n\n**Proposition 1**: Fragility emerges from structural factors at multiple temporal scales\n\n**Proposition 2**: A system can appear stable short-term while hiding long-term tensions\n\n**Proposition 3**: Psychology is an AMPLIFIER, not a cause - weights adjusted accordingly\n\n---\n\n## Temporal Scales:\n| Scale | Horizon | Agents |\n|-------|---------|--------|\n| Ultra-Fast | <1 day | HFT, algorithms |\n| Short | 1-10 days | Traders, retail |\n| Medium | 10-60 days | Hedge funds, tactical |\n| Long | 60-250 days | Institutions |\n| Ultra-Long | >250 days | Central banks, pensions |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install PyWavelets -q\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from scipy import stats, signal\n",
                "from scipy.ndimage import gaussian_filter1d\n",
                "import pywt\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm.auto import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "df = pd.read_parquet('/content/drive/MyDrive/CARIA/data/raw/yahoo_market.parquet')\n",
                "COUNTRIES = ['USA', 'CHN', 'JPN', 'DEU', 'GBR', 'FRA', 'BRA', 'MEX', 'KOR', 'AUS', 'IND', 'ZAF']\n",
                "idx_cols = [f'{c}_index' for c in COUNTRIES if f'{c}_index' in df.columns]\n",
                "ret = df[idx_cols].pct_change().dropna()\n",
                "ret.columns = [c.replace('_index', '') for c in ret.columns]\n",
                "print(f'Data: {ret.shape}, {ret.index.min().date()} to {ret.index.max().date()}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Crisis Catalog ===\n",
                "CRISES = {\n",
                "    'Lehman': pd.Timestamp('2008-09-15'),\n",
                "    'Flash_Crash': pd.Timestamp('2010-05-06'),\n",
                "    'Euro_Crisis': pd.Timestamp('2011-08-05'),\n",
                "    'Taper_Tantrum': pd.Timestamp('2013-05-22'),\n",
                "    'China_Crash': pd.Timestamp('2015-08-24'),\n",
                "    'Brexit': pd.Timestamp('2016-06-24'),\n",
                "    'Volmageddon': pd.Timestamp('2018-02-05'),\n",
                "    'COVID': pd.Timestamp('2020-03-11'),\n",
                "    'Gilt_Crisis': pd.Timestamp('2022-09-23'),\n",
                "    'SVB': pd.Timestamp('2023-03-10')\n",
                "}\n",
                "data_start = ret.index.min()\n",
                "CRISES = {k: v for k, v in CRISES.items() if v > data_start + pd.Timedelta(days=300)}\n",
                "print(f'Crises in range: {len(CRISES)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 1: 5-SCALE TEMPORAL DECOMPOSITION\n\nUsing moving average filters for interpretable scale separation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1A: Base Crisis Factor ===\n",
                "def compute_cf(r, w=20):\n",
                "    cf = []\n",
                "    for i in range(w, len(r)):\n",
                "        wr = r.iloc[i-w:i]\n",
                "        c = wr.corr().values\n",
                "        ac = (c.sum() - len(c)) / (len(c) * (len(c) - 1))\n",
                "        cf.append(ac * wr.std().mean() * 100)\n",
                "    return pd.Series(cf, index=r.index[w:])\n",
                "\n",
                "CF = compute_cf(ret)\n",
                "print(f'CF computed: {len(CF)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1B: 5-Scale Decomposition ===\n",
                "print('=== 5-Scale Temporal Decomposition ===')\n",
                "\n",
                "SCALES = {\n",
                "    'ultra_fast': {'window': 1, 'description': '<1 day (HFT noise)'},\n",
                "    'short': {'window': 5, 'description': '1-10 days (traders)'},\n",
                "    'medium': {'window': 30, 'description': '10-60 days (tactical)'},\n",
                "    'long': {'window': 120, 'description': '60-250 days (institutions)'},\n",
                "    'ultra_long': {'window': 252, 'description': '>250 days (macro cycle)'}\n",
                "}\n",
                "\n",
                "def decompose_scales(series, scales):\n",
                "    \"\"\"Decompose into 5 scales using cascading moving averages\"\"\"\n",
                "    bands = {}\n",
                "    residual = series.copy()\n",
                "    \n",
                "    sorted_scales = sorted(scales.items(), key=lambda x: x[1]['window'])\n",
                "    \n",
                "    for i, (name, config) in enumerate(sorted_scales):\n",
                "        w = config['window']\n",
                "        smooth = residual.rolling(w, min_periods=1).mean()\n",
                "        \n",
                "        if i < len(sorted_scales) - 1:\n",
                "            # This band = current smooth - next smooth\n",
                "            next_w = sorted_scales[i+1][1]['window']\n",
                "            next_smooth = residual.rolling(next_w, min_periods=1).mean()\n",
                "            bands[name] = smooth - next_smooth\n",
                "        else:\n",
                "            # Last band = remaining trend\n",
                "            bands[name] = smooth\n",
                "    \n",
                "    return bands\n",
                "\n",
                "CF_scales = decompose_scales(CF, SCALES)\n",
                "\n",
                "for name, config in SCALES.items():\n",
                "    print(f\"  {name}: {config['description']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1C: Compute indicators at each scale ===\n",
                "print('\\n=== Indicators per Scale ===')\n",
                "\n",
                "def compute_scale_indicators(band, window=60):\n",
                "    \"\"\"Compute variance, ACF1, skewness for each scale\"\"\"\n",
                "    return pd.DataFrame({\n",
                "        'variance': band.rolling(window).var(),\n",
                "        'acf1': band.rolling(window).apply(lambda x: x.autocorr(1) if len(x) > 1 else 0, raw=False),\n",
                "        'skewness': band.rolling(window).skew()\n",
                "    })\n",
                "\n",
                "scale_indicators = {}\n",
                "for name, band in CF_scales.items():\n",
                "    scale_indicators[name] = compute_scale_indicators(band)\n",
                "    print(f\"  {name}: computed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1D: Visualize scales ===\n",
                "\n",
                "fig, axes = plt.subplots(6, 1, figsize=(14, 18), sharex=True)\n",
                "\n",
                "# Full CF\n",
                "axes[0].plot(CF.index, CF.values, 'k-', linewidth=0.5)\n",
                "axes[0].set_ylabel('CF (full)')\n",
                "axes[0].set_title('Crisis Factor: 5-Scale Temporal Decomposition', fontsize=14)\n",
                "\n",
                "# Each scale\n",
                "colors = {'ultra_fast': 'red', 'short': 'orange', 'medium': 'gold', \n",
                "          'long': 'blue', 'ultra_long': 'purple'}\n",
                "\n",
                "for i, (name, band) in enumerate(CF_scales.items()):\n",
                "    ax = axes[i+1]\n",
                "    ax.plot(band.index, band.values, color=colors[name], linewidth=0.8)\n",
                "    ax.set_ylabel(name)\n",
                "    ax.set_title(f\"{SCALES[name]['description']}\", fontsize=10)\n",
                "    \n",
                "    # Mark crises\n",
                "    for date in CRISES.values():\n",
                "        ax.axvline(date, color='gray', alpha=0.3, linestyle=':')\n",
                "\n",
                "axes[0].legend([plt.Line2D([0], [0], color='gray', linestyle=':')], ['Crises'])\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/drive/MyDrive/CARIA/research/formal_fragility/5_scale_decomposition.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 2: LEAD TIME ANALYSIS AND WEIGHT CALIBRATION\n\nWeights inversely proportional to lead time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 2A: Compute lead time for each scale ===\n",
                "print('=== Lead Time Analysis per Scale ===')\n",
                "\n",
                "def compute_lead_time(indicator, crisis_date, threshold_pct=0.8, lookback=180):\n",
                "    \"\"\"Days before crisis that indicator crossed threshold\"\"\"\n",
                "    threshold = indicator.quantile(threshold_pct)\n",
                "    pre = indicator[(indicator.index < crisis_date) & \n",
                "                    (indicator.index > crisis_date - pd.Timedelta(days=lookback))]\n",
                "    crossings = pre[pre > threshold]\n",
                "    if len(crossings) > 0:\n",
                "        return (crisis_date - crossings.index[0]).days\n",
                "    return 0\n",
                "\n",
                "# Compute lead times for variance at each scale\n",
                "lead_times = {scale: [] for scale in SCALES.keys()}\n",
                "\n",
                "for crisis_name, crisis_date in CRISES.items():\n",
                "    for scale in SCALES.keys():\n",
                "        var = scale_indicators[scale]['variance'].dropna()\n",
                "        if len(var) > 0 and crisis_date > var.index.min() + pd.Timedelta(days=180):\n",
                "            lead = compute_lead_time(var, crisis_date)\n",
                "            lead_times[scale].append(lead)\n",
                "\n",
                "# Average lead times\n",
                "avg_leads = {scale: np.mean(leads) if leads else 0 for scale, leads in lead_times.items()}\n",
                "\n",
                "print('\\nAverage Lead Times (days):')\n",
                "for scale, lead in sorted(avg_leads.items(), key=lambda x: -x[1]):\n",
                "    print(f\"  {scale:12s}: {lead:6.1f} days\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 2B: Compute weights inversely proportional to lead time ===\n",
                "print('\\n=== Weight Calibration (1/lead_time) ===')\n",
                "\n",
                "# Avoid division by zero\n",
                "min_lead = 1  # Minimum 1 day\n",
                "adjusted_leads = {k: max(v, min_lead) for k, v in avg_leads.items()}\n",
                "\n",
                "# Inverse weights (longer lead = more weight)\n",
                "raw_weights = {k: v for k, v in adjusted_leads.items()}\n",
                "total = sum(raw_weights.values())\n",
                "scale_weights = {k: v / total for k, v in raw_weights.items()}\n",
                "\n",
                "print('\\nScale Weights (based on lead time):')\n",
                "for scale, weight in sorted(scale_weights.items(), key=lambda x: -x[1]):\n",
                "    print(f\"  {scale:12s}: {weight:.3f} (lead={avg_leads[scale]:.0f}d)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 3: PSYCHOLOGY AS AMPLIFIER (Reduced Weights)\n\nSynchronization and crowding are late signals - reduce their weight"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 3A: Compute synchronization (Kuramoto) ===\n",
                "print('=== Psychology Indicators (Amplifiers) ===')\n",
                "\n",
                "def kuramoto_sync(returns, window=60):\n",
                "    phases = pd.DataFrame({\n",
                "        c: np.angle(signal.hilbert(returns[c].fillna(0) - \n",
                "                                   gaussian_filter1d(returns[c].fillna(0).values, 60)))\n",
                "        for c in returns.columns\n",
                "    }, index=returns.index)\n",
                "    \n",
                "    sync = [np.abs(np.exp(1j * phases.iloc[i].values).mean()) \n",
                "            for i in range(window, len(phases))]\n",
                "    return pd.Series(sync, index=phases.index[window:])\n",
                "\n",
                "SYNC = kuramoto_sync(ret)\n",
                "\n",
                "# Lead time for sync\n",
                "sync_leads = []\n",
                "for crisis_date in CRISES.values():\n",
                "    if crisis_date > SYNC.index.min() + pd.Timedelta(days=180):\n",
                "        lead = compute_lead_time(SYNC, crisis_date)\n",
                "        sync_leads.append(lead)\n",
                "\n",
                "avg_sync_lead = np.mean(sync_leads) if sync_leads else 0\n",
                "print(f'Synchronization avg lead: {avg_sync_lead:.1f} days')\n",
                "print('‚Üí Typically shorter than structural signals ‚Üí AMPLIFIER role confirmed')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 3B: Adjusted weights for psychology ===\n",
                "\n",
                "# Psychology weight based on its lead time\n",
                "# Since it's typically late, it gets lower weight\n",
                "total_structural_lead = sum(avg_leads.values())\n",
                "psych_weight_raw = avg_sync_lead\n",
                "total_lead = total_structural_lead + psych_weight_raw\n",
                "\n",
                "# Final weight allocation\n",
                "STRUCTURAL_WEIGHT = total_structural_lead / total_lead  # ~0.85-0.95\n",
                "PSYCHOLOGY_WEIGHT = psych_weight_raw / total_lead       # ~0.05-0.15\n",
                "\n",
                "print(f'\\nWeight Allocation:')\n",
                "print(f'  Structural factors: {STRUCTURAL_WEIGHT:.1%}')\n",
                "print(f'  Psychology (amplifier): {PSYCHOLOGY_WEIGHT:.1%}')\n",
                "\n",
                "# Redistribute structural weight to scales\n",
                "final_scale_weights = {k: v * STRUCTURAL_WEIGHT for k, v in scale_weights.items()}\n",
                "final_scale_weights['psychology'] = PSYCHOLOGY_WEIGHT"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 4: MULTI-SCALE FRAGILITY INDEX"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 4A: Normalize all signals ===\n",
                "print('=== Constructing Multi-Scale Fragility Index ===')\n",
                "\n",
                "# Align all signals\n",
                "common_idx = CF.index\n",
                "for scale in SCALES.keys():\n",
                "    common_idx = common_idx.intersection(scale_indicators[scale]['variance'].dropna().index)\n",
                "common_idx = common_idx.intersection(SYNC.dropna().index)\n",
                "\n",
                "# Standardize each scale's variance\n",
                "scaler = StandardScaler()\n",
                "normalized = {}\n",
                "\n",
                "for scale in SCALES.keys():\n",
                "    var = scale_indicators[scale]['variance'].loc[common_idx]\n",
                "    normalized[scale] = pd.Series(\n",
                "        scaler.fit_transform(var.values.reshape(-1, 1)).flatten(),\n",
                "        index=common_idx\n",
                "    )\n",
                "\n",
                "# Psychology (sync)\n",
                "normalized['psychology'] = pd.Series(\n",
                "    scaler.fit_transform(SYNC.loc[common_idx].values.reshape(-1, 1)).flatten(),\n",
                "    index=common_idx\n",
                ")\n",
                "\n",
                "print(f'Normalized signals: {len(normalized)} components')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 4B: Combine with calibrated weights ===\n",
                "\n",
                "# Final weights\n",
                "print('\\nFinal Weights:')\n",
                "for name, weight in sorted(final_scale_weights.items(), key=lambda x: -x[1]):\n",
                "    role = 'AMPLIFIER' if name == 'psychology' else 'STRUCTURAL'\n",
                "    print(f\"  {name:12s}: {weight:.3f} ({role})\")\n",
                "\n",
                "# Compute index\n",
                "MSFI = sum(normalized[name] * weight for name, weight in final_scale_weights.items())\n",
                "\n",
                "# Normalize to 0-1 range\n",
                "MSFI = (MSFI - MSFI.min()) / (MSFI.max() - MSFI.min())\n",
                "\n",
                "print(f'\\nMSFI computed: {len(MSFI)} samples')\n",
                "print(f'Range: {MSFI.min():.3f} - {MSFI.max():.3f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 4C: Visualize final index ===\n",
                "\n",
                "fig, axes = plt.subplots(4, 1, figsize=(14, 14), sharex=True)\n",
                "\n",
                "# MSFI\n",
                "axes[0].fill_between(MSFI.index, MSFI.values, alpha=0.3, color='red')\n",
                "axes[0].plot(MSFI.index, MSFI.values, 'r-', linewidth=0.8)\n",
                "axes[0].axhline(MSFI.quantile(0.8), color='orange', linestyle='--', label='Warning (80%)')\n",
                "axes[0].axhline(MSFI.quantile(0.95), color='darkred', linestyle='--', label='Critical (95%)')\n",
                "axes[0].set_ylabel('MSFI')\n",
                "axes[0].set_title('Multi-Scale Fragility Index', fontsize=14)\n",
                "axes[0].legend()\n",
                "\n",
                "# Structural component\n",
                "structural = sum(normalized[s] * final_scale_weights[s] for s in SCALES.keys())\n",
                "axes[1].plot(structural.index, structural.values, 'b-', linewidth=0.8)\n",
                "axes[1].set_ylabel('Structural')\n",
                "axes[1].set_title(f'Structural Component ({STRUCTURAL_WEIGHT:.0%} weight)')\n",
                "\n",
                "# Psychology component\n",
                "psychology = normalized['psychology'] * final_scale_weights['psychology']\n",
                "axes[2].plot(psychology.index, psychology.values, 'purple', linewidth=0.8)\n",
                "axes[2].set_ylabel('Psychology')\n",
                "axes[2].set_title(f'Psychology Amplifier ({PSYCHOLOGY_WEIGHT:.0%} weight)')\n",
                "\n",
                "# S&P 500\n",
                "sp500 = df['USA_index'].loc[MSFI.index].dropna()\n",
                "axes[3].plot(sp500.index, sp500.values, 'k-', linewidth=0.5)\n",
                "axes[3].set_ylabel('S&P 500')\n",
                "axes[3].set_yscale('log')\n",
                "\n",
                "# Mark crises\n",
                "for ax in axes:\n",
                "    for name, date in CRISES.items():\n",
                "        ax.axvline(date, color='gray', alpha=0.4, linestyle=':')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/drive/MyDrive/CARIA/research/formal_fragility/msfi_final.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 5: RELATIVE TEMPORAL PERCEPTION\n\nComparing G7 vs Emerging Markets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 5A: Country groups ===\n",
                "print('=== Temporal Relativity: G7 vs Emerging ===')\n",
                "\n",
                "G7 = ['USA', 'JPN', 'DEU', 'GBR', 'FRA']\n",
                "EM = ['CHN', 'BRA', 'MEX', 'KOR', 'IND', 'ZAF']\n",
                "\n",
                "g7_cols = [c for c in ret.columns if c in G7]\n",
                "em_cols = [c for c in ret.columns if c in EM]\n",
                "\n",
                "# Compute CF for each group\n",
                "CF_G7 = compute_cf(ret[g7_cols])\n",
                "CF_EM = compute_cf(ret[em_cols])\n",
                "\n",
                "# Decompose each\n",
                "G7_scales = decompose_scales(CF_G7, SCALES)\n",
                "EM_scales = decompose_scales(CF_EM, SCALES)\n",
                "\n",
                "print(f'G7 countries: {g7_cols}')\n",
                "print(f'EM countries: {em_cols}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 5B: Compare lead times by group ===\n",
                "\n",
                "group_leads = {'G7': {}, 'EM': {}}\n",
                "\n",
                "for scale in SCALES.keys():\n",
                "    # G7\n",
                "    g7_var = G7_scales[scale].rolling(60).var().dropna()\n",
                "    g7_leads = []\n",
                "    for crisis_date in CRISES.values():\n",
                "        if crisis_date > g7_var.index.min() + pd.Timedelta(days=180):\n",
                "            lead = compute_lead_time(g7_var, crisis_date)\n",
                "            g7_leads.append(lead)\n",
                "    group_leads['G7'][scale] = np.mean(g7_leads) if g7_leads else 0\n",
                "    \n",
                "    # EM\n",
                "    em_var = EM_scales[scale].rolling(60).var().dropna()\n",
                "    em_leads = []\n",
                "    for crisis_date in CRISES.values():\n",
                "        if crisis_date > em_var.index.min() + pd.Timedelta(days=180):\n",
                "            lead = compute_lead_time(em_var, crisis_date)\n",
                "            em_leads.append(lead)\n",
                "    group_leads['EM'][scale] = np.mean(em_leads) if em_leads else 0\n",
                "\n",
                "# Display\n",
                "print('\\nLead Times by Group (days):')\n",
                "print(f'{\"Scale\":12s} | {\"G7\":>8s} | {\"EM\":>8s} | {\"Diff\":>8s}')\n",
                "print('-' * 45)\n",
                "for scale in SCALES.keys():\n",
                "    g7 = group_leads['G7'][scale]\n",
                "    em = group_leads['EM'][scale]\n",
                "    diff = g7 - em\n",
                "    print(f'{scale:12s} | {g7:8.1f} | {em:8.1f} | {diff:+8.1f}')\n",
                "\n",
                "print('\\n‚Üí Different temporal perception = TEMPORAL RELATIVITY confirmed')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 5C: Visualize temporal relativity ===\n",
                "\n",
                "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
                "\n",
                "# Long scale comparison\n",
                "g7_long = G7_scales['long'].rolling(60).var()\n",
                "em_long = EM_scales['long'].rolling(60).var()\n",
                "\n",
                "axes[0].plot(g7_long.index, g7_long.values, 'b-', label='G7', alpha=0.8)\n",
                "axes[0].plot(em_long.index, em_long.values, 'orange', label='EM', alpha=0.8)\n",
                "axes[0].set_ylabel('Variance (long scale)')\n",
                "axes[0].set_title('Temporal Relativity: G7 vs Emerging Markets (60-250d scale)')\n",
                "axes[0].legend()\n",
                "\n",
                "# Lead difference\n",
                "lead_diff = g7_long - em_long\n",
                "axes[1].fill_between(lead_diff.index, lead_diff.values, 0, \n",
                "                     where=lead_diff > 0, alpha=0.5, color='blue', label='G7 higher')\n",
                "axes[1].fill_between(lead_diff.index, lead_diff.values, 0,\n",
                "                     where=lead_diff <= 0, alpha=0.5, color='orange', label='EM higher')\n",
                "axes[1].set_ylabel('G7 - EM')\n",
                "axes[1].set_title('Which group perceives risk earlier?')\n",
                "axes[1].legend()\n",
                "\n",
                "for ax in axes:\n",
                "    for date in CRISES.values():\n",
                "        ax.axvline(date, color='gray', alpha=0.3, linestyle=':')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/drive/MyDrive/CARIA/research/formal_fragility/temporal_relativity.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 6: BIFURCATION DETECTION (Multi-Scale Instability)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 6A: Detect when multiple scales are unstable ===\n",
                "print('=== Bifurcation Detection ===')\n",
                "\n",
                "def multi_scale_instability(scale_indicators, threshold_pct=0.8):\n",
                "    \"\"\"Count how many scales are above their instability threshold\"\"\"\n",
                "    instability_count = pd.Series(0, index=common_idx)\n",
                "    \n",
                "    for scale in SCALES.keys():\n",
                "        var = scale_indicators[scale]['variance'].loc[common_idx]\n",
                "        threshold = var.quantile(threshold_pct)\n",
                "        instability_count += (var > threshold).astype(int)\n",
                "    \n",
                "    return instability_count\n",
                "\n",
                "instability_count = multi_scale_instability(scale_indicators)\n",
                "\n",
                "print(f'Instability count range: {instability_count.min()} - {instability_count.max()} scales')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 6B: Bifurcation warning ===\n",
                "\n",
                "# Bifurcation = 3+ scales unstable simultaneously\n",
                "BIFURCATION_THRESHOLD = 3\n",
                "bifurcation_warning = instability_count >= BIFURCATION_THRESHOLD\n",
                "\n",
                "print(f'Bifurcation warnings: {bifurcation_warning.sum()} days ({bifurcation_warning.mean():.1%})')\n",
                "\n",
                "# Check pre-crisis\n",
                "print('\\nPre-crisis bifurcation status (60d before):')\n",
                "for crisis_name, crisis_date in CRISES.items():\n",
                "    if crisis_date > instability_count.index.min():\n",
                "        pre = instability_count[(instability_count.index < crisis_date) & \n",
                "                                (instability_count.index > crisis_date - pd.Timedelta(days=60))]\n",
                "        avg_unstable = pre.mean()\n",
                "        had_warning = (pre >= BIFURCATION_THRESHOLD).any()\n",
                "        print(f\"  {crisis_name}: avg {avg_unstable:.1f} scales unstable, warning={had_warning}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 6C: Final visualization ===\n",
                "\n",
                "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
                "\n",
                "# MSFI\n",
                "axes[0].fill_between(MSFI.index, MSFI.values, alpha=0.3, color='red')\n",
                "axes[0].plot(MSFI.index, MSFI.values, 'r-', linewidth=0.8)\n",
                "axes[0].set_ylabel('MSFI')\n",
                "axes[0].set_title('Multi-Scale Fragility Index with Bifurcation Detection')\n",
                "\n",
                "# Instability count\n",
                "axes[1].fill_between(instability_count.index, instability_count.values, alpha=0.5, color='purple')\n",
                "axes[1].axhline(BIFURCATION_THRESHOLD, color='red', linestyle='--', label=f'Bifurcation ({BIFURCATION_THRESHOLD}+ scales)')\n",
                "axes[1].set_ylabel('Unstable Scales')\n",
                "axes[1].set_title('Multi-Scale Instability Count')\n",
                "axes[1].legend()\n",
                "\n",
                "# S&P 500 with bifurcation markers\n",
                "sp500 = df['USA_index'].loc[MSFI.index].dropna()\n",
                "axes[2].plot(sp500.index, sp500.values, 'k-', linewidth=0.5)\n",
                "# Mark bifurcation periods\n",
                "bif_periods = bifurcation_warning[bifurcation_warning].index\n",
                "for date in bif_periods[::5]:  # Sample to avoid clutter\n",
                "    if date in sp500.index:\n",
                "        axes[2].axvspan(date, date + pd.Timedelta(days=1), alpha=0.3, color='red')\n",
                "axes[2].set_ylabel('S&P 500')\n",
                "axes[2].set_yscale('log')\n",
                "axes[2].set_title('S&P 500 with Bifurcation Warnings (red)')\n",
                "\n",
                "for ax in axes:\n",
                "    for date in CRISES.values():\n",
                "        ax.axvline(date, color='blue', alpha=0.4, linestyle=':')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/drive/MyDrive/CARIA/research/formal_fragility/bifurcation_detection.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === FINAL SUMMARY ===\n",
                "print('\\n' + '='*70)\n",
                "print('GREAT CARIA - MULTI-SCALE FRAGILITY WITH TEMPORAL RELATIVITY')\n",
                "print('='*70)\n",
                "\n",
                "print('\\nüìä TEMPORAL SCALES:')\n",
                "for scale, config in SCALES.items():\n",
                "    lead = avg_leads.get(scale, 0)\n",
                "    weight = final_scale_weights.get(scale, 0)\n",
                "    print(f\"  {scale:12s}: {config['description']:25s} lead={lead:5.0f}d, weight={weight:.3f}\")\n",
                "\n",
                "print(f\"\\nüß† PSYCHOLOGY (AMPLIFIER):\")\n",
                "print(f\"  Weight: {PSYCHOLOGY_WEIGHT:.1%} (reduced due to short lead time)\")\n",
                "print(f\"  Role: Confirms structural signals, does not initiate\")\n",
                "\n",
                "print(f\"\\nüåç TEMPORAL RELATIVITY:\")\n",
                "print(f\"  G7 avg lead: {np.mean(list(group_leads['G7'].values())):.0f} days\")\n",
                "print(f\"  EM avg lead: {np.mean(list(group_leads['EM'].values())):.0f} days\")\n",
                "print(f\"  ‚Üí Each group perceives risk in different 'temporal dimension'\")\n",
                "\n",
                "print(f\"\\nüåÄ BIFURCATION DETECTION:\")\n",
                "print(f\"  Threshold: {BIFURCATION_THRESHOLD}+ scales unstable\")\n",
                "print(f\"  Total warnings: {bifurcation_warning.sum()} days\")\n",
                "\n",
                "print('\\n' + '='*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === EXPORT ===\n",
                "import json\n",
                "\n",
                "def safe_serialize(obj):\n",
                "    if hasattr(obj, 'isoformat'):\n",
                "        return obj.isoformat()\n",
                "    elif isinstance(obj, (np.integer, np.floating)):\n",
                "        return float(obj)\n",
                "    elif isinstance(obj, dict):\n",
                "        return {str(k): safe_serialize(v) for k, v in obj.items()}\n",
                "    elif isinstance(obj, (list, tuple)):\n",
                "        return [safe_serialize(i) for i in obj]\n",
                "    return obj\n",
                "\n",
                "export = {\n",
                "    'version': 'Great Caria Multi-Scale Fragility v2.0',\n",
                "    'generated': pd.Timestamp.now().isoformat(),\n",
                "    'methodology': {\n",
                "        'scales': SCALES,\n",
                "        'weights': safe_serialize(final_scale_weights),\n",
                "        'structural_weight': float(STRUCTURAL_WEIGHT),\n",
                "        'psychology_weight': float(PSYCHOLOGY_WEIGHT),\n",
                "        'bifurcation_threshold': BIFURCATION_THRESHOLD\n",
                "    },\n",
                "    'lead_times': safe_serialize(avg_leads),\n",
                "    'temporal_relativity': {\n",
                "        'G7': safe_serialize(group_leads['G7']),\n",
                "        'EM': safe_serialize(group_leads['EM'])\n",
                "    },\n",
                "    'thresholds': {\n",
                "        'warning': float(MSFI.quantile(0.8)),\n",
                "        'critical': float(MSFI.quantile(0.95))\n",
                "    },\n",
                "    'current': {\n",
                "        'msfi': float(MSFI.iloc[-1]),\n",
                "        'structural': float(structural.iloc[-1]),\n",
                "        'psychology': float(psychology.iloc[-1]),\n",
                "        'unstable_scales': int(instability_count.iloc[-1]),\n",
                "        'bifurcation_warning': bool(bifurcation_warning.iloc[-1])\n",
                "    },\n",
                "    'crises_validated': len(CRISES)\n",
                "}\n",
                "\n",
                "OUTPUT_DIR = '/content/drive/MyDrive/CARIA/research/formal_fragility'\n",
                "with open(f'{OUTPUT_DIR}/multiscale_fragility_v2.json', 'w') as f:\n",
                "    json.dump(export, f, indent=2)\n",
                "\n",
                "print('\\n‚úì Exported: multiscale_fragility_v2.json')\n",
                "print(f'\\nSaved figures:')\n",
                "print('  5_scale_decomposition.png')\n",
                "print('  msfi_final.png')\n",
                "print('  temporal_relativity.png')\n",
                "print('  bifurcation_detection.png')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}