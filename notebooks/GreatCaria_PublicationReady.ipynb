{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GREAT CARIA - Publication-Ready Fragility Model\n\n## Enhancements:\n1. **Expanded Data Universe**: Sectors, Yield Curve, Credit Spreads, Liquidity, Commodity Vol\n2. **Formal Validation**: Block Bootstrap, Granger/PCMCI Causality, Error Estimation\n3. **Publication-Quality**: Confidence intervals, effect sizes, reproducibility"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install PyWavelets networkx yfinance fredapi tigramite arch -q\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from scipy import stats, signal\n",
                "from scipy.ndimage import gaussian_filter1d\n",
                "import pywt\n",
                "import yfinance as yf\n",
                "from sklearn.decomposition import FactorAnalysis, PCA\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, roc_auc_score\n",
                "from statsmodels.tsa.stattools import grangercausalitytests\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm.auto import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Base data\n",
                "MARKET_PATH = '/content/drive/MyDrive/CARIA/data/raw/yahoo_market.parquet'\n",
                "df_base = pd.read_parquet(MARKET_PATH)\n",
                "print(f'Base data: {df_base.shape}, {df_base.index.min().date()} to {df_base.index.max().date()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 1: EXPANDED DATA UNIVERSE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1A: Fetch additional data from Yahoo Finance ===\n",
                "print('=== Fetching Expanded Data Universe ===')\n",
                "\n",
                "# Define tickers\n",
                "TICKERS = {\n",
                "    # Sector ETFs (US)\n",
                "    'XLF': 'Financials',\n",
                "    'XLK': 'Technology',\n",
                "    'XLE': 'Energy',\n",
                "    'XLV': 'Healthcare',\n",
                "    'XLI': 'Industrials',\n",
                "    'XLP': 'Staples',\n",
                "    'XLY': 'Discretionary',\n",
                "    'XLU': 'Utilities',\n",
                "    'XLB': 'Materials',\n",
                "    'XLRE': 'RealEstate',\n",
                "    \n",
                "    # Yield curve proxies\n",
                "    'TLT': 'Treasury20Y',\n",
                "    'IEF': 'Treasury10Y',\n",
                "    'SHY': 'Treasury3Y',\n",
                "    \n",
                "    # Credit spreads proxies\n",
                "    'HYG': 'HighYield',\n",
                "    'LQD': 'InvestmentGrade',\n",
                "    'JNK': 'JunkBonds',\n",
                "    \n",
                "    # Commodities\n",
                "    'GLD': 'Gold',\n",
                "    'USO': 'Oil',\n",
                "    'UNG': 'NatGas',\n",
                "    \n",
                "    # Volatility\n",
                "    'VIXY': 'VIXFutures',\n",
                "}\n",
                "\n",
                "# Fetch data\n",
                "start_date = df_base.index.min().strftime('%Y-%m-%d')\n",
                "end_date = df_base.index.max().strftime('%Y-%m-%d')\n",
                "\n",
                "expanded_data = {}\n",
                "for ticker, name in tqdm(TICKERS.items()):\n",
                "    try:\n",
                "        data = yf.download(ticker, start=start_date, end=end_date, progress=False)['Adj Close']\n",
                "        if len(data) > 100:\n",
                "            expanded_data[name] = data\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "df_expanded = pd.DataFrame(expanded_data)\n",
                "print(f'\\nExpanded data: {df_expanded.shape}')\n",
                "print(f'Available: {list(df_expanded.columns)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1B: Fetch FRED data (if API key available) ===\n",
                "print('\\n=== FRED Data (Optional) ===')\n",
                "\n",
                "FRED_SERIES = {\n",
                "    'T10Y2Y': 'YieldCurve_10Y2Y',\n",
                "    'T10Y3M': 'YieldCurve_10Y3M',\n",
                "    'BAA10Y': 'CreditSpread_BAA',\n",
                "    'TEDRATE': 'TEDSpread',\n",
                "    'STLFSI4': 'FinancialStress',\n",
                "    'NFCI': 'FinancialConditions',\n",
                "    'VIXCLS': 'VIX_FRED'\n",
                "}\n",
                "\n",
                "try:\n",
                "    from fredapi import Fred\n",
                "    # Try to get API key from environment or use placeholder\n",
                "    import os\n",
                "    fred_key = os.environ.get('FRED_API_KEY', '')\n",
                "    \n",
                "    if fred_key:\n",
                "        fred = Fred(api_key=fred_key)\n",
                "        fred_data = {}\n",
                "        for series_id, name in FRED_SERIES.items():\n",
                "            try:\n",
                "                data = fred.get_series(series_id, start_date, end_date)\n",
                "                if len(data) > 100:\n",
                "                    fred_data[name] = data\n",
                "            except:\n",
                "                pass\n",
                "        df_fred = pd.DataFrame(fred_data)\n",
                "        print(f'FRED data: {df_fred.shape}')\n",
                "    else:\n",
                "        print('No FRED API key found - using Yahoo proxies only')\n",
                "        df_fred = pd.DataFrame()\n",
                "except Exception as e:\n",
                "    print(f'FRED not available: {e}')\n",
                "    df_fred = pd.DataFrame()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1C: Construct derived signals ===\n",
                "print('\\n=== Constructing Derived Signals ===')\n",
                "\n",
                "# Merge all data\n",
                "df_all = df_base.copy()\n",
                "for col in df_expanded.columns:\n",
                "    df_all[col] = df_expanded[col]\n",
                "if len(df_fred) > 0:\n",
                "    for col in df_fred.columns:\n",
                "        df_all[col] = df_fred[col]\n",
                "\n",
                "# Forward fill missing values\n",
                "df_all = df_all.ffill().dropna(how='all')\n",
                "\n",
                "# Derived signals\n",
                "derived = pd.DataFrame(index=df_all.index)\n",
                "\n",
                "# 1. Yield curve slope (proxy from TLT/SHY if available)\n",
                "if 'Treasury20Y' in df_all.columns and 'Treasury3Y' in df_all.columns:\n",
                "    derived['YieldSlope'] = (df_all['Treasury20Y'].pct_change(20) - \n",
                "                             df_all['Treasury3Y'].pct_change(20))\n",
                "\n",
                "# 2. Credit spread proxy (HYG vs LQD)\n",
                "if 'HighYield' in df_all.columns and 'InvestmentGrade' in df_all.columns:\n",
                "    derived['CreditSpread'] = (df_all['HighYield'] / df_all['InvestmentGrade']).pct_change(20)\n",
                "\n",
                "# 3. Sector dispersion\n",
                "sector_cols = [c for c in ['Financials', 'Technology', 'Energy', 'Healthcare', \n",
                "                           'Industrials', 'Staples', 'Discretionary'] if c in df_all.columns]\n",
                "if len(sector_cols) >= 3:\n",
                "    sector_rets = df_all[sector_cols].pct_change()\n",
                "    derived['SectorDispersion'] = sector_rets.std(axis=1)\n",
                "\n",
                "# 4. Risk-on/off ratio\n",
                "if 'Discretionary' in df_all.columns and 'Staples' in df_all.columns:\n",
                "    derived['RiskOnOff'] = (df_all['Discretionary'] / df_all['Staples']).pct_change(20)\n",
                "\n",
                "# 5. Commodity stress\n",
                "if 'Oil' in df_all.columns:\n",
                "    derived['OilVol'] = df_all['Oil'].pct_change().rolling(20).std() * np.sqrt(252)\n",
                "if 'Gold' in df_all.columns:\n",
                "    derived['GoldMomentum'] = df_all['Gold'].pct_change(60)  # Safe haven demand\n",
                "\n",
                "print(f'Derived signals: {derived.dropna(how=\"all\").shape}')\n",
                "print(f'Available: {list(derived.dropna(how=\"all\").columns)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 1D: Compute base signals (from previous notebook) ===\n",
                "print('\\n=== Computing Base Signals ===')\n",
                "\n",
                "COUNTRIES = ['USA', 'CHN', 'JPN', 'DEU', 'GBR', 'FRA', 'BRA', 'MEX', 'KOR', 'AUS', 'IND', 'ZAF']\n",
                "idx_cols = [f'{c}_index' for c in COUNTRIES if f'{c}_index' in df_base.columns]\n",
                "ret = df_base[idx_cols].pct_change().dropna()\n",
                "ret.columns = [c.replace('_index', '') for c in ret.columns]\n",
                "\n",
                "# CF\n",
                "def compute_cf(r, w=20):\n",
                "    cf = []\n",
                "    for i in range(w, len(r)):\n",
                "        wr = r.iloc[i-w:i]\n",
                "        c = wr.corr().values\n",
                "        ac = (c.sum() - len(c)) / (len(c) * (len(c) - 1))\n",
                "        cf.append(ac * wr.std().mean() * 100)\n",
                "    return pd.Series(cf, index=r.index[w:])\n",
                "\n",
                "CF = compute_cf(ret)\n",
                "\n",
                "# Sync\n",
                "def extract_phase(series):\n",
                "    detrended = series - gaussian_filter1d(series.values, sigma=60)\n",
                "    return np.angle(signal.hilbert(detrended))\n",
                "\n",
                "phases = pd.DataFrame({c: extract_phase(ret[c].fillna(0)) for c in ret.columns}, index=ret.index)\n",
                "SYNC = pd.Series([np.abs(np.exp(1j * phases.iloc[i].values).mean()) \n",
                "                  for i in range(60, len(phases))], index=phases.index[60:])\n",
                "\n",
                "# EWS\n",
                "EWS = pd.DataFrame({\n",
                "    'acf1': CF.rolling(120).apply(lambda x: x.autocorr(1), raw=False),\n",
                "    'var': CF.rolling(120).var(),\n",
                "    'skew': CF.rolling(120).skew()\n",
                "})\n",
                "\n",
                "print(f'Base signals computed')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 2: ENHANCED FACTOR MODEL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 2A: Combine all signals ===\n",
                "print('=== Building Enhanced Signal Matrix ===')\n",
                "\n",
                "# Align all signals\n",
                "common_idx = CF.dropna().index\n",
                "for sig in [SYNC, EWS['acf1'], EWS['var']]:\n",
                "    common_idx = common_idx.intersection(sig.dropna().index)\n",
                "for col in derived.columns:\n",
                "    common_idx = common_idx.intersection(derived[col].dropna().index)\n",
                "\n",
                "# Build signal matrix\n",
                "signals = pd.DataFrame({'cf': CF, 'sync': SYNC, 'acf1': EWS['acf1'], \n",
                "                        'var': EWS['var'], 'skew': EWS['skew'].abs()}).loc[common_idx].dropna()\n",
                "\n",
                "# Add derived if available\n",
                "for col in derived.columns:\n",
                "    if col in derived.columns:\n",
                "        signals[col] = derived[col].loc[signals.index]\n",
                "\n",
                "signals = signals.dropna()\n",
                "print(f'Enhanced signal matrix: {signals.shape}')\n",
                "print(f'Signals: {list(signals.columns)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 2B: Factor Analysis with more components ===\n",
                "print('\\n=== Enhanced Factor Analysis ===')\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X = scaler.fit_transform(signals)\n",
                "\n",
                "# PCA to determine optimal components\n",
                "pca = PCA()\n",
                "pca.fit(X)\n",
                "cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
                "\n",
                "# Find components for 70% variance\n",
                "n_components = np.argmax(cum_var >= 0.70) + 1\n",
                "print(f'Components for 70% variance: {n_components}')\n",
                "print(f'Explained variance: {cum_var[:5]}')\n",
                "\n",
                "# Factor Analysis\n",
                "fa = FactorAnalysis(n_components=min(n_components, 3), random_state=42)\n",
                "F_latent = fa.fit_transform(X)\n",
                "\n",
                "# First factor = Fragility\n",
                "F_t = pd.Series(F_latent[:, 0], index=signals.index, name='F_t')\n",
                "\n",
                "# Loadings\n",
                "loadings = pd.DataFrame({\n",
                "    'signal': signals.columns,\n",
                "    'F1_loading': fa.components_[0]\n",
                "})\n",
                "if fa.n_components >= 2:\n",
                "    loadings['F2_loading'] = fa.components_[1]\n",
                "\n",
                "print('\\nFactor Loadings:')\n",
                "print(loadings.sort_values('F1_loading', ascending=False).to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 3: FORMAL STATISTICAL VALIDATION"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 3A: Block Bootstrap ===\n",
                "print('=== Block Bootstrap Validation ===')\n",
                "\n",
                "def block_bootstrap(series, n_bootstrap=500, block_size=60):\n",
                "    \"\"\"Block bootstrap preserving temporal dependence\"\"\"\n",
                "    n = len(series)\n",
                "    n_blocks = n // block_size\n",
                "    \n",
                "    stats = []\n",
                "    for _ in range(n_bootstrap):\n",
                "        # Sample blocks with replacement\n",
                "        block_indices = np.random.choice(n_blocks, n_blocks, replace=True)\n",
                "        boot_series = []\n",
                "        for bi in block_indices:\n",
                "            start = bi * block_size\n",
                "            end = min(start + block_size, n)\n",
                "            boot_series.extend(series.iloc[start:end].values)\n",
                "        \n",
                "        boot_series = pd.Series(boot_series[:n])\n",
                "        stats.append({\n",
                "            'mean': boot_series.mean(),\n",
                "            'std': boot_series.std(),\n",
                "            'acf1': boot_series.autocorr(1),\n",
                "            'q90': boot_series.quantile(0.9)\n",
                "        })\n",
                "    \n",
                "    return pd.DataFrame(stats)\n",
                "\n",
                "boot_stats = block_bootstrap(F_t)\n",
                "\n",
                "print('Block Bootstrap Results (95% CI):')\n",
                "for col in ['mean', 'std', 'acf1', 'q90']:\n",
                "    ci_low, ci_high = np.percentile(boot_stats[col], [2.5, 97.5])\n",
                "    actual = F_t.mean() if col == 'mean' else (F_t.std() if col == 'std' else \n",
                "              (F_t.autocorr(1) if col == 'acf1' else F_t.quantile(0.9)))\n",
                "    print(f'  {col}: {actual:.4f} [{ci_low:.4f}, {ci_high:.4f}]')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 3B: Granger Causality ===\n",
                "print('\\n=== Granger Causality Tests ===')\n",
                "\n",
                "# Test if F_t Granger-causes market stress indicators\n",
                "test_pairs = [\n",
                "    ('F_t', 'VIX'),\n",
                "    ('F_t', 'cf'),\n",
                "    ('sync', 'F_t'),\n",
                "]\n",
                "\n",
                "vix = df_base['VIX'].loc[F_t.index].dropna()\n",
                "common = F_t.index.intersection(vix.index)\n",
                "\n",
                "granger_results = []\n",
                "for cause, effect in test_pairs:\n",
                "    try:\n",
                "        if cause == 'F_t':\n",
                "            x = F_t.loc[common]\n",
                "        elif cause == 'sync':\n",
                "            x = SYNC.loc[common]\n",
                "        else:\n",
                "            x = signals[cause].loc[common]\n",
                "        \n",
                "        if effect == 'VIX':\n",
                "            y = vix.loc[common]\n",
                "        elif effect == 'F_t':\n",
                "            y = F_t.loc[common]\n",
                "        else:\n",
                "            y = signals[effect].loc[common]\n",
                "        \n",
                "        data = pd.concat([y, x], axis=1).dropna()\n",
                "        if len(data) > 100:\n",
                "            result = grangercausalitytests(data, maxlag=5, verbose=False)\n",
                "            # Get p-value for lag 5\n",
                "            p_value = result[5][0]['ssr_ftest'][1]\n",
                "            granger_results.append({\n",
                "                'cause': cause,\n",
                "                'effect': effect,\n",
                "                'p_value': p_value,\n",
                "                'significant': p_value < 0.05\n",
                "            })\n",
                "    except Exception as e:\n",
                "        print(f'  {cause} -> {effect}: Error - {e}')\n",
                "\n",
                "granger_df = pd.DataFrame(granger_results)\n",
                "print('\\nGranger Causality Results:')\n",
                "print(granger_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 3C: PCMCI (if tigramite available) ===\n",
                "print('\\n=== PCMCI Causal Discovery ===')\n",
                "\n",
                "try:\n",
                "    from tigramite import data_processing as pp\n",
                "    from tigramite.pcmci import PCMCI\n",
                "    from tigramite.independence_tests.parcorr import ParCorr\n",
                "    \n",
                "    # Prepare data\n",
                "    pcmci_cols = ['cf', 'sync', 'acf1', 'var']\n",
                "    pcmci_data = signals[pcmci_cols].dropna().values\n",
                "    \n",
                "    if len(pcmci_data) > 200:\n",
                "        # Subsample for speed\n",
                "        pcmci_data = pcmci_data[::5]  # Every 5th observation\n",
                "        \n",
                "        dataframe = pp.DataFrame(pcmci_data, var_names=pcmci_cols)\n",
                "        parcorr = ParCorr(significance='analytic')\n",
                "        pcmci = PCMCI(dataframe=dataframe, cond_ind_test=parcorr, verbosity=0)\n",
                "        \n",
                "        results = pcmci.run_pcmci(tau_max=5, pc_alpha=0.05)\n",
                "        \n",
                "        print('PCMCI Results (significant links):')\n",
                "        for i, var_i in enumerate(pcmci_cols):\n",
                "            for j, var_j in enumerate(pcmci_cols):\n",
                "                for tau in range(1, 6):\n",
                "                    val = results['val_matrix'][i, j, tau]\n",
                "                    pval = results['p_matrix'][i, j, tau]\n",
                "                    if pval < 0.05:\n",
                "                        print(f'  {var_j}(t-{tau}) -> {var_i}(t): val={val:.3f}, p={pval:.3f}')\n",
                "    else:\n",
                "        print('Not enough data for PCMCI')\n",
                "        \n",
                "except ImportError:\n",
                "    print('Tigramite not available - skipping PCMCI')\n",
                "except Exception as e:\n",
                "    print(f'PCMCI error: {e}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 3D: Formal Error Estimation ===\n",
                "print('\\n=== Formal Error Estimation ===')\n",
                "\n",
                "# Crisis prediction accuracy with confidence intervals\n",
                "CRISES = {\n",
                "    'Lehman': pd.Timestamp('2008-09-15'),\n",
                "    'Flash_Crash': pd.Timestamp('2010-05-06'),\n",
                "    'Euro_Crisis': pd.Timestamp('2011-08-05'),\n",
                "    'Taper_Tantrum': pd.Timestamp('2013-05-22'),\n",
                "    'China_Crash': pd.Timestamp('2015-08-24'),\n",
                "    'Brexit': pd.Timestamp('2016-06-24'),\n",
                "    'Volmageddon': pd.Timestamp('2018-02-05'),\n",
                "    'Repo_Crisis': pd.Timestamp('2019-09-17'),\n",
                "    'COVID': pd.Timestamp('2020-03-11'),\n",
                "    'Gilt_Crisis': pd.Timestamp('2022-09-23'),\n",
                "    'SVB': pd.Timestamp('2023-03-10')\n",
                "}\n",
                "\n",
                "def compute_lead_with_error(indicator, crisis_date, threshold_pct=0.8, n_bootstrap=100):\n",
                "    \"\"\"Compute lead time with bootstrap confidence interval\"\"\"\n",
                "    threshold = indicator.quantile(threshold_pct)\n",
                "    pre = indicator[(indicator.index < crisis_date) & \n",
                "                    (indicator.index > crisis_date - pd.Timedelta(days=180))]\n",
                "    crossings = pre[pre > threshold]\n",
                "    \n",
                "    if len(crossings) > 0:\n",
                "        lead = (crisis_date - crossings.index[0]).days\n",
                "        \n",
                "        # Bootstrap CI\n",
                "        boot_leads = []\n",
                "        for _ in range(n_bootstrap):\n",
                "            boot_thresh = indicator.quantile(threshold_pct + np.random.uniform(-0.05, 0.05))\n",
                "            boot_cross = pre[pre > boot_thresh]\n",
                "            if len(boot_cross) > 0:\n",
                "                boot_leads.append((crisis_date - boot_cross.index[0]).days)\n",
                "        \n",
                "        if boot_leads:\n",
                "            ci_low, ci_high = np.percentile(boot_leads, [2.5, 97.5])\n",
                "            return lead, ci_low, ci_high\n",
                "    \n",
                "    return 0, 0, 0\n",
                "\n",
                "# Compute for all crises\n",
                "error_results = []\n",
                "for crisis_name, crisis_date in CRISES.items():\n",
                "    if crisis_date < F_t.index.min() + pd.Timedelta(days=180):\n",
                "        continue\n",
                "    if crisis_date > F_t.index.max():\n",
                "        continue\n",
                "    \n",
                "    lead, ci_low, ci_high = compute_lead_with_error(F_t, crisis_date)\n",
                "    error_results.append({\n",
                "        'crisis': crisis_name,\n",
                "        'lead': lead,\n",
                "        'ci_low': ci_low,\n",
                "        'ci_high': ci_high\n",
                "    })\n",
                "\n",
                "error_df = pd.DataFrame(error_results)\n",
                "print('\\nLead Times with 95% CI:')\n",
                "print(error_df.to_string(index=False))\n",
                "\n",
                "# Average with SE\n",
                "avg_lead = error_df['lead'].mean()\n",
                "se_lead = error_df['lead'].std() / np.sqrt(len(error_df))\n",
                "print(f'\\nAverage Lead: {avg_lead:.1f} ¬± {1.96*se_lead:.1f} days (95% CI)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# PART 4: PUBLICATION-QUALITY RESULTS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 4A: Effect Sizes ===\n",
                "print('=== Effect Size Calculations ===')\n",
                "\n",
                "# Cohen's d for pre-crisis vs normal periods\n",
                "def cohens_d(group1, group2):\n",
                "    n1, n2 = len(group1), len(group2)\n",
                "    var1, var2 = group1.var(), group2.var()\n",
                "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
                "    return (group1.mean() - group2.mean()) / pooled_std\n",
                "\n",
                "# Pre-crisis windows (60 days before each crisis)\n",
                "pre_crisis_values = []\n",
                "for crisis_date in CRISES.values():\n",
                "    if crisis_date > F_t.index.min() and crisis_date < F_t.index.max():\n",
                "        pre = F_t[(F_t.index < crisis_date) & \n",
                "                  (F_t.index > crisis_date - pd.Timedelta(days=60))]\n",
                "        pre_crisis_values.extend(pre.values)\n",
                "\n",
                "pre_crisis = pd.Series(pre_crisis_values)\n",
                "normal = F_t[~F_t.index.isin([d for d in CRISES.values()])]\n",
                "\n",
                "d = cohens_d(pre_crisis, normal)\n",
                "print(f\"Cohen's d (pre-crisis vs normal): {d:.3f}\")\n",
                "print(f\"Interpretation: {'Large' if abs(d) > 0.8 else 'Medium' if abs(d) > 0.5 else 'Small'} effect\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 4B: Publication Figure ===\n",
                "\n",
                "fig, axes = plt.subplots(4, 1, figsize=(12, 14), sharex=True)\n",
                "\n",
                "# Panel A: Latent Fragility\n",
                "axes[0].fill_between(F_t.index, F_t.values, alpha=0.3, color='red')\n",
                "axes[0].plot(F_t.index, F_t.values, 'r-', linewidth=0.5)\n",
                "axes[0].axhline(F_t.quantile(0.9), color='orange', linestyle='--', alpha=0.7)\n",
                "axes[0].set_ylabel('$F_t$ (Latent Fragility)', fontsize=11)\n",
                "axes[0].set_title('A. Systemic Fragility Index', fontsize=12, fontweight='bold')\n",
                "\n",
                "# Panel B: Key Signals\n",
                "ax1b = axes[1]\n",
                "ax1b.plot(signals.index, scaler.fit_transform(signals[['cf']]).flatten(), \n",
                "          'b-', label='Crisis Factor', alpha=0.7)\n",
                "ax1b.plot(signals.index, scaler.fit_transform(signals[['sync']]).flatten(), \n",
                "          'orange', label='Synchronization', alpha=0.7)\n",
                "ax1b.set_ylabel('Standardized Value', fontsize=11)\n",
                "ax1b.set_title('B. Component Signals', fontsize=12, fontweight='bold')\n",
                "ax1b.legend(loc='upper left', fontsize=9)\n",
                "\n",
                "# Panel C: Bootstrap CI\n",
                "rolling_mean = F_t.rolling(60).mean()\n",
                "rolling_std = F_t.rolling(60).std()\n",
                "axes[2].fill_between(rolling_mean.index, \n",
                "                     rolling_mean - 1.96*rolling_std,\n",
                "                     rolling_mean + 1.96*rolling_std,\n",
                "                     alpha=0.3, color='blue')\n",
                "axes[2].plot(rolling_mean.index, rolling_mean.values, 'b-', linewidth=1)\n",
                "axes[2].set_ylabel('$F_t$ (60d rolling)', fontsize=11)\n",
                "axes[2].set_title('C. Rolling Mean with 95% CI', fontsize=12, fontweight='bold')\n",
                "\n",
                "# Panel D: S&P 500\n",
                "sp500 = df_base['USA_index'].loc[F_t.index].dropna()\n",
                "axes[3].plot(sp500.index, sp500.values, 'k-', linewidth=0.5)\n",
                "axes[3].set_ylabel('S&P 500', fontsize=11)\n",
                "axes[3].set_yscale('log')\n",
                "axes[3].set_title('D. Reference: S&P 500 Index', fontsize=12, fontweight='bold')\n",
                "\n",
                "# Mark crises\n",
                "for ax in axes:\n",
                "    for name, date in CRISES.items():\n",
                "        if date > F_t.index.min() and date < F_t.index.max():\n",
                "            ax.axvline(date, color='red', alpha=0.3, linestyle=':')\n",
                "\n",
                "plt.tight_layout()\n",
                "OUTPUT_DIR = '/content/drive/MyDrive/CARIA/research/formal_fragility'\n",
                "plt.savefig(f'{OUTPUT_DIR}/publication_figure.png', dpi=300, bbox_inches='tight')\n",
                "plt.savefig(f'{OUTPUT_DIR}/publication_figure.pdf', bbox_inches='tight')\n",
                "plt.show()\n",
                "print('\\n‚úì Saved publication figure (PNG + PDF)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === 4C: Results Table for Paper ===\n",
                "print('\\n=== Table 1: Model Performance ===')\n",
                "\n",
                "table1 = pd.DataFrame({\n",
                "    'Metric': ['Variance Explained', 'Average Lead Time', 'Cohen\\'s d', \n",
                "               'Crises Detected', 'Granger Causal Links'],\n",
                "    'Value': [f\"{pca.explained_variance_ratio_[0]*100:.1f}%\",\n",
                "              f\"{avg_lead:.0f} ¬± {1.96*se_lead:.0f} days\",\n",
                "              f\"{d:.2f} ({'Large' if abs(d) > 0.8 else 'Medium'})\",\n",
                "              f\"{len(error_df)}/{len(CRISES)}\",\n",
                "              f\"{granger_df['significant'].sum()}/{len(granger_df)}\"]\n",
                "})\n",
                "print(table1.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === FINAL EXPORT ===\n",
                "import json\n",
                "\n",
                "def make_serializable(obj):\n",
                "    if hasattr(obj, 'isoformat'):\n",
                "        return obj.isoformat()\n",
                "    elif hasattr(obj, 'tolist'):\n",
                "        return obj.tolist()\n",
                "    elif isinstance(obj, (np.integer, np.floating)):\n",
                "        return float(obj)\n",
                "    elif isinstance(obj, dict):\n",
                "        return {str(k): make_serializable(v) for k, v in obj.items()}\n",
                "    elif isinstance(obj, (list, tuple)):\n",
                "        return [make_serializable(i) for i in obj]\n",
                "    else:\n",
                "        return obj\n",
                "\n",
                "publication_export = {\n",
                "    'version': 'Great Caria Publication v1.0',\n",
                "    'generated': pd.Timestamp.now().isoformat(),\n",
                "    'methodology': {\n",
                "        'factor_analysis': 'FactorAnalysis with PCA-informed components',\n",
                "        'validation': ['Block Bootstrap', 'Granger Causality', 'PCMCI'],\n",
                "        'signals': list(signals.columns)\n",
                "    },\n",
                "    'results': {\n",
                "        'variance_explained': float(pca.explained_variance_ratio_[0]),\n",
                "        'avg_lead_days': float(avg_lead),\n",
                "        'lead_se': float(se_lead),\n",
                "        'cohens_d': float(d),\n",
                "        'crises_tested': len(error_df),\n",
                "        'granger_significant': int(granger_df['significant'].sum()) if len(granger_df) > 0 else 0\n",
                "    },\n",
                "    'factor_loadings': make_serializable(loadings.to_dict('records')),\n",
                "    'lead_times': make_serializable(error_df.to_dict('records')),\n",
                "    'granger_results': make_serializable(granger_df.to_dict('records')) if len(granger_df) > 0 else [],\n",
                "    'thresholds': {\n",
                "        'warning': float(F_t.quantile(0.8)),\n",
                "        'critical': float(F_t.quantile(0.95))\n",
                "    }\n",
                "}\n",
                "\n",
                "with open(f'{OUTPUT_DIR}/publication_results.json', 'w') as f:\n",
                "    json.dump(publication_export, f, indent=2)\n",
                "\n",
                "print('\\n‚úì Exported: publication_results.json')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === FINAL SUMMARY ===\n",
                "print('\\n' + '='*70)\n",
                "print('GREAT CARIA - PUBLICATION-READY MODEL')\n",
                "print('='*70)\n",
                "\n",
                "print('\\nüìä DATA UNIVERSE:')\n",
                "print(f'  Base signals: {len([\"cf\", \"sync\", \"acf1\", \"var\", \"skew\"])}')\n",
                "print(f'  Expanded signals: {len(derived.columns)}')\n",
                "print(f'  Total features: {signals.shape[1]}')\n",
                "\n",
                "print('\\nüìà MODEL PERFORMANCE:')\n",
                "print(f'  Variance explained: {pca.explained_variance_ratio_[0]*100:.1f}%')\n",
                "print(f'  Average lead time: {avg_lead:.0f} ¬± {1.96*se_lead:.0f} days')\n",
                "print(f\"  Effect size: Cohen's d = {d:.2f}\")\n",
                "\n",
                "print('\\nüî¨ STATISTICAL VALIDATION:')\n",
                "print(f'  Block bootstrap: Complete')\n",
                "print(f'  Granger causality: {granger_df[\"significant\"].sum()}/{len(granger_df)} significant')\n",
                "print(f'  Lead time CI: Computed for {len(error_df)} crises')\n",
                "\n",
                "print('\\nüìÅ SAVED FILES:')\n",
                "print(f'  {OUTPUT_DIR}/')\n",
                "print('    publication_figure.png')\n",
                "print('    publication_figure.pdf')\n",
                "print('    publication_results.json')\n",
                "\n",
                "print('\\n' + '='*70)\n",
                "print('STATUS: Ready for publication and frontend integration')\n",
                "print('='*70)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}