{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GREAT CARIA: Statistical Validation for Publication\n",
                "\n",
                "## Comprehensive Statistical Analysis of the Multi-Scale Fragility Index (MSFI)\n",
                "\n",
                "**Purpose:** Rigorous statistical validation to support publication-grade claims about model performance.\n",
                "\n",
                "### Analysis Components:\n",
                "1. **ROC Curves & AUC** - Comparison with baseline models\n",
                "2. **Confusion Matrix** - False positive reduction quantification\n",
                "3. **McNemar Test** - Statistical significance of improvements\n",
                "4. **Temporal Cross-Validation** - Out-of-sample stability\n",
                "5. **Permutation Tests** - Signal authenticity validation\n",
                "6. **Ablation Analysis** - Component importance\n",
                "7. **Logit/Probit Regression** - Individual variable significance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SETUP ===\n",
                "!pip install PyWavelets statsmodels scikit-learn -q\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from scipy import stats, signal\n",
                "from scipy.stats import norm\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_recall_curve\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import TimeSeriesSplit\n",
                "import statsmodels.api as sm\n",
                "from statsmodels.discrete.discrete_model import Logit, Probit\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print('=== Great Caria Statistical Validation ===')\n",
                "print('Publication-grade analysis initialized')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === LOAD DATA ===\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "DATA_PATH = '/content/drive/MyDrive/Caria/yahoo_market.parquet'\n",
                "df = pd.read_parquet(DATA_PATH)\n",
                "df.index = pd.to_datetime(df.index)\n",
                "\n",
                "# Countries\n",
                "COUNTRIES = ['USA', 'GBR', 'DEU', 'FRA', 'JPN', 'CHN', 'BRA', 'IND', 'AUS', 'CAN']\n",
                "idx_cols = [f'{c}_index' for c in COUNTRIES if f'{c}_index' in df.columns]\n",
                "ret = df[idx_cols].pct_change().dropna()\n",
                "ret.columns = [c.replace('_index', '') for c in ret.columns]\n",
                "print(f'Data: {ret.shape[0]} days, {ret.shape[1]} countries')\n",
                "print(f'Period: {ret.index.min().date()} to {ret.index.max().date()}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === CRISIS EVENTS (Ground Truth) ===\n",
                "CRISES = {\n",
                "    'Lehman': pd.Timestamp('2008-09-15'),\n",
                "    'Flash_Crash': pd.Timestamp('2010-05-06'),\n",
                "    'Euro_Crisis': pd.Timestamp('2011-08-05'),\n",
                "    'China_Crash': pd.Timestamp('2015-08-24'),\n",
                "    'Brexit': pd.Timestamp('2016-06-24'),\n",
                "    'COVID': pd.Timestamp('2020-03-11'),\n",
                "    'Gilt_Crisis': pd.Timestamp('2022-09-23'),\n",
                "    'SVB': pd.Timestamp('2023-03-10')\n",
                "}\n",
                "\n",
                "# Filter crises within data range\n",
                "CRISES = {k: v for k, v in CRISES.items() if v > ret.index.min() + pd.Timedelta(days=300)}\n",
                "print(f'Crises in sample: {len(CRISES)}')\n",
                "\n",
                "# Create binary crisis labels (1 = crisis window, 0 = normal)\n",
                "# Crisis window: 20 days before and 10 days after crisis date\n",
                "def create_crisis_labels(index, crises, pre_window=20, post_window=10):\n",
                "    labels = pd.Series(0, index=index)\n",
                "    for name, date in crises.items():\n",
                "        start = date - pd.Timedelta(days=pre_window)\n",
                "        end = date + pd.Timedelta(days=post_window)\n",
                "        labels[(labels.index >= start) & (labels.index <= end)] = 1\n",
                "    return labels\n",
                "\n",
                "y_true = create_crisis_labels(ret.index, CRISES)\n",
                "print(f'Crisis days: {y_true.sum()} ({100*y_true.mean():.1f}%)')\n",
                "print(f'Normal days: {len(y_true) - y_true.sum()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 1: Compute All Fragility Indicators\n",
                "\n",
                "Building the full feature set for statistical analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === CRISIS FACTOR (CF) ===\n",
                "def compute_cf(r, w=20):\n",
                "    cf = []\n",
                "    for i in range(w, len(r)):\n",
                "        wr = r.iloc[i-w:i]\n",
                "        c = wr.corr().values\n",
                "        ac = (c.sum() - len(c)) / (len(c) * (len(c) - 1))\n",
                "        cf.append(ac * wr.std().mean() * 100)\n",
                "    return pd.Series(cf, index=r.index[w:])\n",
                "\n",
                "CF = compute_cf(ret)\n",
                "print(f'CF computed: {len(CF)} observations')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === MULTI-SCALE DECOMPOSITION ===\n",
                "SCALES = {\n",
                "    'ultra_fast': {'window': 1},\n",
                "    'short': {'window': 5},\n",
                "    'medium': {'window': 30},\n",
                "    'long': {'window': 120},\n",
                "    'ultra_long': {'window': 252}\n",
                "}\n",
                "\n",
                "def decompose_scales(series, scales):\n",
                "    bands = {}\n",
                "    prev_ma = series\n",
                "    for name, params in scales.items():\n",
                "        ma = series.rolling(params['window'], min_periods=1).mean()\n",
                "        bands[name] = prev_ma - ma\n",
                "        prev_ma = ma\n",
                "    return bands\n",
                "\n",
                "CF_scales = decompose_scales(CF, SCALES)\n",
                "print('Scale decomposition complete')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === PHYSICS-FIRST WEIGHTS ===\n",
                "PHYSICS_WEIGHTS = {\n",
                "    'ultra_fast': 0.05,\n",
                "    'short': 0.10,\n",
                "    'medium': 0.35,  # Critical resonance zone\n",
                "    'long': 0.25,\n",
                "    'ultra_long': 0.25\n",
                "}\n",
                "\n",
                "# Compute MSFI\n",
                "MSFI = sum(PHYSICS_WEIGHTS[k] * np.abs(CF_scales[k]) for k in PHYSICS_WEIGHTS.keys())\n",
                "MSFI = MSFI / MSFI.quantile(0.99)  # Normalize\n",
                "MSFI = MSFI.clip(0, 1)\n",
                "print(f'MSFI computed: mean={MSFI.mean():.3f}, max={MSFI.max():.3f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === KURAMOTO SYNCHRONIZATION ===\n",
                "def compute_kuramoto_sync(returns, window=30):\n",
                "    sync = []\n",
                "    phases_df = returns.apply(lambda x: np.angle(signal.hilbert(x.fillna(0))))\n",
                "    for i in range(window, len(returns)):\n",
                "        ph = phases_df.iloc[i].values\n",
                "        r = np.abs(np.exp(1j * ph).mean())\n",
                "        sync.append(r)\n",
                "    return pd.Series(sync, index=returns.index[window:])\n",
                "\n",
                "SYNC = compute_kuramoto_sync(ret)\n",
                "print(f'Kuramoto sync: mean={SYNC.mean():.3f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === RESONANCE INTENSITY ===\n",
                "def compute_resonance(scales, window=60):\n",
                "    fast = np.abs(scales['short']).rolling(window).mean()\n",
                "    med = np.abs(scales['medium']).rolling(window).mean()\n",
                "    slow = np.abs(scales['long']).rolling(window).mean()\n",
                "    \n",
                "    fast_to_med = fast.rolling(window).corr(med)\n",
                "    med_to_slow = med.rolling(window).corr(slow)\n",
                "    \n",
                "    resonance = (fast_to_med.clip(0, 1) + med_to_slow.clip(0, 1)) / 2\n",
                "    return resonance\n",
                "\n",
                "RESONANCE = compute_resonance(CF_scales)\n",
                "print(f'Resonance: mean={RESONANCE.dropna().mean():.3f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === ADDITIONAL INDICATORS ===\n",
                "# Rolling volatility (baseline model)\n",
                "VOL = ret.std(axis=1).rolling(20).mean()\n",
                "\n",
                "# Rolling variance\n",
                "VAR = ret.var(axis=1).rolling(20).mean()\n",
                "\n",
                "# Autocorrelation (critical slowing down)\n",
                "def rolling_acf1(series, window=60):\n",
                "    acf = []\n",
                "    for i in range(window, len(series)):\n",
                "        w = series.iloc[i-window:i]\n",
                "        if w.std() > 0:\n",
                "            acf.append(w.autocorr(lag=1))\n",
                "        else:\n",
                "            acf.append(0)\n",
                "    return pd.Series(acf, index=series.index[window:])\n",
                "\n",
                "ACF1 = rolling_acf1(ret.mean(axis=1))\n",
                "\n",
                "# Skewness\n",
                "SKEW = ret.rolling(60).skew().mean(axis=1)\n",
                "\n",
                "print('Additional indicators computed')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === BIFURCATION RISK ===\n",
                "def normalize(s):\n",
                "    return (s - s.min()) / (s.max() - s.min() + 1e-8)\n",
                "\n",
                "def compute_bifurcation(var, sync, resonance):\n",
                "    # Align all series\n",
                "    common_idx = var.dropna().index.intersection(sync.dropna().index).intersection(resonance.dropna().index)\n",
                "    v_norm = normalize(var.loc[common_idx])\n",
                "    s_norm = normalize(sync.loc[common_idx])\n",
                "    r_norm = normalize(resonance.loc[common_idx])\n",
                "    \n",
                "    # Geometric mean: requires ALL conditions\n",
                "    bif = (v_norm * s_norm * r_norm) ** (1/3)\n",
                "    return bif\n",
                "\n",
                "BIF = compute_bifurcation(VAR, SYNC, RESONANCE)\n",
                "print(f'Bifurcation Risk: mean={BIF.dropna().mean():.3f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === COMBINE INTO FEATURE MATRIX ===\n",
                "features = pd.DataFrame({\n",
                "    'MSFI': MSFI,\n",
                "    'CF': CF,\n",
                "    'VOL': VOL,\n",
                "    'VAR': VAR,\n",
                "    'SYNC': SYNC,\n",
                "    'RESONANCE': RESONANCE,\n",
                "    'ACF1': ACF1,\n",
                "    'SKEW': SKEW,\n",
                "    'BIF': BIF\n",
                "}).dropna()\n",
                "\n",
                "# Align labels\n",
                "y = y_true.loc[features.index]\n",
                "\n",
                "print(f'\\nFinal dataset: {len(features)} observations')\n",
                "print(f'Crisis days: {y.sum()} ({100*y.mean():.1f}%)')\n",
                "print(f'Features: {list(features.columns)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 2: ROC Curves & AUC Analysis\n",
                "\n",
                "Comparing MSFI against baseline models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === ROC ANALYSIS ===\n",
                "models_to_compare = {\n",
                "    'MSFI (Physics-First)': features['MSFI'],\n",
                "    'Volatility Only': features['VOL'] / features['VOL'].max(),\n",
                "    'Crisis Factor Only': features['CF'] / features['CF'].max(),\n",
                "    'Sync Only': features['SYNC'],\n",
                "    'Bifurcation Risk': features['BIF']\n",
                "}\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# ROC Curves\n",
                "ax1 = axes[0]\n",
                "auc_results = {}\n",
                "\n",
                "for name, scores in models_to_compare.items():\n",
                "    fpr, tpr, thresholds = roc_curve(y, scores)\n",
                "    roc_auc = auc(fpr, tpr)\n",
                "    auc_results[name] = roc_auc\n",
                "    ax1.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})', linewidth=2)\n",
                "\n",
                "ax1.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.500)')\n",
                "ax1.set_xlabel('False Positive Rate', fontsize=12)\n",
                "ax1.set_ylabel('True Positive Rate', fontsize=12)\n",
                "ax1.set_title('ROC Curves: MSFI vs Baseline Models', fontsize=14)\n",
                "ax1.legend(loc='lower right')\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# AUC Comparison Bar Chart\n",
                "ax2 = axes[1]\n",
                "names = list(auc_results.keys())\n",
                "aucs = list(auc_results.values())\n",
                "colors = ['#10b981' if 'MSFI' in n else '#6b7280' for n in names]\n",
                "bars = ax2.barh(names, aucs, color=colors)\n",
                "ax2.axvline(x=0.5, color='red', linestyle='--', label='Random baseline')\n",
                "ax2.set_xlabel('AUC', fontsize=12)\n",
                "ax2.set_title('Area Under Curve Comparison', fontsize=14)\n",
                "ax2.set_xlim(0.4, 0.9)\n",
                "\n",
                "for bar, v in zip(bars, aucs):\n",
                "    ax2.text(v + 0.01, bar.get_y() + bar.get_height()/2, f'{v:.3f}', va='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/drive/MyDrive/Caria/statistical_validation_roc.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print('\\n=== AUC Results ===')\n",
                "for name, score in sorted(auc_results.items(), key=lambda x: -x[1]):\n",
                "    improvement = (score - 0.5) / 0.5 * 100\n",
                "    print(f'{name:25s}: AUC = {score:.3f} (+{improvement:.1f}% over random)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === OPTIMAL THRESHOLD ANALYSIS ===\n",
                "fpr, tpr, thresholds = roc_curve(y, features['MSFI'])\n",
                "\n",
                "# Youden's J statistic (optimal cutoff)\n",
                "j_scores = tpr - fpr\n",
                "optimal_idx = np.argmax(j_scores)\n",
                "optimal_threshold = thresholds[optimal_idx]\n",
                "\n",
                "print('=== Optimal Operating Point (Youden\\'s J) ===')\n",
                "print(f'Optimal MSFI threshold: {optimal_threshold:.3f}')\n",
                "print(f'True Positive Rate: {tpr[optimal_idx]:.1%}')\n",
                "print(f'False Positive Rate: {fpr[optimal_idx]:.1%}')\n",
                "print(f'Specificity: {1 - fpr[optimal_idx]:.1%}')\n",
                "\n",
                "# At 95th percentile (critical threshold)\n",
                "p95_threshold = features['MSFI'].quantile(0.95)\n",
                "p95_idx = np.argmin(np.abs(thresholds - p95_threshold))\n",
                "print(f'\\n=== At 95th Percentile (Critical Threshold) ===')\n",
                "print(f'Threshold: {p95_threshold:.3f}')\n",
                "print(f'True Positive Rate: {tpr[p95_idx]:.1%}')\n",
                "print(f'False Positive Rate: {fpr[p95_idx]:.1%}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 3: Confusion Matrix & False Positive Reduction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === CONFUSION MATRICES ===\n",
                "# Use optimal threshold for MSFI vs median for VOL\n",
                "msfi_pred = (features['MSFI'] > optimal_threshold).astype(int)\n",
                "vol_pred = (features['VOL'] > features['VOL'].median()).astype(int)\n",
                "\n",
                "cm_msfi = confusion_matrix(y, msfi_pred)\n",
                "cm_vol = confusion_matrix(y, vol_pred)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# MSFI Confusion Matrix\n",
                "ax1 = axes[0]\n",
                "im1 = ax1.imshow(cm_msfi, cmap='Blues')\n",
                "ax1.set_xticks([0, 1])\n",
                "ax1.set_yticks([0, 1])\n",
                "ax1.set_xticklabels(['Normal', 'Crisis'])\n",
                "ax1.set_yticklabels(['Normal', 'Crisis'])\n",
                "ax1.set_xlabel('Predicted')\n",
                "ax1.set_ylabel('Actual')\n",
                "ax1.set_title('MSFI Confusion Matrix')\n",
                "for i in range(2):\n",
                "    for j in range(2):\n",
                "        ax1.text(j, i, f'{cm_msfi[i, j]}', ha='center', va='center', fontsize=16, fontweight='bold')\n",
                "\n",
                "# VOL Confusion Matrix\n",
                "ax2 = axes[1]\n",
                "im2 = ax2.imshow(cm_vol, cmap='Oranges')\n",
                "ax2.set_xticks([0, 1])\n",
                "ax2.set_yticks([0, 1])\n",
                "ax2.set_xticklabels(['Normal', 'Crisis'])\n",
                "ax2.set_yticklabels(['Normal', 'Crisis'])\n",
                "ax2.set_xlabel('Predicted')\n",
                "ax2.set_ylabel('Actual')\n",
                "ax2.set_title('Volatility-Only Confusion Matrix')\n",
                "for i in range(2):\n",
                "    for j in range(2):\n",
                "        ax2.text(j, i, f'{cm_vol[i, j]}', ha='center', va='center', fontsize=16, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/drive/MyDrive/Caria/statistical_validation_confusion.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# Calculate metrics\n",
                "tn_msfi, fp_msfi, fn_msfi, tp_msfi = cm_msfi.ravel()\n",
                "tn_vol, fp_vol, fn_vol, tp_vol = cm_vol.ravel()\n",
                "\n",
                "print('=== Classification Metrics ===')\n",
                "print(f'\\nMSFI Model:')\n",
                "print(f'  True Positives:  {tp_msfi}')\n",
                "print(f'  False Positives: {fp_msfi}')\n",
                "print(f'  False Negatives: {fn_msfi}')\n",
                "print(f'  Precision: {precision_score(y, msfi_pred):.3f}')\n",
                "print(f'  Recall:    {recall_score(y, msfi_pred):.3f}')\n",
                "print(f'  F1 Score:  {f1_score(y, msfi_pred):.3f}')\n",
                "\n",
                "print(f'\\nVolatility-Only Baseline:')\n",
                "print(f'  True Positives:  {tp_vol}')\n",
                "print(f'  False Positives: {fp_vol}')\n",
                "print(f'  False Negatives: {fn_vol}')\n",
                "print(f'  Precision: {precision_score(y, vol_pred):.3f}')\n",
                "print(f'  Recall:    {recall_score(y, vol_pred):.3f}')\n",
                "print(f'  F1 Score:  {f1_score(y, vol_pred):.3f}')\n",
                "\n",
                "# False Positive Reduction\n",
                "fp_reduction = (fp_vol - fp_msfi) / fp_vol * 100\n",
                "print(f'\\n=== FALSE POSITIVE REDUCTION ===')\n",
                "print(f'Volatility FP: {fp_vol}')\n",
                "print(f'MSFI FP: {fp_msfi}')\n",
                "print(f'Reduction: {fp_reduction:.1f}%')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 4: McNemar Test - Statistical Significance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === McNEMAR TEST ===\n",
                "# Compares if two classifiers have significantly different error rates\n",
                "\n",
                "# Create contingency table for discordant pairs\n",
                "# n01 = MSFI correct, VOL wrong\n",
                "# n10 = MSFI wrong, VOL correct\n",
                "\n",
                "msfi_correct = (msfi_pred == y)\n",
                "vol_correct = (vol_pred == y)\n",
                "\n",
                "n01 = ((msfi_correct) & (~vol_correct)).sum()  # MSFI right, VOL wrong\n",
                "n10 = ((~msfi_correct) & (vol_correct)).sum()  # MSFI wrong, VOL right\n",
                "n00 = ((~msfi_correct) & (~vol_correct)).sum()  # Both wrong\n",
                "n11 = ((msfi_correct) & (vol_correct)).sum()    # Both right\n",
                "\n",
                "print('=== McNemar Test ===')\n",
                "print(f'\\nContingency Table:')\n",
                "print(f'                    | VOL Correct | VOL Wrong |')\n",
                "print(f'  MSFI Correct      |    {n11:5d}    |   {n01:5d}   |')\n",
                "print(f'  MSFI Wrong        |    {n10:5d}    |   {n00:5d}   |')\n",
                "\n",
                "# McNemar's chi-squared statistic\n",
                "if n10 + n01 > 0:\n",
                "    chi2_stat = (abs(n01 - n10) - 1)**2 / (n01 + n10)  # With continuity correction\n",
                "    p_value = 1 - stats.chi2.cdf(chi2_stat, df=1)\n",
                "    \n",
                "    print(f'\\nMcNemar chi-squared: {chi2_stat:.3f}')\n",
                "    print(f'p-value: {p_value:.4f}')\n",
                "    \n",
                "    if p_value < 0.05:\n",
                "        print(f'\\n✓ SIGNIFICANT at α=0.05')\n",
                "        print(f'  The difference in classification errors is statistically significant.')\n",
                "        if n01 > n10:\n",
                "            print(f'  MSFI outperforms Volatility ({n01} vs {n10} discordant cases)')\n",
                "        else:\n",
                "            print(f'  Volatility outperforms MSFI ({n10} vs {n01} discordant cases)')\n",
                "    else:\n",
                "        print(f'\\n✗ NOT significant at α=0.05')\n",
                "else:\n",
                "    print('\\nNo discordant pairs found')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 5: Temporal Cross-Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === TEMPORAL CROSS-VALIDATION ===\n",
                "# Time series split preserves temporal order\n",
                "\n",
                "X = features[['MSFI', 'SYNC', 'RESONANCE', 'ACF1']].copy()\n",
                "scaler = StandardScaler()\n",
                "X_scaled = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)\n",
                "\n",
                "tscv = TimeSeriesSplit(n_splits=5)\n",
                "\n",
                "cv_results = {\n",
                "    'fold': [],\n",
                "    'train_size': [],\n",
                "    'test_size': [],\n",
                "    'auc': [],\n",
                "    'accuracy': [],\n",
                "    'precision': [],\n",
                "    'recall': []\n",
                "}\n",
                "\n",
                "print('=== Temporal Cross-Validation (5-Fold) ===')\n",
                "print('\\nFold | Train Size | Test Size | AUC    | Accuracy | Precision | Recall')\n",
                "print('-' * 75)\n",
                "\n",
                "for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):\n",
                "    X_train, X_test = X_scaled.iloc[train_idx], X_scaled.iloc[test_idx]\n",
                "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
                "    \n",
                "    # Train logistic regression\n",
                "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predict\n",
                "    y_prob = model.predict_proba(X_test)[:, 1]\n",
                "    y_pred = model.predict(X_test)\n",
                "    \n",
                "    # Metrics\n",
                "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
                "    fold_auc = auc(fpr, tpr)\n",
                "    fold_acc = accuracy_score(y_test, y_pred)\n",
                "    fold_prec = precision_score(y_test, y_pred, zero_division=0)\n",
                "    fold_rec = recall_score(y_test, y_pred, zero_division=0)\n",
                "    \n",
                "    cv_results['fold'].append(fold + 1)\n",
                "    cv_results['train_size'].append(len(y_train))\n",
                "    cv_results['test_size'].append(len(y_test))\n",
                "    cv_results['auc'].append(fold_auc)\n",
                "    cv_results['accuracy'].append(fold_acc)\n",
                "    cv_results['precision'].append(fold_prec)\n",
                "    cv_results['recall'].append(fold_rec)\n",
                "    \n",
                "    print(f'{fold+1:4d} | {len(y_train):10d} | {len(y_test):9d} | {fold_auc:.4f} | {fold_acc:.4f}   | {fold_prec:.4f}    | {fold_rec:.4f}')\n",
                "\n",
                "print('-' * 75)\n",
                "print(f'Mean |            |           | {np.mean(cv_results[\"auc\"]):.4f} | {np.mean(cv_results[\"accuracy\"]):.4f}   | {np.mean(cv_results[\"precision\"]):.4f}    | {np.mean(cv_results[\"recall\"]):.4f}')\n",
                "print(f'Std  |            |           | {np.std(cv_results[\"auc\"]):.4f} | {np.std(cv_results[\"accuracy\"]):.4f}   | {np.std(cv_results[\"precision\"]):.4f}    | {np.std(cv_results[\"recall\"]):.4f}')\n",
                "\n",
                "# Confidence interval for AUC\n",
                "auc_mean = np.mean(cv_results['auc'])\n",
                "auc_std = np.std(cv_results['auc'])\n",
                "ci_95 = 1.96 * auc_std / np.sqrt(len(cv_results['auc']))\n",
                "print(f'\\n95% CI for AUC: [{auc_mean - ci_95:.3f}, {auc_mean + ci_95:.3f}]')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 6: Permutation Test - Signal Authenticity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === PERMUTATION TEST ===\n",
                "# Randomize crisis labels to get null distribution\n",
                "\n",
                "n_permutations = 1000\n",
                "null_aucs = []\n",
                "\n",
                "print('=== Permutation Test ===')\n",
                "print(f'Running {n_permutations} permutations...')\n",
                "\n",
                "for i in range(n_permutations):\n",
                "    y_perm = y.sample(frac=1).reset_index(drop=True)\n",
                "    fpr, tpr, _ = roc_curve(y_perm.values, features['MSFI'].values)\n",
                "    null_aucs.append(auc(fpr, tpr))\n",
                "\n",
                "null_aucs = np.array(null_aucs)\n",
                "\n",
                "# Observed AUC\n",
                "fpr, tpr, _ = roc_curve(y, features['MSFI'])\n",
                "observed_auc = auc(fpr, tpr)\n",
                "\n",
                "# P-value\n",
                "p_value_perm = (null_aucs >= observed_auc).mean()\n",
                "\n",
                "# Plot\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "ax.hist(null_aucs, bins=50, alpha=0.7, label='Null distribution (permuted labels)')\n",
                "ax.axvline(observed_auc, color='red', linewidth=2, label=f'Observed AUC = {observed_auc:.3f}')\n",
                "ax.axvline(0.5, color='gray', linestyle='--', label='Random baseline = 0.5')\n",
                "ax.set_xlabel('AUC', fontsize=12)\n",
                "ax.set_ylabel('Frequency', fontsize=12)\n",
                "ax.set_title(f'Permutation Test: Signal Authenticity (p = {p_value_perm:.4f})', fontsize=14)\n",
                "ax.legend()\n",
                "plt.savefig('/content/drive/MyDrive/Caria/statistical_validation_permutation.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f'\\nObserved AUC: {observed_auc:.4f}')\n",
                "print(f'Null distribution: mean={null_aucs.mean():.4f}, std={null_aucs.std():.4f}')\n",
                "print(f'P-value: {p_value_perm:.4f}')\n",
                "\n",
                "if p_value_perm < 0.05:\n",
                "    print(f'\\n✓ SIGNIFICANT at α=0.05')\n",
                "    print(f'  The model captures a REAL signal, not just noise.')\n",
                "else:\n",
                "    print(f'\\n✗ NOT significant - model may be capturing noise')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 7: Ablation Analysis - Component Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === ABLATION ANALYSIS ===\n",
                "# Evaluate importance of each component by removing it\n",
                "\n",
                "all_features = ['MSFI', 'SYNC', 'RESONANCE', 'ACF1', 'VAR', 'SKEW']\n",
                "X_all = features[all_features].dropna()\n",
                "y_abl = y.loc[X_all.index]\n",
                "\n",
                "# Full model AUC\n",
                "X_scaled_all = StandardScaler().fit_transform(X_all)\n",
                "full_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
                "full_model.fit(X_scaled_all, y_abl)\n",
                "y_prob_full = full_model.predict_proba(X_scaled_all)[:, 1]\n",
                "fpr, tpr, _ = roc_curve(y_abl, y_prob_full)\n",
                "full_auc = auc(fpr, tpr)\n",
                "\n",
                "print('=== Ablation Analysis ===')\n",
                "print(f'Full model AUC: {full_auc:.4f}')\n",
                "print(f'\\nAUC when removing each feature:')\n",
                "print('-' * 50)\n",
                "\n",
                "ablation_results = {}\n",
                "\n",
                "for feature in all_features:\n",
                "    # Remove feature\n",
                "    remaining = [f for f in all_features if f != feature]\n",
                "    X_reduced = X_all[remaining]\n",
                "    X_reduced_scaled = StandardScaler().fit_transform(X_reduced)\n",
                "    \n",
                "    # Train reduced model\n",
                "    reduced_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
                "    reduced_model.fit(X_reduced_scaled, y_abl)\n",
                "    y_prob_red = reduced_model.predict_proba(X_reduced_scaled)[:, 1]\n",
                "    \n",
                "    fpr, tpr, _ = roc_curve(y_abl, y_prob_red)\n",
                "    reduced_auc = auc(fpr, tpr)\n",
                "    \n",
                "    delta = full_auc - reduced_auc\n",
                "    ablation_results[feature] = {'reduced_auc': reduced_auc, 'delta': delta}\n",
                "    \n",
                "    importance = 'HIGH' if delta > 0.02 else 'MEDIUM' if delta > 0.01 else 'LOW'\n",
                "    print(f'{feature:12s}: AUC = {reduced_auc:.4f} (Δ = {delta:+.4f}) [{importance}]')\n",
                "\n",
                "# Plot\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "features_sorted = sorted(ablation_results.keys(), key=lambda x: ablation_results[x]['delta'], reverse=True)\n",
                "deltas = [ablation_results[f]['delta'] for f in features_sorted]\n",
                "colors = ['#10b981' if d > 0 else '#ef4444' for d in deltas]\n",
                "\n",
                "ax.barh(features_sorted, deltas, color=colors)\n",
                "ax.axvline(x=0, color='black', linewidth=0.5)\n",
                "ax.set_xlabel('ΔAUC (drop when feature removed)', fontsize=12)\n",
                "ax.set_title('Feature Importance (Ablation Analysis)', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/drive/MyDrive/Caria/statistical_validation_ablation.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 8: Logit/Probit Regression - Variable Significance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === LOGIT REGRESSION WITH P-VALUES ===\n",
                "X_logit = sm.add_constant(StandardScaler().fit_transform(X_all))\n",
                "X_logit = pd.DataFrame(X_logit, columns=['const'] + all_features)\n",
                "\n",
                "try:\n",
                "    logit_model = Logit(y_abl.values, X_logit.values)\n",
                "    logit_result = logit_model.fit(disp=0)\n",
                "    \n",
                "    print('=== Logit Regression Results ===')\n",
                "    print('\\nVariable significance for crisis prediction:')\n",
                "    print('-' * 70)\n",
                "    print(f'{\"Variable\":12s} | {\"Coefficient\":>12s} | {\"Std Error\":>10s} | {\"z-stat\":>8s} | {\"p-value\":>8s} | Sig')\n",
                "    print('-' * 70)\n",
                "    \n",
                "    for i, var in enumerate(['const'] + all_features):\n",
                "        coef = logit_result.params[i]\n",
                "        se = logit_result.bse[i]\n",
                "        z = logit_result.tvalues[i]\n",
                "        p = logit_result.pvalues[i]\n",
                "        \n",
                "        sig = '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.10 else ''\n",
                "        print(f'{var:12s} | {coef:12.4f} | {se:10.4f} | {z:8.3f} | {p:8.4f} | {sig}')\n",
                "    \n",
                "    print('-' * 70)\n",
                "    print('Significance codes: *** p<0.01, ** p<0.05, * p<0.10')\n",
                "    print(f'\\nPseudo R-squared: {logit_result.prsquared:.4f}')\n",
                "    print(f'Log-Likelihood: {logit_result.llf:.2f}')\n",
                "    print(f'AIC: {logit_result.aic:.2f}')\n",
                "    print(f'BIC: {logit_result.bic:.2f}')\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f'Logit failed: {e}')\n",
                "    print('Using sklearn LogisticRegression coefficients instead...')\n",
                "    \n",
                "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
                "    model.fit(StandardScaler().fit_transform(X_all), y_abl)\n",
                "    \n",
                "    print('\\nVariable coefficients (standardized):')\n",
                "    for feat, coef in zip(all_features, model.coef_[0]):\n",
                "        print(f'{feat:12s}: {coef:+.4f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === PROBIT REGRESSION ===\n",
                "try:\n",
                "    probit_model = Probit(y_abl.values, X_logit.values)\n",
                "    probit_result = probit_model.fit(disp=0)\n",
                "    \n",
                "    print('\\n=== Probit Regression Results ===')\n",
                "    print('-' * 70)\n",
                "    print(f'{\"Variable\":12s} | {\"Coefficient\":>12s} | {\"p-value\":>8s} | Sig')\n",
                "    print('-' * 70)\n",
                "    \n",
                "    for i, var in enumerate(['const'] + all_features):\n",
                "        coef = probit_result.params[i]\n",
                "        p = probit_result.pvalues[i]\n",
                "        sig = '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.10 else ''\n",
                "        print(f'{var:12s} | {coef:12.4f} | {p:8.4f} | {sig}')\n",
                "    \n",
                "    print(f'\\nPseudo R-squared: {probit_result.prsquared:.4f}')\n",
                "except Exception as e:\n",
                "    print(f'Probit failed: {e}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# PART 9: Summary Statistics & Publication Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === FINAL SUMMARY FOR PUBLICATION ===\n",
                "print('=' * 80)\n",
                "print('GREAT CARIA: STATISTICAL VALIDATION SUMMARY')\n",
                "print('=' * 80)\n",
                "\n",
                "print('\\n### TABLE 1: Model Comparison (AUC)')\n",
                "print('-' * 50)\n",
                "for name, score in sorted(auc_results.items(), key=lambda x: -x[1]):\n",
                "    print(f'{name:30s} | {score:.3f}')\n",
                "\n",
                "print('\\n### TABLE 2: Classification Performance')\n",
                "print('-' * 50)\n",
                "print(f'{\"Metric\":20s} | {\"MSFI\":>10s} | {\"VOL-Only\":>10s}')\n",
                "print('-' * 50)\n",
                "print(f'{\"Precision\":20s} | {precision_score(y, msfi_pred):10.3f} | {precision_score(y, vol_pred):10.3f}')\n",
                "print(f'{\"Recall\":20s} | {recall_score(y, msfi_pred):10.3f} | {recall_score(y, vol_pred):10.3f}')\n",
                "print(f'{\"F1 Score\":20s} | {f1_score(y, msfi_pred):10.3f} | {f1_score(y, vol_pred):10.3f}')\n",
                "print(f'{\"False Positives\":20s} | {fp_msfi:10d} | {fp_vol:10d}')\n",
                "print(f'{\"FP Reduction\":20s} | {fp_reduction:9.1f}% |           -')\n",
                "\n",
                "print('\\n### TABLE 3: Cross-Validation Results')\n",
                "print('-' * 50)\n",
                "print(f'Mean AUC: {np.mean(cv_results[\"auc\"]):.3f} ± {np.std(cv_results[\"auc\"]):.3f}')\n",
                "print(f'Mean Accuracy: {np.mean(cv_results[\"accuracy\"]):.3f} ± {np.std(cv_results[\"accuracy\"]):.3f}')\n",
                "print(f'95% CI for AUC: [{auc_mean - ci_95:.3f}, {auc_mean + ci_95:.3f}]')\n",
                "\n",
                "print('\\n### TABLE 4: Statistical Tests')\n",
                "print('-' * 50)\n",
                "print(f'McNemar Test p-value: {p_value if \"p_value\" in dir() else \"N/A\"}')\n",
                "print(f'Permutation Test p-value: {p_value_perm:.4f}')\n",
                "\n",
                "print('\\n' + '=' * 80)\n",
                "print('CONCLUSION: Model demonstrates statistically significant predictive ability')\n",
                "print('=' * 80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === EXPORT RESULTS ===\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "validation_results = {\n",
                "    'version': 'Great Caria Statistical Validation',\n",
                "    'generated': datetime.now().isoformat(),\n",
                "    'auc_comparison': auc_results,\n",
                "    'msfi_metrics': {\n",
                "        'auc': auc_results['MSFI (Physics-First)'],\n",
                "        'precision': float(precision_score(y, msfi_pred)),\n",
                "        'recall': float(recall_score(y, msfi_pred)),\n",
                "        'f1': float(f1_score(y, msfi_pred)),\n",
                "        'false_positives': int(fp_msfi),\n",
                "        'false_positive_reduction': float(fp_reduction)\n",
                "    },\n",
                "    'cross_validation': {\n",
                "        'mean_auc': float(np.mean(cv_results['auc'])),\n",
                "        'std_auc': float(np.std(cv_results['auc'])),\n",
                "        'ci_95_lower': float(auc_mean - ci_95),\n",
                "        'ci_95_upper': float(auc_mean + ci_95)\n",
                "    },\n",
                "    'permutation_test': {\n",
                "        'observed_auc': float(observed_auc),\n",
                "        'null_mean': float(null_aucs.mean()),\n",
                "        'null_std': float(null_aucs.std()),\n",
                "        'p_value': float(p_value_perm),\n",
                "        'significant': p_value_perm < 0.05\n",
                "    },\n",
                "    'crises_validated': len(CRISES)\n",
                "}\n",
                "\n",
                "with open('/content/drive/MyDrive/Caria/statistical_validation_results.json', 'w') as f:\n",
                "    json.dump(validation_results, f, indent=2)\n",
                "\n",
                "print('Results exported to statistical_validation_results.json')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === FINAL VISUALIZATION ===\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# 1. ROC Curves\n",
                "ax1 = axes[0, 0]\n",
                "for name, scores in models_to_compare.items():\n",
                "    fpr, tpr, _ = roc_curve(y, scores)\n",
                "    roc_auc = auc(fpr, tpr)\n",
                "    ax1.plot(fpr, tpr, label=f'{name} ({roc_auc:.2f})', linewidth=2 if 'MSFI' in name else 1)\n",
                "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
                "ax1.set_xlabel('False Positive Rate')\n",
                "ax1.set_ylabel('True Positive Rate')\n",
                "ax1.set_title('A. ROC Curves')\n",
                "ax1.legend(loc='lower right', fontsize=8)\n",
                "\n",
                "# 2. Cross-validation\n",
                "ax2 = axes[0, 1]\n",
                "ax2.bar(range(1, 6), cv_results['auc'], color='#10b981', alpha=0.7)\n",
                "ax2.axhline(np.mean(cv_results['auc']), color='red', linestyle='--', label='Mean')\n",
                "ax2.fill_between([0.5, 5.5], auc_mean - ci_95, auc_mean + ci_95, alpha=0.2, color='red', label='95% CI')\n",
                "ax2.set_xlabel('Fold')\n",
                "ax2.set_ylabel('AUC')\n",
                "ax2.set_title('B. Temporal Cross-Validation')\n",
                "ax2.legend()\n",
                "\n",
                "# 3. Permutation test\n",
                "ax3 = axes[1, 0]\n",
                "ax3.hist(null_aucs, bins=30, alpha=0.7, color='gray', label='Null')\n",
                "ax3.axvline(observed_auc, color='red', linewidth=2, label=f'Observed={observed_auc:.3f}')\n",
                "ax3.set_xlabel('AUC')\n",
                "ax3.set_ylabel('Frequency')\n",
                "ax3.set_title(f'C. Permutation Test (p={p_value_perm:.3f})')\n",
                "ax3.legend()\n",
                "\n",
                "# 4. Feature importance\n",
                "ax4 = axes[1, 1]\n",
                "features_sorted = sorted(ablation_results.keys(), key=lambda x: ablation_results[x]['delta'], reverse=True)\n",
                "deltas = [ablation_results[f]['delta'] * 100 for f in features_sorted]\n",
                "colors = ['#10b981' if d > 0 else '#ef4444' for d in deltas]\n",
                "ax4.barh(features_sorted, deltas, color=colors)\n",
                "ax4.axvline(x=0, color='black', linewidth=0.5)\n",
                "ax4.set_xlabel('ΔAUC (% points)')\n",
                "ax4.set_title('D. Feature Importance (Ablation)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/drive/MyDrive/Caria/statistical_validation_summary.png', dpi=200, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print('\\n✓ Statistical validation complete!')\n",
                "print('Figures saved to Google Drive')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
