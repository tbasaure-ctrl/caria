{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "7ebca588", "cell_type": "markdown", "source": "# GREAT CARIA v3.0 \u2013 Improved Robust Implementation\n\nEsta versi\u00f3n incorpora todas las mejoras sugeridas para fortalecer la validaci\u00f3n y aumentar la se\u00f1al predictiva del modelo.  Incluye:\n\n- Uso de un objetivo de cambio de direcci\u00f3n del Factor de Crisis (CF) a un horizonte configurable (por defecto 5 d\u00edas).\n- Generaci\u00f3n de nuevas variables que reflejan **momentum**, **volatilidad realizada** y **factores macro** (VIX, DXY, Oil, Gold).\n- Expansi\u00f3n a m\u00faltiples pa\u00edses (pa\u00edses seleccionados por disponibilidad de datos en el parquet).\n- Divisi\u00f3n purgada y con embargo para evitar fugas de informaci\u00f3n.\n- Modelo base de **Regresi\u00f3n Log\u00edstica** con ponderaci\u00f3n de clases y evaluaci\u00f3n mediante **Accuracy** y **AUC**.\n- Prueba de barajado (shuffle) para descartar se\u00f1ales espurias.\n- Validaci\u00f3n out\u2011of\u2011sample (rolling OOS) con medidas de desempe\u00f1o por fold.\n- Ejemplo de modelo **Random Forest** para comparar con la regresi\u00f3n log\u00edstica.\n\n***Nota***: para ejecutar este cuaderno necesitas tener acceso a los datos originales (archivos parquet) y modificar la ruta `MARKET_PATH` seg\u00fan tu entorno.\n", "metadata": {}}, {"id": "818a8bb1", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\n# Dispositivo\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')", "outputs": []}, {"id": "b7e1be2f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === 1. CARGA DE DATOS ===\n# Ajusta la ruta al archivo parquet seg\u00fan tu entorno\nMARKET_PATH = '/content/drive/MyDrive/CARIA/data/raw/yahoo_market.parquet'\n\n# Lectura del dataset de mercado diario\ndf_daily = pd.read_parquet(MARKET_PATH)\n\n# Lista de pa\u00edses a utilizar (puedes ampliarla si hay datos disponibles)\nCOUNTRIES = ['USA', 'CHN', 'JPN', 'DEU', 'GBR', 'FRA', 'BRA', 'MEX', 'KOR', 'AUS']\n\nprint(f'Datos diarios cargados: {df_daily.shape}')", "outputs": []}, {"id": "9cc3ce39", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === 2. C\u00c1LCULO DEL FACTOR DE CRISIS (CF) ===\n# Utilizamos retornos diarios de los \u00edndices de cada pa\u00eds\nindex_cols = [f'{c}_index' for c in COUNTRIES if f'{c}_index' in df_daily.columns]\nreturns = df_daily[index_cols].pct_change().dropna()\nreturns.columns = [c.replace('_index','') for c in returns.columns]\n\ndef compute_cf(returns, window=20):\n    cf_series = []\n    for i in range(window, len(returns)):\n        w = returns.iloc[i-window:i]\n        corr = w.corr().values\n        # correlaci\u00f3n media entre todos los pares\n        avg_corr = (corr.sum() - len(corr)) / (len(corr) * (len(corr) - 1))\n        avg_vol = w.std().mean()\n        cf_series.append((avg_corr + avg_vol) / 2)\n    cf_index = returns.index[window:]\n    return pd.Series(cf_series, index=cf_index)\n\nCF = compute_cf(returns)\nprint(f'CF calculado para {len(CF)} observaciones')", "outputs": []}, {"id": "c5a07738", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === 3. OBJETIVO: Cambio de direcci\u00f3n del CF ===\nHORIZON = 5\ncf_future = CF.shift(-HORIZON)\ncf_change = (cf_future > CF).astype(int)  # 1 = CF aumentar\u00e1\ncf_change = cf_change.dropna()\nprint(f'Observaciones de CF para el objetivo: {len(cf_change)}')", "outputs": []}, {"id": "e5db25d3", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === 4. INGENIER\u00cdA DE CARACTER\u00cdSTICAS ===\n# Inicializamos dataframe de caracter\u00edsticas\nfeatures = pd.DataFrame(index=cf_change.index)\n\n# Variables relacionadas con CF\nfeatures['cf_now'] = CF.loc[cf_change.index]\nfeatures['cf_ma5'] = CF.rolling(5).mean().loc[cf_change.index]\nfeatures['cf_ma20'] = CF.rolling(20).mean().loc[cf_change.index]\n\n# Factores macro globales (si est\u00e1n disponibles en df_daily)\nfor col in ['VIX', 'DXY', 'Oil', 'Gold']:\n    if col in df_daily.columns:\n        features[col.lower()] = df_daily[col].loc[cf_change.index]\n\n# Variables de momentum y volatilidad por pa\u00eds\nfor c in COUNTRIES:\n    if c in returns.columns:\n        # Media m\u00f3vil de 5 d\u00edas\n        features[f'ret_{c.lower()}_ma5'] = returns[c].rolling(5).mean().loc[cf_change.index]\n        # Volatilidad realizada en 20 d\u00edas\n        features[f'vol_{c.lower()}_std20'] = returns[c].rolling(20).std().loc[cf_change.index]\n\n# Eliminamos filas con valores faltantes\nfeatures = features.dropna()\n# Reajustamos el objetivo a las fechas disponibles\ntarget = cf_change.loc[features.index]\n\nprint(f'N\u00famero final de muestras: {len(features)}')", "outputs": []}, {"id": "2da9b3c6", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === 5. DIVISI\u00d3N PURGADA Y EMBARGO ===\nPURGE = 20\nEMBARGO = 10\ntrain_end = int(len(features) * 0.7)\ntest_start = train_end + PURGE + EMBARGO\n\nX_train = features.iloc[:train_end].values\ny_train = target.iloc[:train_end].values\nX_test = features.iloc[test_start:].values\ny_test = target.iloc[test_start:].values\n\n# Normalizaci\u00f3n basada en el conjunto de entrenamiento\nmu, sigma = X_train.mean(axis=0), X_train.std(axis=0) + 1e-8\nX_train_norm = (X_train - mu) / sigma\nX_test_norm = (X_test - mu) / sigma\n\nprint(f'Tama\u00f1o entrenamiento: {len(X_train_norm)}, test: {len(X_test_norm)} (purge={PURGE}, embargo={EMBARGO})')", "outputs": []}, {"id": "39834740", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === 6. REGRESI\u00d3N LOG\u00cdSTICA (con ponderaci\u00f3n de clases) ===\n# Se usa class_weight='balanced' para manejar posibles desequilibrios de clases\nlr = LogisticRegression(max_iter=1000, C=0.1, class_weight='balanced')\nlr.fit(X_train_norm, y_train)\n\n# Predicciones de clase y probabilidades\ny_pred_lr = lr.predict(X_test_norm)\ny_pred_proba_lr = lr.predict_proba(X_test_norm)[:, 1]\n\nacc_lr = accuracy_score(y_test, y_pred_lr)\nauc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n\nprint(f'Regresi\u00f3n log\u00edstica \u2013 Accuracy (purged): {acc_lr:.3f}, AUC: {auc_lr:.3f}')", "outputs": []}, {"id": "e84659e2", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === 7. PRUEBA DE BARAJADO (shuffle) ===\ny_train_shuffled = np.random.permutation(y_train)\nlr_shuffled = LogisticRegression(max_iter=1000, C=0.1, class_weight='balanced')\nlr_shuffled.fit(X_train_norm, y_train_shuffled)\n\ny_pred_shuffle = lr_shuffled.predict(X_test_norm)\ny_pred_proba_shuffle = lr_shuffled.predict_proba(X_test_norm)[:, 1]\n\nacc_shuffle = accuracy_score(y_test, y_pred_shuffle)\nauc_shuffle = roc_auc_score(y_test, y_pred_proba_shuffle)\n\nprint(f'Barajado \u2013 Accuracy: {acc_shuffle:.3f}, AUC: {auc_shuffle:.3f}')\nprint(f'Lift sobre random (accuracy): {(acc_lr - acc_shuffle):.3f}')", "outputs": []}, {"id": "105dc3ef", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === 8. VALIDACI\u00d3N ROLLING OOS ===\nn_folds = 5\nfold_size = len(features) // (n_folds + 1)\nrolling_accs = []\nrolling_aucs = []\nfor i in range(n_folds):\n    train_end_i = (i + 1) * fold_size\n    test_start_i = train_end_i + PURGE + EMBARGO\n    test_end_i = test_start_i + fold_size\n    if test_end_i > len(features):\n        break\n    X_tr = features.iloc[:train_end_i].values\n    y_tr = target.iloc[:train_end_i].values\n    X_te = features.iloc[test_start_i:test_end_i].values\n    y_te = target.iloc[test_start_i:test_end_i].values\n    mu_i, sigma_i = X_tr.mean(axis=0), X_tr.std(axis=0) + 1e-8\n    X_trn = (X_tr - mu_i) / sigma_i\n    X_ten = (X_te - mu_i) / sigma_i\n    lr_fold = LogisticRegression(max_iter=1000, C=0.1, class_weight='balanced')\n    lr_fold.fit(X_trn, y_tr)\n    y_pred_fold = lr_fold.predict(X_ten)\n    y_proba_fold = lr_fold.predict_proba(X_ten)[:, 1]\n    acc_fold = accuracy_score(y_te, y_pred_fold)\n    auc_fold = roc_auc_score(y_te, y_proba_fold)\n    rolling_accs.append(acc_fold)\n    rolling_aucs.append(auc_fold)\n    print(f'  Fold {i+1}: Acc {acc_fold:.3f}, AUC {auc_fold:.3f}')\n\nprint(f'\nRolling OOS \u2013 media Accuracy: {np.mean(rolling_accs):.3f}, std: {np.std(rolling_accs):.3f}')\nprint(f'Rolling OOS \u2013 media AUC: {np.mean(rolling_aucs):.3f}, std: {np.std(rolling_aucs):.3f}')", "outputs": []}, {"id": "078b1c06", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === 9. RANDOM FOREST COMO MODELO ALTERNATIVO ===\n# Entrenamos un RandomForestClassifier simple para comparar\nrf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42, class_weight='balanced')\nrf.fit(X_train_norm, y_train)\n\ny_pred_rf = rf.predict(X_test_norm)\ny_proba_rf = rf.predict_proba(X_test_norm)[:, 1]\n\nacc_rf = accuracy_score(y_test, y_pred_rf)\nauc_rf = roc_auc_score(y_test, y_proba_rf)\n\nprint(f'Random Forest \u2013 Accuracy: {acc_rf:.3f}, AUC: {auc_rf:.3f}')", "outputs": []}, {"id": "6bd04d91", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# === 10. RESUMEN ===\nprint('\n' + '='*60)\nprint('Resultados de VALIDACI\u00d3N \u2013 GREAT CARIA v3.0')\nprint('='*60)\nprint(f'Regresi\u00f3n log\u00edstica \u2013 Accuracy: {acc_lr:.3f}, AUC: {auc_lr:.3f}')\nprint(f'Modelo barajado \u2013 Accuracy: {acc_shuffle:.3f}, AUC: {auc_shuffle:.3f}')\nprint(f'Lift sobre random (accuracy): {(acc_lr - acc_shuffle):.3f}')\nprint(f'Rolling OOS \u2013 media Acc: {np.mean(rolling_accs):.3f}, media AUC: {np.mean(rolling_aucs):.3f}')\nprint(f'Random Forest \u2013 Accuracy: {acc_rf:.3f}, AUC: {auc_rf:.3f}')\n\n# Veredicto simple bas\u00e1ndonos en accuracy y AUC\nif acc_lr > 0.55 and auc_lr > 0.55 and (acc_lr - acc_shuffle) > 0.03:\n    print('\u2713 PASADO \u2013 Se detecta se\u00f1al estad\u00edsticamente significativa')\nelse:\n    print('\u2717 FALLA \u2013 La se\u00f1al es marginal o no supera el azar con margen suficiente')\nprint('='*60)", "outputs": []}]}