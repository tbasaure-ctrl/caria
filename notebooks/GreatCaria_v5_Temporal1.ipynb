{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_v5"
            },
            "source": [
                "# GREAT CARIA v5: TEMPORAL RELATIVITY\n",
                "\n",
                "## The Theory of Coupled Oscillators\n",
                "\n",
                "In this model, we treat the Global Financial System as a set of **Coupled Oscillators** operating at different **Proper Times** (Clock Speeds).\n",
                "\n",
                "### Key Concepts\n",
                "1.  **Proper Time ($\\tau$)**: Different agents live in different times.\n",
                "    - **HFT**: $\\tau$ = Milliseconds\n",
                "    - **Hedge Funds**: $\\tau$ = Days/Weeks (Medium Band)\n",
                "    - **Pensions**: $\\tau$ = Quarters/Years (Slow Band)\n",
                "2.  **Synchronization ($r$)**: A crisis occurs when these distinct clocks **align**. This is measured by the **Kuramoto Order Parameter**.\n",
                "    $$ r(t) = \\left| \\frac{1}{N} \\sum_{j=1}^{N} e^{i\\phi_j(t)} \\right| $$\n",
                "3.  **Resonance (Medium Band)**: The critical \"fuse\". The Medium Band (10-60d) is where fast noise becomes slow structure. We weight this at **35%**.\n",
                "\n",
                "### Objective\n",
                "Isolate **Structural Fragility** (Synchronization) from **Propagation** (Energy Flow) to predict Phase Transitions (Crises)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup_v5"
            },
            "outputs": [],
            "source": [
                "# === SETUP ===\n",
                "!pip install PyWavelets scikit-learn numpy pandas scipy matplotlib -q\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from scipy import stats, signal\n",
                "from scipy.ndimage import gaussian_filter1d\n",
                "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
                "from sklearn.metrics import roc_curve, auc,  roc_auc_score\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "print('='*80)\n",
                "print('GREAT CARIA v5: TEMPORAL RELATIVITY')\n",
                "print('Checking Clock Synchronization...')\n",
                "print('='*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load_data_v5"
            },
            "outputs": [],
            "source": [
                "# === LOAD DATA ===\n",
                "DATA_PATH = '/content/drive/MyDrive/Caria/yahoo_market.parquet'\n",
                "try:\n",
                "    df = pd.read_parquet(DATA_PATH)\n",
                "except FileNotFoundError:\n",
                "    # Fallback path if folder structure differs\n",
                "    DATA_PATH = '/content/drive/MyDrive/CARIA/data/raw/yahoo_market.parquet'\n",
                "    df = pd.read_parquet(DATA_PATH)\n",
                "\n",
                "df.index = pd.to_datetime(df.index)\n",
                "\n",
                "# Extract Indices\n",
                "all_cols = [c for c in df.columns if '_index' in c]\n",
                "ret = df[all_cols].pct_change().dropna()\n",
                "ret.columns = [c.replace('_index', '') for c in ret.columns]\n",
                "\n",
                "print(f'Data Loaded: {ret.shape[0]} days, {ret.shape[1]} assets')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "scales_config"
            },
            "outputs": [],
            "source": [
                "# === CONFIGURATION: THE CLOCKS ===\n",
                "SCALES = {\n",
                "    'fast':   {'window': 5,   'weight': 0.15, 'desc': 'Traders (Noise)'},\n",
                "    'medium': {'window': 30,  'weight': 0.35, 'desc': 'Hedge Funds (RESONANCE)'}, # CRITICAL FUSE\n",
                "    'slow':   {'window': 120, 'weight': 0.25, 'desc': 'Institutions (Structure)'},\n",
                "    'macro':  {'window': 252, 'weight': 0.25, 'desc': 'Central Banks (Cycle)'}\n",
                "}\n",
                "\n",
                "# Ground Truth for Validation\n",
                "CRISES = {\n",
                "    'Lehman': pd.Timestamp('2008-09-15'),\n",
                "    'Flash_Crash': pd.Timestamp('2010-05-06'),\n",
                "    'Euro_Crisis': pd.Timestamp('2011-08-05'),\n",
                "    'China_Crash': pd.Timestamp('2015-08-24'),\n",
                "    'Brexit': pd.Timestamp('2016-06-24'),\n",
                "    'Volmageddon': pd.Timestamp('2018-02-05'),\n",
                "    'COVID': pd.Timestamp('2020-03-11'),\n",
                "    'Gilt_Crisis': pd.Timestamp('2022-09-23'),\n",
                "    'SVB': pd.Timestamp('2023-03-10')\n",
                "}\n",
                "# Filter relevant crises\n",
                "CRISES = {k: v for k, v in CRISES.items() if v >= ret.index.min() and v <= ret.index.max()}\n",
                "\n",
                "def create_target(index, crises):\n",
                "    target = pd.Series(0, index=index)\n",
                "    for date in crises.values():\n",
                "        # Crisis window: 2 weeks before to 1 week after\n",
                "        target[(target.index >= date - pd.Timedelta(days=14)) & \n",
                "               (target.index <= date + pd.Timedelta(days=7))] = 1\n",
                "    return target\n",
                "\n",
                "y_true = create_target(ret.index, CRISES)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cf_base"
            },
            "source": [
                "---\n",
                "## Part 1: Crisis Factor (The Base Signal)\n",
                "We start with the empirically robust Crisis Factor (Correlation $\\times$ Volatility) to represent the raw stress in the system."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "calc_cf_v5"
            },
            "outputs": [],
            "source": [
                "def compute_cf(r, window=20):\n",
                "    cf = []\n",
                "    for i in range(window, len(r)):\n",
                "        wr = r.iloc[i-window:i]\n",
                "        corr_matrix = wr.corr().values\n",
                "        n = len(corr_matrix)\n",
                "        avg_corr = (corr_matrix.sum() - n) / (n * (n - 1))\n",
                "        avg_vol = wr.std().mean()\n",
                "        cf.append(avg_corr * avg_vol * 100)\n",
                "    return pd.Series(cf, index=r.index[window:])\n",
                "\n",
                "CF = compute_cf(ret)\n",
                "print(f'Base Crisis Factor computed. Mean: {CF.mean():.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "temporal_relativity"
            },
            "source": [
                "---\n",
                "## Part 2: Temporal Relativity (Decomposition & Synchronization)\n",
                "\n",
                "### 2.1 Scale Decomposition\n",
                "Decompose the CF signal into the 4 constituent clocks: Fast, Medium, Slow, Macro.\n",
                "\n",
                "### 2.2 Phase Extraction (Hilbert Transform)\n",
                "Determine the instantaneous phase $\\phi(t)$ of each clock.\n",
                "\n",
                "### 2.3 Kuramoto Synchronization\n",
                "Calculate how aligned these clocks are."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "calc_sync"
            },
            "outputs": [],
            "source": [
                "# 1. DECOMPOSITION\n",
                "bands = {}\n",
                "sorted_scales = sorted(SCALES.items(), key=lambda x: x[1]['window'])\n",
                "\n",
                "for i, (name, config) in enumerate(sorted_scales):\n",
                "    w = config['window']\n",
                "    smooth = CF.rolling(w, min_periods=1).mean()\n",
                "    if i < len(sorted_scales) - 1:\n",
                "        next_w = sorted_scales[i+1][1]['window']\n",
                "        next_smooth = CF.rolling(next_w, min_periods=1).mean()\n",
                "        # Band is difference between this scale and next scale (Bandpass)\n",
                "        bands[name] = smooth - next_smooth\n",
                "    else:\n",
                "        # Last scale is residual trend\n",
                "        bands[name] = smooth\n",
                "\n",
                "bands_df = pd.DataFrame(bands).dropna()\n",
                "\n",
                "# 2. PHASE EXTRACTION (Hilbert)\n",
                "phases = {}\n",
                "for col in bands_df.columns:\n",
                "    series = bands_df[col].values\n",
                "    # Centering (Hilbert requirement)\n",
                "    centered = series - np.mean(series)\n",
                "    analytic = signal.hilbert(centered)\n",
                "    phases[col] = np.angle(analytic)\n",
                "\n",
                "phases_df = pd.DataFrame(phases, index=bands_df.index)\n",
                "\n",
                "# 3. KURAMOTO SYNCHRONIZATION (r)\n",
                "# r = |mean(e^i*phi)| across scales\n",
                "complex_phases = np.exp(1j * phases_df)\n",
                "kuramoto_r = np.abs(complex_phases.mean(axis=1))\n",
                "CLOCK_SYNC = pd.Series(kuramoto_r, index=phases_df.index)\n",
                "\n",
                "print(f'Clock Synchronization (r) computed. Max Sync: {CLOCK_SYNC.max():.3f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "isolation"
            },
            "source": [
                "---\n",
                "## Part 3: Isolating Structure and Flow\n",
                "\n",
                "### 3.1 Structural Fragility\n",
                "High Synchronization = The structure is rigid (all clocks locked).\n",
                "\n",
                "### 3.2 Propagation (Energy Flow)\n",
                "The **Medium Band** (Resonance). If this band has high energy (volatility), it means shocks are successfully transferring from Fast to Slow.\n",
                "\n",
                "$$ MSFI_{v5} = \\sum (w_i \\cdot |Band_i|) \\times (1 + \\text{Sync}) $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "calc_msfi_v5"
            },
            "outputs": [],
            "source": [
                "# === PHYSICS-FIRST AGGREGATION ===\n",
                "\n",
                "# Normalize Bands Energy (Absolute value)\n",
                "bands_energy = bands_df.abs()\n",
                "\n",
                "# Normalize to 0-1 scale for consistent weighting\n",
                "for col in bands_energy.columns:\n",
                "    bands_energy[col] = (bands_energy[col] - bands_energy[col].rolling(252).min()) / \\\n",
                "                        (bands_energy[col].rolling(252).max() - bands_energy[col].rolling(252).min() + 1e-8)\n",
                "bands_energy = bands_energy.fillna(0)\n",
                "\n",
                "# Weighted Sum of Energies\n",
                "weighted_energy = sum(bands_energy[name] * config['weight'] for name, config in SCALES.items())\n",
                "\n",
                "# === THE UNIFIED TEMPORAL FORMULA ===\n",
                "# MSFI = Weighted Energy * (1 + Synchronization Bonus)\n",
                "# If clocks are synced, the energy is amplified.\n",
                "MSFI_v5 = weighted_energy * (1 + CLOCK_SYNC)\n",
                "\n",
                "# Smooth slightly for legibility\n",
                "MSFI_v5 = MSFI_v5.rolling(5).mean()\n",
                "\n",
                "print('MSFI v5 Computed.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "stat_validation_intro"
            },
            "source": [
                "===\n",
                "## Part 5: Rigorous Statistical Validation\n",
                "\n",
                "We perform a deep statistical audit of the model's performance against a baseline (Traditional VIX/Volatility).\n",
                "\n",
                "### Metrics Calculated:\n",
                "1.  **Contingency Tables**: TP, FP, TN, FN.\n",
                "2.  **Performance Metrics**: Precision, Recall, Specificity, F1-Score.\n",
                "3.  **False Positive Reduction**: Quantifying the \"60% reduction\" claim.\n",
                "4.  **ROC & AUC**: With Bootstrapped 95% Confidence Intervals.\n",
                "5.  **Statistical Significance**: McNemar's Test (for classification difference) and DeLong Test proxy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "stat_validation_code"
            },
            "outputs": [],
            "source": [
                "# === 5.1 PREPARE DATA ===\n",
                "# Align data\n",
                "common_idx = y_true.index.intersection(MSFI_v5.dropna().index)\n",
                "y_eval = y_true.loc[common_idx]\n",
                "score_model = MSFI_v5.loc[common_idx].fillna(0)\n",
                "\n",
                "# Create a Benchmark (e.g., Raw Volatility of S&P 500 equivalent)\n",
                "# Assuming first column is a major index (e.g., USA)\n",
                "benchmark_vol = ret.iloc[:, 0].rolling(20).std().loc[common_idx].fillna(0)\n",
                "score_bench = (benchmark_vol - benchmark_vol.min()) / (benchmark_vol.max() - benchmark_vol.min()) # MinMax Scaled\n",
                "\n",
                "# === 5.2 CONTINGENCY TABLES & METRICS ===\n",
                "def get_metrics(y_true, y_prob, threshold=0.8):\n",
                "    y_pred = (y_prob > threshold).astype(int)\n",
                "    \n",
                "    # Confusion Matrix\n",
                "    tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
                "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
                "    tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
                "    fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
                "    \n",
                "    # Metrics\n",
                "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0 # Sensitivity\n",
                "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
                "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
                "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
                "    \n",
                "    return {\n",
                "        'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn,\n",
                "        'Precision': precision, 'Recall': recall, 'Specificity': specificity,\n",
                "        'F1': f1, 'FPR': fpr\n",
                "    }\n",
                "\n",
                "# Optimize Threshold (Maximize F1)\n",
                "best_thresh_model = 0.5\n",
                "best_f1_model = 0\n",
                "for t in np.linspace(0.1, 0.9, 50):\n",
                "    m = get_metrics(y_eval, score_model, t)\n",
                "    if m['F1'] > best_f1_model:\n",
                "        best_f1_model = m['F1']\n",
                "        best_thresh_model = t\n",
                "\n",
                "metrics_model = get_metrics(y_eval, score_model, best_thresh_model)\n",
                "metrics_bench = get_metrics(y_eval, score_bench, best_thresh_model) # Using same thresh for comparison approx\n",
                "\n",
                "print(f\"=== Performance Metrics (Threshold: {best_thresh_model:.2f}) ===\")\n",
                "print(f\"Model     -> FP: {metrics_model['FP']}, TP: {metrics_model['TP']}, Specificity: {metrics_model['Specificity']:.3f}\")\n",
                "print(f\"Benchmark -> FP: {metrics_bench['FP']}, TP: {metrics_bench['TP']}, Specificity: {metrics_bench['Specificity']:.3f}\")\n",
                "\n",
                "# False Positive Reduction\n",
                "fp_reduction = (metrics_bench['FP'] - metrics_model['FP']) / metrics_bench['FP'] if metrics_bench['FP'] > 0 else 0\n",
                "print(f\"\\nFALSE POSITIVE REDUCTION: {fp_reduction:.1%}\")\n",
                "\n",
                "# === 5.3 MCNEMAR'S TEST (Statistical Significance) ===\n",
                "# H0: The two models make errors in the same way.\n",
                "# Contingency Table comparing classify correctness\n",
                "model_correct = ((score_model > best_thresh_model) == y_eval)\n",
                "bench_correct = ((score_bench > best_thresh_model) == y_eval)\n",
                "\n",
                "a = ((model_correct) & (bench_correct)).sum() # Both Correct\n",
                "b = ((model_correct) & (~bench_correct)).sum() # Model Correct, Bench Wrong\n",
                "c = ((~model_correct) & (bench_correct)).sum() # Model Wrong, Bench Correct\n",
                "d = ((~model_correct) & (~bench_correct)).sum() # Both Wrong\n",
                "\n",
                "mcnemar_stat = (abs(b - c) - 1)**2 / (b + c) if (b+c) > 0 else 0\n",
                "p_value_mcnemar = 1 - stats.chi2.cdf(mcnemar_stat, 1)\n",
                "\n",
                "print(f\"\\n=== McNemar's Test ===\")\n",
                "print(f\"Statistic: {mcnemar_stat:.4f}, p-value: {p_value_mcnemar:.6f}\")\n",
                "if p_value_mcnemar < 0.05:\n",
                "    print(\"RESULT: Statistically Significant improvement (p < 0.05)\")\n",
                "else:\n",
                "    print(\"RESULT: No statistical significance detected\")\n",
                "\n",
                "# === 5.4 BOOTSTRAPPED AUC INTERVALS ===\n",
                "n_bootstraps = 1000\n",
                "bootstrapped_scores = []\n",
                "\n",
                "rng = np.random.RandomState(42)\n",
                "for i in range(n_bootstraps):\n",
                "    indices = rng.randint(0, len(y_eval), len(y_eval))\n",
                "    if len(np.unique(y_eval.iloc[indices])) < 2:\n",
                "        continue\n",
                "    score = roc_auc_score(y_eval.iloc[indices], score_model.iloc[indices])\n",
                "    bootstrapped_scores.append(score)\n",
                "\n",
                "sorted_scores = np.array(bootstrapped_scores)\n",
                "sorted_scores.sort()\n",
                "ci_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
                "ci_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
                "\n",
                "fpr_m, tpr_m, _ = roc_curve(y_eval, score_model)\n",
                "roc_auc_m = auc(fpr_m, tpr_m)\n",
                "\n",
                "print(f\"\\n=== AUC with 95% Confidence Interval ===\")\n",
                "print(f\"AUC: {roc_auc_m:.4f} [{ci_lower:.4f} - {ci_upper:.4f}]\")\n",
                "\n",
                "# === 5.5 VISUAL SUMMARY ===\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(fpr_m, tpr_m, label=f'Great Caria v5 (AUC = {roc_auc_m:.2f})', color='cyan', linewidth=2)\n",
                "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
                "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
                "plt.ylabel('True Positive Rate (Recall)')\n",
                "plt.title('ROC Curve Analysis: v5 vs Random')\n",
                "plt.legend(loc='lower right')\n",
                "plt.grid(alpha=0.3)\n",
                "plt.savefig('/content/drive/MyDrive/Caria/great_caria_v5_roc_analysis.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "export_v5"
            },
            "outputs": [],
            "source": [
                "# === EXPORT ===\n",
                "latest_idx = -1\n",
                "export_data = {\n",
                "    \"version\": \"v5.0 (Temporal Relativity)\",\n",
                "    \"generated_at\": datetime.now().isoformat(),\n",
                "    \"msfi\": float(MSFI_v5.iloc[latest_idx]),\n",
                "    \"clock_sync\": float(CLOCK_SYNC.iloc[latest_idx]),\n",
                "    \"resonance\": float(bands_energy['medium'].iloc[latest_idx]),\n",
                "    \"status\": \"WARNING\" if MSFI_v5.iloc[latest_idx] > MSFI_v5.quantile(0.8) else \"STABLE\",\n",
                "    \"auc\": roc_auc_m,\n",
                "    \"history\": {\n",
                "        \"dates\": [d.strftime('%Y-%m-%d') for d in MSFI_v5.index[-100:]],\n",
                "        \"msfi\": MSFI_v5.iloc[-100:].tolist(),\n",
                "        \"sync\": CLOCK_SYNC.iloc[-100:].tolist()\n",
                "    }\n",
                "}\n",
                "\n",
                "with open('/content/drive/MyDrive/Caria/great_caria_v5.json', 'w') as f:\n",
                "    json.dump(export_data, f, indent=2)\n",
                "    \n",
                "print('Exported v5 JSON.')"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "name": "GreatCaria_v5_Temporal.ipynb",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}