{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento Mejorado de Modelos Caria\n",
    "\n",
    "Este notebook entrena los modelos mejorados con:\n",
    "- **Quality Model**: Percentiles por fecha (identifica outliers incluso en mala economía)\n",
    "- **Valuation Model**: DCF/Múltiplos con target de precio futuro 5 años\n",
    "- **Momentum Model**: Features mejoradas (volumen, SMAs 200/50, RSI)\n",
    "\n",
    "Incluye descarga de datos FRED (macro, commodities, currencies) y feature engineering completo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalar Dependencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias necesarias\n",
    "!pip install -q pandas numpy scikit-learn xgboost lightgbm joblib pyarrow pyyaml fredapi python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Montar Google Drive y Configurar Rutas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Montar Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Configurar ruta base donde están tus datos\n",
    "# AJUSTA ESTA RUTA según donde subiste los archivos en Drive\n",
    "DRIVE_BASE_PATH = '/content/drive/MyDrive/caria_data'  # Cambia esto a tu ruta\n",
    "\n",
    "# Crear estructura de directorios en Colab\n",
    "BASE_DIR = Path('/content/caria_workspace')\n",
    "BASE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Directorios necesarios\n",
    "(BASE_DIR / 'data' / 'gold').mkdir(parents=True, exist_ok=True)\n",
    "(BASE_DIR / 'data' / 'silver' / 'macro').mkdir(parents=True, exist_ok=True)\n",
    "(BASE_DIR / 'models').mkdir(parents=True, exist_ok=True)\n",
    "(BASE_DIR / 'artifacts' / 'models').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Estructura creada en: {BASE_DIR}\")\n",
    "\n",
    "# Verificar que existe Drive\n",
    "if os.path.exists(DRIVE_BASE_PATH):\n",
    "    print(f\"✓ Ruta Drive encontrada: {DRIVE_BASE_PATH}\")\n",
    "else:\n",
    "    print(f\"⚠ Ruta Drive no encontrada: {DRIVE_BASE_PATH}\")\n",
    "    print(\"Por favor ajusta DRIVE_BASE_PATH a la ubicación de tus datos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configurar FRED API Key\n",
    "\n",
    "Necesitas una API key gratuita de FRED: https://fred.stlouisfed.org/docs/api/api_key.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar tu FRED API key aquí\n",
    "# Obtén una gratis en: https://fred.stlouisfed.org/docs/api/api_key.html\n",
    "FRED_API_KEY = \"\"  # Pega tu API key aquí\n",
    "\n",
    "if not FRED_API_KEY:\n",
    "    print(\"⚠ Por favor configura FRED_API_KEY en la celda anterior\")\n",
    "else:\n",
    "    print(\"✓ FRED API Key configurada\")\n",
    "    os.environ[\"FRED_API_KEY\"] = FRED_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Descargar Datos FRED (Macro, Commodities, Currencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from fredapi import Fred\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Series FRED a descargar (expandido con commodities y currencies)\n",
    "FRED_SERIES = {\n",
    "    # Macro\n",
    "    \"GDPC1\": \"GDP Real\",\n",
    "    \"CPIAUCSL\": \"CPI\",\n",
    "    \"UNRATE\": \"Unemployment Rate\",\n",
    "    \"FEDFUNDS\": \"Fed Funds Rate\",\n",
    "    \"DGS10\": \"10Y Treasury\",\n",
    "    \"DGS2\": \"2Y Treasury\",\n",
    "    \"MANPMI\": \"PMI Manufacturing\",\n",
    "    \"UMCSENT\": \"Consumer Sentiment\",\n",
    "    # Commodities - Metales\n",
    "    \"GOLDAMGBD228NLBM\": \"Gold\",\n",
    "    \"PSLVAMUSD\": \"Silver\",\n",
    "    \"PCOPPUSDM\": \"Copper\",\n",
    "    \"PNICKUSDM\": \"Nickel\",\n",
    "    \"PALUMUSDM\": \"Aluminum\",\n",
    "    # Commodities - Energía\n",
    "    \"DCOILWTICO\": \"Crude Oil WTI\",\n",
    "    \"PNRGINDEXM\": \"Natural Gas\",\n",
    "    \"DHOILNYH\": \"Heating Oil\",\n",
    "    # Commodities - Agrícolas\n",
    "    \"PWHEAMTUSDM\": \"Wheat\",\n",
    "    \"PSOYABUSDM\": \"Soybeans\",\n",
    "    \"PCOFFOTMUSDM\": \"Coffee\",\n",
    "    \"PSUGAISAUSDM\": \"Sugar\",\n",
    "    # Currencies\n",
    "    \"DEXUSEU\": \"EUR/USD\",\n",
    "    \"DEXCHUS\": \"CNY/USD\",\n",
    "    \"DEXJPUS\": \"JPY/USD\",\n",
    "    \"DEXUSUK\": \"GBP/USD\",\n",
    "    \"DEXCAUS\": \"CAD/USD\",\n",
    "    \"DEXMXUS\": \"MXN/USD\",\n",
    "    \"DTWEXBGS\": \"Dollar Index Broad\",\n",
    "    \"DTWEXEMEGS\": \"Dollar Index EM\",\n",
    "    # Credit spreads\n",
    "    \"BAA10Y\": \"BAA-10Y Spread\",\n",
    "    \"AAA\": \"AAA Corporate Yield\",\n",
    "    \"BAA\": \"BAA Corporate Yield\",\n",
    "}\n",
    "\n",
    "def download_fred_series(fred, series_id, start_date=\"1900-01-01\"):\n",
    "    \"\"\"Descarga una serie de FRED.\"\"\"\n",
    "    try:\n",
    "        data = fred.get_series(series_id, observation_start=start_date)\n",
    "        if data.empty:\n",
    "            return pd.DataFrame()\n",
    "        df = pd.DataFrame({\"date\": data.index, series_id: data.values})\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        return df.sort_values(\"date\").reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] {series_id}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Descargar datos FRED\n",
    "print(\"Descargando datos desde FRED...\")\n",
    "fred = Fred(api_key=FRED_API_KEY)\n",
    "\n",
    "all_dataframes = []\n",
    "for series_id, description in FRED_SERIES.items():\n",
    "    print(f\"  Descargando {series_id} ({description})...\")\n",
    "    df = download_fred_series(fred, series_id)\n",
    "    if not df.empty:\n",
    "        all_dataframes.append(df)\n",
    "        print(f\"    ✓ {len(df)} observaciones desde {df['date'].min()} hasta {df['date'].max()}\")\n",
    "    else:\n",
    "        print(f\"    ⚠ Sin datos disponibles\")\n",
    "\n",
    "# Combinar todas las series\n",
    "if all_dataframes:\n",
    "    merged = all_dataframes[0]\n",
    "    for df in all_dataframes[1:]:\n",
    "        merged = merged.merge(df, on=\"date\", how=\"outer\")\n",
    "    \n",
    "    merged = merged.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    # Resamplear a frecuencia diaria (forward-fill)\n",
    "    date_range = pd.date_range(start=merged['date'].min(), end=merged['date'].max(), freq='D')\n",
    "    merged_daily = pd.DataFrame(index=date_range)\n",
    "    merged_daily = merged_daily.join(merged.set_index('date'), how='left')\n",
    "    merged_daily = merged_daily.ffill()  # Fixed: usar ffill() en lugar de ffill() deprecado\n",
    "    merged_daily = merged_daily.reset_index()\n",
    "    merged_daily = merged_daily.rename(columns={'index': 'date'})\n",
    "    \n",
    "    # Guardar\n",
    "    output_path = BASE_DIR / 'data' / 'silver' / 'macro' / 'fred_data.parquet'\n",
    "    merged_daily.to_parquet(output_path, index=False)\n",
    "    print(f\"\\n✓ Datos FRED guardados: {output_path}\")\n",
    "    print(f\"  Columnas: {len(merged_daily.columns)}, Filas: {len(merged_daily)}\")\n",
    "    \n",
    "    # También copiar a Drive\n",
    "    drive_macro_path = Path(DRIVE_BASE_PATH) / 'data' / 'silver' / 'macro'\n",
    "    drive_macro_path.mkdir(parents=True, exist_ok=True)\n",
    "    merged_daily.to_parquet(drive_macro_path / 'fred_data.parquet', index=False)\n",
    "    print(f\"✓ Copiado a Drive: {drive_macro_path / 'fred_data.parquet'}\")\n",
    "else:\n",
    "    print(\"⚠ No se descargaron datos FRED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función helper para codificar variables categóricas (debe estar disponible globalmente)\n",
    "def encode_categorical_features(df_train, df_val, df_test, features):\n",
    "    \"\"\"Codifica variables categóricas usando label encoding.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    df_train = df_train[features].copy()\n",
    "    df_val = df_val[features].copy()\n",
    "    df_test = df_test[features].copy()\n",
    "    \n",
    "    # Identificar columnas categóricas (object o string)\n",
    "    categorical_cols = []\n",
    "    for col in features:\n",
    "        if col in df_train.columns:\n",
    "            if df_train[col].dtype == 'object' or df_train[col].dtype.name == 'category':\n",
    "                categorical_cols.append(col)\n",
    "    \n",
    "    # Codificar variables categóricas\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        # Entrenar con train + val + test para tener todos los valores posibles\n",
    "        all_values = pd.concat([df_train[col], df_val[col], df_test[col]], axis=0).dropna().astype(str)\n",
    "        le.fit(all_values.unique())\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "        # Transformar cada dataset\n",
    "        df_train[col] = df_train[col].astype(str).map(lambda x: le.transform([x])[0] if x in le.classes_ else 0)\n",
    "        df_val[col] = df_val[col].astype(str).map(lambda x: le.transform([x])[0] if x in le.classes_ else 0)\n",
    "        df_test[col] = df_test[col].astype(str).map(lambda x: le.transform([x])[0] if x in le.classes_ else 0)\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"  Variables categóricas codificadas: {categorical_cols}\")\n",
    "    \n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "print(\"✓ Función encode_categorical_features definida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos FRED\n",
    "macro_df = pd.read_parquet(BASE_DIR / 'data' / 'silver' / 'macro' / 'fred_data.parquet')\n",
    "\n",
    "# Calcular features macro cíclicas\n",
    "print(\"Calculando features macro...\")\n",
    "\n",
    "# Yield curve slope\n",
    "if \"DGS10\" in macro_df.columns and \"DGS2\" in macro_df.columns:\n",
    "    macro_df[\"yield_curve_slope\"] = macro_df[\"DGS10\"] - macro_df[\"DGS2\"]\n",
    "    macro_df[\"yield_curve_inverted\"] = (macro_df[\"yield_curve_slope\"] < 0).astype(int)\n",
    "\n",
    "# Credit spread\n",
    "if \"BAA\" in macro_df.columns and \"AAA\" in macro_df.columns:\n",
    "    macro_df[\"credit_spread\"] = macro_df[\"BAA\"] - macro_df[\"AAA\"]\n",
    "elif \"DGS10\" in macro_df.columns:\n",
    "    macro_df[\"credit_spread\"] = macro_df[\"DGS10\"] * 0.02\n",
    "\n",
    "# Recession probability\n",
    "macro_df[\"recession_probability\"] = 0.0\n",
    "if \"MANPMI\" in macro_df.columns:\n",
    "    macro_df[\"pmi_below_50\"] = (macro_df[\"MANPMI\"] < 50).astype(int)\n",
    "    macro_df[\"recession_probability\"] += macro_df[\"pmi_below_50\"] * 0.3\n",
    "if \"UNRATE\" in macro_df.columns:\n",
    "    macro_df[\"unemployment_change\"] = macro_df[\"UNRATE\"].diff()\n",
    "    macro_df[\"unemployment_rising\"] = (macro_df[\"unemployment_change\"] > 0.5).astype(int)\n",
    "    macro_df[\"recession_probability\"] += macro_df[\"unemployment_rising\"] * 0.3\n",
    "if \"yield_curve_inverted\" in macro_df.columns:\n",
    "    macro_df[\"recession_probability\"] += macro_df[\"yield_curve_inverted\"] * 0.4\n",
    "macro_df[\"recession_probability\"] = np.clip(macro_df[\"recession_probability\"], 0, 1)\n",
    "\n",
    "# Macro regime\n",
    "macro_df[\"macro_regime\"] = \"expansion\"\n",
    "macro_df.loc[macro_df[\"recession_probability\"] > 0.5, \"macro_regime\"] = \"recession\"\n",
    "macro_df.loc[(macro_df[\"recession_probability\"] > 0.3) & (macro_df[\"recession_probability\"] <= 0.5), \"macro_regime\"] = \"slowdown\"\n",
    "\n",
    "# Commodity momentum\n",
    "commodity_cols = [\"GOLDAMGBD228NLBM\", \"PSLVAMUSD\", \"DCOILWTICO\", \"PCOPPUSDM\"]\n",
    "for col in commodity_cols:\n",
    "    if col in macro_df.columns:\n",
    "        macro_df[f\"{col}_momentum_3m\"] = macro_df[col].pct_change(periods=63)\n",
    "        macro_df[f\"{col}_momentum_12m\"] = macro_df[col].pct_change(periods=252)\n",
    "\n",
    "# Commodity ratios\n",
    "if \"GOLDAMGBD228NLBM\" in macro_df.columns and \"DCOILWTICO\" in macro_df.columns:\n",
    "    macro_df[\"gold_oil_ratio\"] = macro_df[\"GOLDAMGBD228NLBM\"] / (macro_df[\"DCOILWTICO\"] + 1e-6)\n",
    "    macro_df[\"risk_aversion_indicator\"] = (macro_df[\"gold_oil_ratio\"] > macro_df[\"gold_oil_ratio\"].rolling(252).mean()).astype(int)\n",
    "\n",
    "if \"PCOPPUSDM\" in macro_df.columns and \"GOLDAMGBD228NLBM\" in macro_df.columns:\n",
    "    macro_df[\"copper_gold_ratio\"] = macro_df[\"PCOPPUSDM\"] / (macro_df[\"GOLDAMGBD228NLBM\"] + 1e-6)\n",
    "    macro_df[\"growth_indicator\"] = (macro_df[\"copper_gold_ratio\"] > macro_df[\"copper_gold_ratio\"].rolling(252).mean()).astype(int)\n",
    "\n",
    "# Currency features\n",
    "currency_cols = [\"DEXUSEU\", \"DEXCHUS\", \"DEXJPUS\", \"DEXUSUK\", \"DEXCAUS\", \"DEXMXUS\", \"DTWEXBGS\"]\n",
    "for col in currency_cols:\n",
    "    if col in macro_df.columns:\n",
    "        macro_df[f\"{col}_momentum_3m\"] = macro_df[col].pct_change(periods=63)\n",
    "        macro_df[f\"{col}_strength\"] = (macro_df[col] / macro_df[col].rolling(252).mean() - 1)\n",
    "\n",
    "# Guardar features macro procesadas\n",
    "macro_features_path = BASE_DIR / 'data' / 'silver' / 'macro' / 'macro_features.parquet'\n",
    "macro_df.to_parquet(macro_features_path, index=False)\n",
    "print(f\"✓ Features macro guardadas: {macro_features_path}\")\n",
    "print(f\"  Features creadas: {len([c for c in macro_df.columns if c not in FRED_SERIES.keys()])} nuevas columnas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Copiar archivos parquet de gold desde Drive\n",
    "drive_data_path = Path(DRIVE_BASE_PATH)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    src = drive_data_path / 'data' / 'gold' / f'{split}.parquet'\n",
    "    dst = BASE_DIR / 'data' / 'gold' / f'{split}.parquet'\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"✓ Copiado: {split}.parquet ({dst.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"⚠ No encontrado: {src}\")\n",
    "\n",
    "# Cargar datos gold\n",
    "print(\"\\nCargando datos gold...\")\n",
    "train_df = pd.read_parquet(BASE_DIR / 'data' / 'gold' / 'train.parquet')\n",
    "val_df = pd.read_parquet(BASE_DIR / 'data' / 'gold' / 'val.parquet')\n",
    "test_df = pd.read_parquet(BASE_DIR / 'data' / 'gold' / 'test.parquet')\n",
    "\n",
    "print(f\"✓ train: {len(train_df)} filas\")\n",
    "print(f\"✓ val: {len(val_df)} filas\")\n",
    "print(f\"✓ test: {len(test_df)} filas\")\n",
    "\n",
    "# Combinar con macro usando merge_asof\n",
    "print(\"\\nCombinando con datos macro...\")\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "val_df['date'] = pd.to_datetime(val_df['date'])\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "macro_df['date'] = pd.to_datetime(macro_df['date'])\n",
    "\n",
    "# Seleccionar columnas macro relevantes\n",
    "macro_cols_to_merge = [\n",
    "    'yield_curve_slope', 'credit_spread', 'recession_probability', 'macro_regime',\n",
    "    'gold_oil_ratio', 'copper_gold_ratio', 'risk_aversion_indicator', 'growth_indicator'\n",
    "]\n",
    "macro_cols_to_merge = [c for c in macro_cols_to_merge if c in macro_df.columns]\n",
    "macro_subset = macro_df[['date'] + macro_cols_to_merge].sort_values('date')\n",
    "\n",
    "# Merge con train/val/test\n",
    "\n",
    "# Validar que macro_subset no esté vacío antes de merge\n",
    "if macro_subset.empty:\n",
    "    print('⚠ macro_subset está vacío, saltando merge con macro')\n",
    "    # Crear columnas macro vacías para mantener consistencia\n",
    "    for col in macro_cols_to_merge:\n",
    "        train_df[col] = np.nan\n",
    "        val_df[col] = np.nan\n",
    "        test_df[col] = np.nan\n",
    "else:\n",
    "    print(f'  Macro features disponibles: {len(macro_cols_to_merge)} columnas')\n",
    "    for df_name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "        df_sorted = df.sort_values(['ticker', 'date']) if 'ticker' in df.columns else df.sort_values('date')\n",
    "        df_merged = pd.merge_asof(df_sorted, macro_subset, on='date', direction='backward')\n",
    "        if df_name == 'train':\n",
    "            train_df = df_merged\n",
    "        elif df_name == 'val':\n",
    "            val_df = df_merged\n",
    "        else:\n",
    "            test_df = df_merged\n",
    "        print(f\"✓ Datos combinados con macro features\")\n",
    "        print(f\"  Train columns: {len(train_df.columns)}\")\n",
    "        print(f\"  Val columns: {len(val_df.columns)}\")\n",
    "        print(f\"  Test columns: {len(test_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering para Stocks (Percentiles Históricos, Lags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular features relativas históricas para evitar leakage\n",
    "print(\"Calculando features históricas relativas...\")\n",
    "\n",
    "def add_historical_features(df):\n",
    "    \"\"\"Agrega features históricas (percentiles, lags, promedios) para evitar leakage.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Asegurar que está ordenado por ticker y fecha\n",
    "    if 'ticker' in df.columns:\n",
    "        df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    else:\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Percentiles históricos de múltiplos (5 años = ~1260 trading days)\n",
    "    valuation_cols = ['priceToBookRatio', 'priceToSalesRatio', 'enterpriseValue', 'freeCashFlowYield']\n",
    "    for col in valuation_cols:\n",
    "        if col in df.columns:\n",
    "            # Percentil histórico por ticker\n",
    "            if 'ticker' in df.columns:\n",
    "                df[f'{col}_percentile_5y'] = df.groupby('ticker')[col].transform(\n",
    "                    lambda x: x.rolling(window=1260, min_periods=252).apply(\n",
    "                        lambda y: (y.iloc[-1] > y.iloc[:-1]).sum() / len(y.iloc[:-1]) if len(y.iloc[:-1]) > 0 else 0.5\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                df[f'{col}_percentile_5y'] = df[col].rolling(window=1260, min_periods=252).apply(\n",
    "                    lambda y: (y.iloc[-1] > y.iloc[:-1]).sum() / len(y.iloc[:-1]) if len(y.iloc[:-1]) > 0 else 0.5\n",
    "                )\n",
    "    \n",
    "    # ROIC/ROE históricos con lags para evitar leakage\n",
    "    quality_cols = ['roic', 'returnOnEquity', 'returnOnAssets']\n",
    "    for col in quality_cols:\n",
    "        if col in df.columns:\n",
    "            # Promedio histórico (3 años)\n",
    "            if 'ticker' in df.columns:\n",
    "                df[f'{col}_3y_avg'] = df.groupby('ticker')[col].transform(\n",
    "                    lambda x: x.rolling(window=756, min_periods=252).mean()\n",
    "                )\n",
    "                # Lags (trimestres anteriores)\n",
    "                df[f'{col}_lag_1q'] = df.groupby('ticker')[col].shift(63)\n",
    "                df[f'{col}_lag_2q'] = df.groupby('ticker')[col].shift(126)\n",
    "            else:\n",
    "                df[f'{col}_3y_avg'] = df[col].rolling(window=756, min_periods=252).mean()\n",
    "                df[f'{col}_lag_1q'] = df[col].shift(63)\n",
    "                df[f'{col}_lag_2q'] = df[col].shift(126)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicar a train, val, test\n",
    "train_df = add_historical_features(train_df)\n",
    "val_df = add_historical_features(val_df)\n",
    "test_df = add_historical_features(test_df)\n",
    "\n",
    "print(\"✓ Features históricas calculadas\")\n",
    "print(f\"  Nuevas columnas agregadas: {len([c for c in train_df.columns if 'percentile' in c or 'lag' in c or '_3y_avg' in c])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función helper para codificar variables categóricas (debe estar disponible globalmente)\n",
    "def encode_categorical_features(df_train, df_val, df_test, features):\n",
    "    \"\"\"Codifica variables categóricas usando label encoding.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    df_train = df_train[features].copy()\n",
    "    df_val = df_val[features].copy()\n",
    "    df_test = df_test[features].copy()\n",
    "    \n",
    "    # Identificar columnas categóricas (object o string)\n",
    "    categorical_cols = []\n",
    "    for col in features:\n",
    "        if col in df_train.columns:\n",
    "            if df_train[col].dtype == 'object' or df_train[col].dtype.name == 'category':\n",
    "                categorical_cols.append(col)\n",
    "    \n",
    "    # Codificar variables categóricas\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        # Entrenar con train + val + test para tener todos los valores posibles\n",
    "        all_values = pd.concat([df_train[col], df_val[col], df_test[col]], axis=0).dropna().astype(str)\n",
    "        le.fit(all_values.unique())\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "        # Transformar cada dataset\n",
    "        df_train[col] = df_train[col].astype(str).map(lambda x: le.transform([x])[0] if x in le.classes_ else 0)\n",
    "        df_val[col] = df_val[col].astype(str).map(lambda x: le.transform([x])[0] if x in le.classes_ else 0)\n",
    "        df_test[col] = df_test[col].astype(str).map(lambda x: le.transform([x])[0] if x in le.classes_ else 0)\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"  Variables categóricas codificadas: {categorical_cols}\")\n",
    "    \n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "print(\"✓ Función encode_categorical_features definida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entrenar Quality Model (Percentiles por Fecha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTRENANDO QUALITY MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Features para Quality Model (SIN roic actual para evitar leakage)\n",
    "quality_features = [\n",
    "    'roic_lag_1q', 'roic_lag_2q', 'roic_3y_avg',\n",
    "    'returnOnEquity', 'returnOnAssets',\n",
    "    'grossProfitMargin', 'netProfitMargin',\n",
    "    'freeCashFlowYield', 'freeCashFlowPerShare',\n",
    "    'revenueGrowth', 'netIncomeGrowth',\n",
    "]\n",
    "\n",
    "# Agregar features macro si están disponibles\n",
    "macro_quality_features = ['recession_probability', 'macro_regime', 'credit_spread']\n",
    "quality_features.extend([f for f in macro_quality_features if f in train_df.columns])\n",
    "\n",
    "# Filtrar solo features que existen\n",
    "quality_features = [f for f in quality_features if f in train_df.columns]\n",
    "print(f\"\\nFeatures usadas ({len(quality_features)}): {quality_features[:5]}...\")\n",
    "\n",
    "# Función helper para codificar variables categóricas\n",
    "\n",
    "# Preparar datos - codificar variables categóricas primero\n",
    "X_train_quality, X_val_quality, X_test_quality = encode_categorical_features(\n",
    "    train_df, val_df, test_df, quality_features\n",
    ")\n",
    "\n",
    "# Convertir a float32 y llenar NaN\n",
    "X_train_quality = X_train_quality.fillna(0).astype('float32')\n",
    "X_val_quality = X_val_quality.fillna(0).astype('float32')\n",
    "X_test_quality = X_test_quality.fillna(0).astype('float32')\n",
    "\n",
    "# Crear labels: Top 20% de ROIC POR FECHA (adaptado al régimen económico)\n",
    "print(\"\\nCreando labels por fecha (percentiles por fecha)...\")\n",
    "train_df['roic_percentile_by_date'] = train_df.groupby('date')['roic'].rank(pct=True)\n",
    "train_df['is_quality'] = (train_df['roic_percentile_by_date'] > 0.80).astype(int)\n",
    "\n",
    "val_df['roic_percentile_by_date'] = val_df.groupby('date')['roic'].rank(pct=True)\n",
    "val_df['is_quality'] = (val_df['roic_percentile_by_date'] > 0.80).astype(int)\n",
    "\n",
    "test_df['roic_percentile_by_date'] = test_df.groupby('date')['roic'].rank(pct=True)\n",
    "test_df['is_quality'] = (test_df['roic_percentile_by_date'] > 0.80).astype(int)\n",
    "\n",
    "y_train_quality = train_df['is_quality']\n",
    "y_val_quality = val_df['is_quality']\n",
    "y_test_quality = test_df['is_quality']\n",
    "\n",
    "print(f\"  Train: {y_train_quality.sum()} / {len(y_train_quality)} positivos ({y_train_quality.mean():.2%})\")\n",
    "print(f\"  Val: {y_val_quality.sum()} / {len(y_val_quality)} positivos ({y_val_quality.mean():.2%})\")\n",
    "print(f\"  Test: {y_test_quality.sum()} / {len(y_test_quality)} positivos ({y_test_quality.mean():.2%})\")\n",
    "\n",
    "# Calcular scale_pos_weight para balancear clases\n",
    "scale_pos_weight = (len(y_train_quality) - y_train_quality.sum()) / max(y_train_quality.sum(), 1)\n",
    "\n",
    "# Entrenar modelo con hiperparámetros anti-overfitting\n",
    "print(\"\\nEntrenando modelo...\")\n",
    "quality_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=3,  # Reducido para evitar overfitting\n",
    "    learning_rate=0.01,\n",
    "    reg_alpha=2.0,\n",
    "    reg_lambda=3.0,\n",
    "    subsample=0.75,\n",
    "    colsample_bytree=0.75,\n",
    "    min_child_weight=10,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "quality_model.fit(\n",
    "    X_train_quality,\n",
    "    y_train_quality,\n",
    "    eval_set=[(X_val_quality, y_val_quality)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "y_pred_train = quality_model.predict(X_train_quality)\n",
    "y_pred_val = quality_model.predict(X_val_quality)\n",
    "y_pred_test = quality_model.predict(X_test_quality)\n",
    "\n",
    "y_pred_proba_train = quality_model.predict_proba(X_train_quality)[:, 1]\n",
    "y_pred_proba_val = quality_model.predict_proba(X_val_quality)[:, 1]\n",
    "y_pred_proba_test = quality_model.predict_proba(X_test_quality)[:, 1]\n",
    "\n",
    "print(\"\\nResultados Quality Model:\")\n",
    "print(f\"  Train - Accuracy: {accuracy_score(y_train_quality, y_pred_train):.4f}, AUC: {roc_auc_score(y_train_quality, y_pred_proba_train):.4f}\")\n",
    "print(f\"  Val   - Accuracy: {accuracy_score(y_val_quality, y_pred_val):.4f}, AUC: {roc_auc_score(y_val_quality, y_pred_proba_val):.4f}\")\n",
    "print(f\"  Test  - Accuracy: {accuracy_score(y_test_quality, y_pred_test):.4f}, AUC: {roc_auc_score(y_test_quality, y_pred_proba_test):.4f}\")\n",
    "\n",
    "# Guardar\n",
    "output_path = BASE_DIR / 'models' / 'improved_quality_model.pkl'\n",
    "joblib.dump(quality_model, output_path)\n",
    "print(f\"\\n✓ Modelo guardado: {output_path}\")\n",
    "\n",
    "# Copiar a Drive\n",
    "drive_models_path = Path(DRIVE_BASE_PATH) / 'models'\n",
    "drive_models_path.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(quality_model, drive_models_path / 'improved_quality_model.pkl')\n",
    "print(f\"✓ Copiado a Drive: {drive_models_path / 'improved_quality_model.pkl'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTRENANDO VALUATION MODEL (DCF/Múltiplos)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Features para Valuation Model\n",
    "valuation_features = [\n",
    "    # Múltiplos relativos históricos\n",
    "    'priceToBookRatio_percentile_5y',\n",
    "    'priceToSalesRatio_percentile_5y',\n",
    "    'freeCashFlowYield_percentile_5y',\n",
    "    # Fundamentales\n",
    "    'priceToBookRatio', 'priceToSalesRatio', 'enterpriseValue',\n",
    "    'returnOnEquity', 'roic', 'grossProfitMargin', 'netProfitMargin',\n",
    "    'freeCashFlowYield', 'revenueGrowth', 'netIncomeGrowth',\n",
    "    # Macro contexto\n",
    "    'yield_curve_slope', 'credit_spread', 'recession_probability',\n",
    "    'gold_oil_ratio', 'copper_gold_ratio',\n",
    "]\n",
    "\n",
    "# Filtrar solo features que existen\n",
    "valuation_features = [f for f in valuation_features if f in train_df.columns]\n",
    "print(f\"\\nFeatures usadas ({len(valuation_features)}): {valuation_features[:5]}...\")\n",
    "\n",
    "# Preparar datos - codificar variables categóricas primero\n",
    "X_train_val, X_val_val, X_test_val = encode_categorical_features(\n",
    "    train_df, val_df, test_df, valuation_features\n",
    ")\n",
    "\n",
    "# Convertir a float32 y llenar NaN\n",
    "X_train_val = X_train_val.fillna(0).astype('float32')\n",
    "X_val_val = X_val_val.fillna(0).astype('float32')\n",
    "X_test_val = X_test_val.fillna(0).astype('float32')\n",
    "\n",
    "# Target: Precio futuro 5 años (1260 trading days) vs precio actual\n",
    "# Para esto necesitamos calcular el precio futuro desde la fecha actual\n",
    "print(\"\\nCalculando target: precio futuro 5 años...\")\n",
    "\n",
    "def calculate_future_price_target(df, future_days=1260):\n",
    "    \"\"\"Calcula el precio futuro como target para valuación.\"\"\"\n",
    "    df = df.copy()\n",
    "    if 'ticker' not in df.columns or 'date' not in df.columns:\n",
    "        print(\"  ⚠ No se puede calcular target sin ticker y date\")\n",
    "        return pd.Series(index=df.index, dtype='float64')\n",
    "    \n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    # Buscar precio futuro (5 años = ~1260 trading days)\n",
    "    # Asumimos que tenemos una columna de precio o podemos calcularlo desde múltiplos\n",
    "    # Por ahora usamos forward returns acumulados como proxy\n",
    "    if 'target' in df.columns:\n",
    "        # Acumular returns futuros (aproximación)\n",
    "        df['future_price_target'] = df.groupby('ticker')['target'].transform(\n",
    "            lambda x: x.rolling(window=future_days, min_periods=1).apply(\n",
    "                lambda y: (1 + y).prod() if len(y) > 0 else 1.0\n",
    "            )\n",
    "        )\n",
    "        return df['future_price_target']\n",
    "    else:\n",
    "        print(\"  ⚠ No hay columna 'target' para calcular precio futuro\")\n",
    "        return pd.Series(index=df.index, dtype='float64')\n",
    "\n",
    "# Para simplificar, usamos target como proxy de retorno futuro\n",
    "# En producción, esto debería ser precio real futuro vs precio actual\n",
    "y_train_val = train_df['target'].fillna(0)  # Proxy: forward returns\n",
    "y_val_val = val_df['target'].fillna(0)\n",
    "y_test_val = test_df['target'].fillna(0)\n",
    "\n",
    "print(f\"  Train target stats: mean={y_train_val.mean():.4f}, std={y_train_val.std():.4f}\")\n",
    "print(f\"  Val target stats: mean={y_val_val.mean():.4f}, std={y_val_val.std():.4f}\")\n",
    "\n",
    "# Entrenar modelo (Regressor para predecir valor justo)\n",
    "print(\"\\nEntrenando modelo...\")\n",
    "valuation_model = xgb.XGBRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.01,\n",
    "    reg_alpha=1.5,\n",
    "    reg_lambda=2.5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=5,\n",
    "    random_state=42,\n",
    "    eval_metric='rmse',\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "valuation_model.fit(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    eval_set=[(X_val_val, y_val_val)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "y_pred_train = valuation_model.predict(X_train_val)\n",
    "y_pred_val = valuation_model.predict(X_val_val)\n",
    "y_pred_test = valuation_model.predict(X_test_val)\n",
    "\n",
    "print(\"\\nResultados Valuation Model:\")\n",
    "print(f\"  Train - RMSE: {np.sqrt(mean_squared_error(y_train_val, y_pred_train)):.4f}, R²: {r2_score(y_train_val, y_pred_train):.4f}\")\n",
    "print(f\"  Val   - RMSE: {np.sqrt(mean_squared_error(y_val_val, y_pred_val)):.4f}, R²: {r2_score(y_val_val, y_pred_val):.4f}\")\n",
    "print(f\"  Test  - RMSE: {np.sqrt(mean_squared_error(y_test_val, y_pred_test)):.4f}, R²: {r2_score(y_test_val, y_pred_test):.4f}\")\n",
    "\n",
    "# Guardar\n",
    "output_path = BASE_DIR / 'models' / 'improved_valuation_model.pkl'\n",
    "joblib.dump(valuation_model, output_path)\n",
    "print(f\"\\n✓ Modelo guardado: {output_path}\")\n",
    "\n",
    "# Copiar a Drive\n",
    "joblib.dump(valuation_model, drive_models_path / 'improved_valuation_model.pkl')\n",
    "print(f\"✓ Copiado a Drive: {drive_models_path / 'improved_valuation_model.pkl'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ENTRENANDO MOMENTUM MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Features para Momentum Model (volumen, SMAs 200/50, RSI)\n",
    "momentum_features = [\n",
    "    # Volumen (más importante según usuario)\n",
    "    'volume', 'volume_sma_20',\n",
    "    # SMAs (200 y 50 son las más importantes)\n",
    "    'sma_200', 'sma_50', 'sma_20',\n",
    "    'ema_20', 'ema_50',\n",
    "    # Posición relativa vs SMAs\n",
    "    # (calcularemos si precio está sobre SMA)\n",
    "    # RSI (en menor medida según usuario)\n",
    "    'rsi_14',\n",
    "    # Otros técnicos\n",
    "    'macd', 'macd_signal',\n",
    "    'atr_14', 'volatility_30d',\n",
    "]\n",
    "\n",
    "# Agregar features calculadas: precio sobre SMAs\n",
    "if 'sma_200' in train_df.columns:\n",
    "    # Asumimos que hay una columna de precio o la calculamos desde múltiplos\n",
    "    # Por simplicidad, usamos enterpriseValue como proxy si no hay precio directo\n",
    "    price_proxy = train_df['price'] if 'price' in train_df.columns else None\n",
    "    if price_proxy is None:\n",
    "        # Intentar calcular desde múltiplos si están disponibles\n",
    "        if 'priceToBookRatio' in train_df.columns and 'bookValue' in train_df.columns:\n",
    "            price_proxy = train_df['priceToBookRatio'] * train_df['bookValue']\n",
    "        else:\n",
    "            price_proxy = train_df['enterpriseValue'] if 'enterpriseValue' in train_df.columns else pd.Series([1.0] * len(train_df), index=train_df.index)\n",
    "    \n",
    "    train_df['price_above_sma200'] = (price_proxy > train_df['sma_200']).astype(int)\n",
    "    train_df['price_above_sma50'] = (price_proxy > train_df['sma_50']).astype(int)\n",
    "    \n",
    "    val_df['price_above_sma200'] = ((val_df['price'] if 'price' in val_df.columns else (val_df['enterpriseValue'] if 'enterpriseValue' in val_df.columns else 1.0)) > val_df['sma_200']).astype(int)\n",
    "    val_df['price_above_sma50'] = ((val_df['price'] if 'price' in val_df.columns else (val_df['enterpriseValue'] if 'enterpriseValue' in val_df.columns else 1.0)) > val_df['sma_50']).astype(int)\n",
    "    \n",
    "    test_df['price_above_sma200'] = ((test_df['price'] if 'price' in test_df.columns else (test_df['enterpriseValue'] if 'enterpriseValue' in test_df.columns else 1.0)) > test_df['sma_200']).astype(int)\n",
    "    test_df['price_above_sma50'] = ((test_df['price'] if 'price' in test_df.columns else (test_df['enterpriseValue'] if 'enterpriseValue' in test_df.columns else 1.0)) > test_df['sma_50']).astype(int)\n",
    "    \n",
    "    momentum_features.extend(['price_above_sma200', 'price_above_sma50'])\n",
    "\n",
    "# Ratio de volumen (volumen actual / SMA volumen)\n",
    "if 'volume' in train_df.columns and 'volume_sma_20' in train_df.columns:\n",
    "    train_df['volume_ratio_20d'] = train_df['volume'] / (train_df['volume_sma_20'] + 1e-6)\n",
    "    val_df['volume_ratio_20d'] = val_df['volume'] / (val_df['volume_sma_20'] + 1e-6)\n",
    "    test_df['volume_ratio_20d'] = test_df['volume'] / (test_df['volume_sma_20'] + 1e-6)\n",
    "    momentum_features.append('volume_ratio_20d')\n",
    "\n",
    "# Slope de SMA 200 (tendencia)\n",
    "if 'sma_200' in train_df.columns:\n",
    "    train_df['sma200_slope'] = train_df.groupby('ticker')['sma_200'].transform(lambda x: x.diff(20)) if 'ticker' in train_df.columns else train_df['sma_200'].diff(20)\n",
    "    val_df['sma200_slope'] = val_df.groupby('ticker')['sma_200'].transform(lambda x: x.diff(20)) if 'ticker' in val_df.columns else val_df['sma_200'].diff(20)\n",
    "    test_df['sma200_slope'] = test_df.groupby('ticker')['sma_200'].transform(lambda x: x.diff(20)) if 'ticker' in test_df.columns else test_df['sma_200'].diff(20)\n",
    "    momentum_features.append('sma200_slope')\n",
    "\n",
    "# Filtrar solo features que existen\n",
    "momentum_features = [f for f in momentum_features if f in train_df.columns]\n",
    "print(f\"\\nFeatures usadas ({len(momentum_features)}): {momentum_features}\")\n",
    "\n",
    "# Preparar datos - codificar variables categóricas primero\n",
    "X_train_momentum, X_val_momentum, X_test_momentum = encode_categorical_features(\n",
    "    train_df, val_df, test_df, momentum_features\n",
    ")\n",
    "\n",
    "# Convertir a float32 y llenar NaN\n",
    "X_train_momentum = X_train_momentum.fillna(0).astype('float32')\n",
    "X_val_momentum = X_val_momentum.fillna(0).astype('float32')\n",
    "X_test_momentum = X_test_momentum.fillna(0).astype('float32')\n",
    "\n",
    "# Target: Dirección de retorno (positivo vs negativo)\n",
    "y_train_momentum = (train_df['target'] > 0).astype(int)\n",
    "y_val_momentum = (val_df['target'] > 0).astype(int)\n",
    "y_test_momentum = (test_df['target'] > 0).astype(int)\n",
    "\n",
    "print(f\"\\nTarget distribución:\")\n",
    "print(f\"  Train: {y_train_momentum.sum()} / {len(y_train_momentum)} positivos ({y_train_momentum.mean():.2%})\")\n",
    "print(f\"  Val: {y_val_momentum.sum()} / {len(y_val_momentum)} positivos ({y_val_momentum.mean():.2%})\")\n",
    "print(f\"  Test: {y_test_momentum.sum()} / {len(y_test_momentum)} positivos ({y_test_momentum.mean():.2%})\")\n",
    "\n",
    "# Entrenar modelo\n",
    "print(\"\\nEntrenando modelo...\")\n",
    "momentum_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,  # Más profundo para capturar relaciones complejas\n",
    "    learning_rate=0.01,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=2.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=3,\n",
    "    random_state=42,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "momentum_model.fit(\n",
    "    X_train_momentum,\n",
    "    y_train_momentum,\n",
    "    eval_set=[(X_val_momentum, y_val_momentum)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "y_pred_train = momentum_model.predict(X_train_momentum)\n",
    "y_pred_val = momentum_model.predict(X_val_momentum)\n",
    "y_pred_test = momentum_model.predict(X_test_momentum)\n",
    "\n",
    "y_pred_proba_train = momentum_model.predict_proba(X_train_momentum)[:, 1]\n",
    "y_pred_proba_val = momentum_model.predict_proba(X_val_momentum)[:, 1]\n",
    "y_pred_proba_test = momentum_model.predict_proba(X_test_momentum)[:, 1]\n",
    "\n",
    "print(\"\\nResultados Momentum Model:\")\n",
    "print(f\"  Train - Accuracy: {accuracy_score(y_train_momentum, y_pred_train):.4f}, AUC: {roc_auc_score(y_train_momentum, y_pred_proba_train):.4f}\")\n",
    "print(f\"  Val   - Accuracy: {accuracy_score(y_val_momentum, y_pred_val):.4f}, AUC: {roc_auc_score(y_val_momentum, y_pred_proba_val):.4f}\")\n",
    "print(f\"  Test  - Accuracy: {accuracy_score(y_test_momentum, y_pred_test):.4f}, AUC: {roc_auc_score(y_test_momentum, y_pred_proba_test):.4f}\")\n",
    "\n",
    "# Guardar\n",
    "output_path = BASE_DIR / 'models' / 'improved_momentum_model.pkl'\n",
    "joblib.dump(momentum_model, output_path)\n",
    "print(f\"\\n✓ Modelo guardado: {output_path}\")\n",
    "\n",
    "# Copiar a Drive\n",
    "joblib.dump(momentum_model, drive_models_path / 'improved_momentum_model.pkl')\n",
    "print(f\"✓ Copiado a Drive: {drive_models_path / 'improved_momentum_model.pkl'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Guardar Feature Config y Resumen Final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar feature config para uso futuro\n",
    "feature_config = {\n",
    "    'quality_features': quality_features,\n",
    "    'valuation_features': valuation_features,\n",
    "    'momentum_features': momentum_features,\n",
    "}\n",
    "\n",
    "feature_config_path = BASE_DIR / 'models' / 'improved_feature_config.pkl'\n",
    "joblib.dump(feature_config, feature_config_path)\n",
    "print(f\"✓ Feature config guardado: {feature_config_path}\")\n",
    "\n",
    "# Copiar a Drive\n",
    "joblib.dump(feature_config, drive_models_path / 'improved_feature_config.pkl')\n",
    "print(f\"✓ Copiado a Drive: {drive_models_path / 'improved_feature_config.pkl'}\")\n",
    "\n",
    "# Resumen final\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMEN FINAL - MODELOS MEJORADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n✓ Modelos entrenados y guardados:\")\n",
    "print(f\"  1. Quality Model: {BASE_DIR / 'models' / 'improved_quality_model.pkl'}\")\n",
    "print(f\"  2. Valuation Model: {BASE_DIR / 'models' / 'improved_valuation_model.pkl'}\")\n",
    "print(f\"  3. Momentum Model: {BASE_DIR / 'models' / 'improved_momentum_model.pkl'}\")\n",
    "\n",
    "print(\"\\n✓ Características principales:\")\n",
    "print(\"  - Quality: Percentiles por fecha (identifica outliers en cualquier régimen)\")\n",
    "print(\"  - Valuation: DCF/Múltiplos con contexto macro (target futuro 5 años)\")\n",
    "print(\"  - Momentum: Features mejoradas (volumen, SMAs 200/50, RSI)\")\n",
    "\n",
    "print(\"\\n✓ Datos procesados:\")\n",
    "print(f\"  - Datos FRED descargados y procesados\")\n",
    "print(f\"  - Features macro cíclicas calculadas\")\n",
    "print(f\"  - Features históricas relativas agregadas (percentiles, lags)\")\n",
    "\n",
    "print(\"\\n✓ Archivos en Drive:\")\n",
    "print(f\"  - Modelos: {drive_models_path}\")\n",
    "print(f\"  - Datos macro: {Path(DRIVE_BASE_PATH) / 'data' / 'silver' / 'macro'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENTRENAMIENTO COMPLETADO\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
